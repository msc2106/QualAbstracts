ABSTRACT: 
Understanding how the dynamics of a neural network is shaped by the network structure, and consequently how the network structure facilitates the functions implemented by the neural system, is at the core of using mathematical models to elucidate brain functions. This study investigates the tracking dynamics of continuous attractor neural networks (CANNs). Due to the translational invariance of neuronal recurrent interactions, CANNs can hold a continuous family of stationary states. They form a continuous manifold in which the neural system is neutrally stable. We systematically explore how this property facilitates the tracking performance of a CANN, which is believed to have clear correspondence with brain functions. By using the wave functions of the quantum harmonic oscillator as the basis, we demonstrate how the dynamics of a CANN is decomposed into different motion modes, corresponding to distortions in the amplitude, position, width or skewness of the network state. We then develop a perturbative approach that utilizes the dominating movement of the network's stationary states in the state space. This method allows us to approximate the network dynamics up to an arbitrary accuracy depending on the order of perturbation used. We quantify the distortions of a Gaussian bump during tracking, and study their effects on the tracking performance. Results are obtained on the maximum speed for a moving stimulus to be trackable and the reaction time for the network to catch up with an abrupt change in the stimulus.Comment: 43 pages, 10 figure. 
 
PREDICTION: 
Recently, a type of recurrent network, called contin- uous attractor neural networks (CANNs), has received wide attention (Wilson and Cowan 1972; Amari 1977; Georgopoulos, Kalaska, Caminiti, and Massey 1982; Maunsell and Van Essen 1983; Funahashi, Bruce, and Goldman-Rakic 1989; Wilson and McNaughton 1993; Rolls, Robertson, and Georges-Francois 1995; Ben-Yishai, Lev Bar-Or, and Sompolinsky 1995; Zhang 1996; Seung 1996; Samsonovich and McNaughton 1997; Camperi and Wang 1998; Ermentrout 1998; Hansel and Sompolinsky 1998; Taube 1998; Deneve, Latham, and Pouget 1999; Wang 2001; Laing and Chow 2001; Wu, Amari, and Nakahara 2002; Stringer, Trappenberg, Rolls, and Aranjo 2002; Brody, Romo, and Kepecs 2003; Erlhagen 2003; Gutkin, Pinto, and Ermentrout 2003; Renart, Song, and Wang 2003; Trappenberg 2003; Folias and Bressloff 2004; Wu and Amari 2005; Chow and Coombes 2006; Miller 2006; Machens and Brody 2008; Wu, Hamaguchi, and Amari 2008). Compared with these studies, our contributions are that: 1) we elucidate the tracking behavior of a CANN in much more detail, particularly the effect of distortions in the network state on the tracking performance and the development of a perturbative method to systematically improve the description of the network dynamics; 2) we predict on the tracking behavior of a CANN, namely, the maximum speed for a moving stimulus to be trackable by the network (Hansel and Sompolinsky 1998) and the logarithmic nature of the reaction time; and 3) we also analytically investigate the tracking dynamics of a two-dimensional CANN. -2 -1 0 1 2 x-1 -0.5 0 0.5 1 1.5 u n R n = 0 n = 1 n = 2 n = 3 A U0 U1 B  
 
FULL TEXT: 
 ’s stationary states in the state space. This method allows us to approximate the network dynamics up to an arbitrary accuracy depending on the order of perturbation used. We quantify the distortions of a Gaussian bump during tracking, and study their effects on the tracking performance. Results are obtained on the maximum speed for a moving stimulus to be trackable and the reaction time for the network to catch up with an abrupt change in the stimulus. 1 1 Introduction The brain performs computation by updating its internal states according to dynamical rules that are determined by both the nature of external inputs and the structure of neural networks. A neural system acquires its network structure through either learning from experience, or inheriting from evolution or both. Its impact on network dynamics is twofold: on one hand, it determines the stationary states of the network that lead to associative memory; and on the other hand, it carves the landscape of the state space of the network as a whole, which may contribute to other cognitive functions, such as movement control, spatial navigation, population decoding and object categorization. Understanding how the dynamics of a neural network is shaped by its network structure is at the core of using mathematical models to elucidate brain functions (Dayan and Abbott 2001). Associative memories, such as those based on the Hopfield model (Hopfield 1984; Domany, van Hemmen, and Schulten 1995), have been intensively studied over the past decades. However, an equally important issue has not been investigated to the same extent, namely, how a large-scale structure of the state space of a neural system may facilitate brain functions. Recently, a type of recurrent network, called contin- uous attractor neural networks (CANNs), has received wide attention (Wilson and Cowan 1972; Amari 1977; Georgopoulos, Kalaska, Caminiti, and Massey 1982; Maunsell and Van Essen 1983; Funahashi, Bruce, and Goldman-Rakic 1989; Wilson and McNaughton 1993; Rolls, Robertson, and Georges-Francois 1995; Ben-Yishai, Lev Bar-Or, and Sompolinsky 1995; Zhang 1996; Seung 1996; Samsonovich and McNaughton 1997; Camperi and Wang 1998; Ermentrout 1998; Hansel and Sompolinsky 1998; Taube 1998; Deneve, Latham, and Pouget 1999; Wang 2001; Laing and Chow 2001; Wu, Amari, and Nakahara 2002; Stringer, Trappenberg, Rolls, and Aranjo 2002; Brody, Romo, and Kepecs 2003; Erlhagen 2003; Gutkin, Pinto, and Ermentrout 2003; Renart, Song, and Wang 2003; Trappenberg 2003; Folias and Bressloff 2004; Wu and Amari 2005; Chow and Coombes 2006; Miller 2006; Machens and Brody 2008; Wu, Hamaguchi, and Amari 2008). These models are particularly useful when modeling the encoding of continuous stimuli in neural systems. Different from other attractor models, CANNs have a distinctive feature that their neuronal interactions are translationally invariant. As a result, they can hold a family of stationary states that can be translated into each other without the need to overcome any barriers. In the continuum limit, these stationary states form a continuous manifold1 in which the system is neutrally stable, and 1A manifold is a mathematical concept. Simply stated, it is a set of points with a coordinate system. Here, each stationary state of the network is a point in the manifold. As will be shown later, these stationary states have the shape of a Gaussian bump, and their peak positions define the coordinates. 2 the network state can translate easily when the external stimulus changes continuously. Beyond pure memory retrieval, this large-scale structure of the state space endows the neural system with a tracking capability. An illustration of the landscape of the state space of a CANN and how it leads to neutral stability of the network stationary states is shown in Fig. 1. The tracking dynamics of a CANN has been theoretically studied by several authors in the lit- erature (see, e.g.,(Ben-Yishai, Lev Bar-Or, and Sompolinsky 1995; Zhang 1996; Samsonovich and McNaughton 1997; Xie, Hahnloser, and Seung 2002; Folias and Bressloff 2004; Wu and Amari 2005)). These stud- ies have demonstrated clearly that a CANN has the capacity of tracking a moving stimulus continuously and that this tracking property can describe many brain functions well. Detailed rigorous analyses of the tracking behaviors of a CANN are still lacking, however, including, for instance, 1) the conditions under which a CANN can track a moving stimulus successfully; 2) how the network state is distorted during the tracking; and 3) how these distortions affect the tracking performance of a CANN. In this study, we systematically investigate these issues. We use a simple, theoretically solvable model of a CANN as the working example. The sta- tionary states of the network have a Gaussian shape, and the network dynamics can be described using the basis functions of quantum harmonic oscillators. These analytical results allow us to explore the tracking performances of a CANN in much more detail than in previous studies based on simulation observations. In particular, we demonstrate clearly how the dynamics of a CANN is decomposed into different modes corresponding to distortions in the height, position, width or skewness of the network bump states. Their contributions to the tracking performance are de- termined by the amplitudes of the corresponding eigenvalues of the neuronal interaction kernel. For a CANN, neutral stability implies that the mode of positional shift dominates the network dynamics and that other modes have high-order contributions. From this property, we develop a time-dependent perturbative approach to simplify the network dynamics. Geometrically, this corresponds to projecting the network dynamics on its dominating motion modes. The solution of the perturbation method is expressed in a simple closed form, and we can approximate the network dynamics up to an arbitrary accuracy depending on the order of perturbation used. With this method, we explore two tracking performances of a CANN, namely, the condition under which the network can successfully track a moving stimulus and the reaction time for the network to catch up with sudden changes in the stimulus. Both properties are associated with the unique dynamics of a CANN and can be tested in practice. The idea of utilizing the special structure of a CANN to simplify its dynamics has also been 3 reported in other studies. Wu et al projected the dynamics of a CANN on its single dominating motion mode, the tangent of the attractor space, and simplify the network dynamics into a one- dimensional equation (Wu, Hamaguchi, and Amari 2008). Several other authors also proposed different approaches to simplifying the network dynamics (Ben-Yishai, Lev Bar-Or, and Sompolinsky 1995; Wennekers 2002; Renart, Song, and Wang 2003; Miller 2006). Compared with these studies, our contributions are that: 1) we elucidate the tracking behavior of a CANN in much more detail, particularly the effect of distortions in the network state on the tracking performance and the development of a perturbative method to systematically improve the description of the network dynamics; 2) we predict on the tracking behavior of a CANN, namely, the maximum speed for a moving stimulus to be trackable by the network (Hansel and Sompolinsky 1998) and the logarithmic nature of the reaction time; and 3) we also analytically investigate the tracking dynamics of a two-dimensional CANN. We expect that the mathematical framework developed in this work will have wide applications in the theoretical study of CANNs. The organization of the paper is as follows. In Section 2, the unique dynamics of a CANN is explored, and we demonstrate how a CANN is decomposed into different modes. In Section 3, a simple method of projecting the network dynamics onto its single primary motion mode, i.e., the tangent of the manifold formed by all stationary states, is used to investigate the tracking performance of a CANN. In Section 4, a perturbative approach is developed to systematically improve the description of the tracking dynamics of the network. The effects of the shape distortions of the network state on the tracking dynamics are described. In Section 5, the dynamics of a two-dimensional CANN is investigated. In Section 6, we present the discussion and conclusions. The mathematical details are presented in the appendices. Preliminary results have been published in (Fung, Wong, and Wu 2008). 2 Intrinsic Dynamics of a CANN Let us consider a one-dimensional continuous stimulus x encoded by an ensemble of neurons. For example, the stimulus may represent the direction of movement, the orientation or a general continuous feature of objects extracted by the neural system. We consider the range of possible values of the stimulus being much larger than the range of neuronal interactions. We can thus effectively take x ∈ (−∞,∞). Let U(x, t) be the synaptic input at time t to the neurons whose preferred stimulus is x, 4 -2 0 2 x 0 0.5 1 1.5 2 U (x) Figure 1: The energy landscape of the state space of a CANN. A canyon is formed by projecting the stationary states onto the subspace formed by δU1 and δU0, i.e., the position and the height distortions of the network states. Motion along the canyon corresponds to the positional shift of the bump (inset). and let r(x, t) be the firing rate of these neurons. r(x, t) increases with the synaptic input, but saturates in the presence of global activity-dependent inhibition. A solvable model that cap- tures these features is given by the divisive normalization (Deneve, Latham, and Pouget 1999; Wu, Amari, and Nakahara 2002): r(x, t) = U(x, t)2 1 + kρ ∫ −∞ −∞ dx ′U(x′, t)2 , (1) where ρ is the neural density and k is a small positive constant controlling the strength of global inhibition. The dynamics of the synaptic input U(x, t) is determined by the external input Iext(x, t), the network input from other neurons, and its own relaxation. It is given by τ ∂U(x, t) ∂t = Iext(x, t) + ρ ∫ ∞ −∞ dx′J(x, x′)r(x′, t)− U(x, t), (2) where τ is a time constant, which is typically on the order of 1 ms (Ermentrout 1998; Gutkin, Pinto, and Ermentrout 2003; Wu, Hamaguchi, and Amari 2008), and J(x, x′) is the neural interaction from x′ to x. The key character of CANNs is the translational invariance of their neural interactions. In our solvable model, we choose Gaussian interactions with a range a, namely, J(x, x′) = A√ 2pia exp [ −(x− x ′)2 2a2 ] , (3) 5 where A is a constant. CANNs with other neural interactions and inhibition mechanisms have been studied in the lit- erature (see, e.g., (Amari 1977; Ben-Yishai, Lev Bar-Or, and Sompolinsky 1995; Ermentrout 1998; Wang 2001)). Here, we choose this model for the advantage that it permits an analytical solu- tion of the network dynamics. Nevertheless, the final conclusions of our model are qualitatively applicable to general cases. We first consider the intrinsic dynamics of the CANN model in the absence of external stimuli. It is straightforward to check that, for 0 < k < kc ≡ A2ρ/(8 √ 2pia), the network holds a continuous family of stationary states, which are (see Fig. 2A) U˜(x|z) = U0 exp [ −(x− z) 2 4a2 ] , (4) r˜(x|z) = r0 exp [ −(x− z) 2 2a2 ] , (5) where U0 = [1 + (1 − k/kc)1/2]A/(4 √ piak) and r0 = [1 + (1 − k/kc)1/2]/(2 √ 2piakρ). These stationary states are translationally invariant among themselves and have a Gaussian shape with a free parameter z indicating their positions. z is therefore the coordinate of the manifold formed by all stationary states. The stability of the Gaussian bumps can be studied by considering the dynamics of fluctu- ations. Consider the network state U(x, t) = U˜(x|z) + δU(x, t). As derived in Appendix B, we have τ ∂ ∂t δU(x, t) = ∫ ∞ −∞ dx′F (x, x′|z)δU(x′, t)− δU(x, t), (6) where the interaction kernel F (x, x′|z) is given by F (x, x′|z) = 2ρU(x ′) B [ J(x, x′)− kρ ∫ ∞ −∞ dx′′J(x, x′′)r(x′′) ] . (7) We are interested in the eigenfunctions and eigenvalues of the kernel F (x, x′|z). Since the kernel is not symmetric with respect to the permutation of x and x′, we have to distinguish between its left and right eigenfunctions. To compute them, we need to choose a convenient set of basis functions. They should satisfy the boundary condition that they vanish when x approaches ±∞. A convenient choice is one that contains a basis function taking the shape of a Gaussian bump. Since the quantum ground state of the harmonic oscillator potential has a Gaussian shape, a natural choice of basis functions is the wave functions of quantum harmonic 6 oscillators (Griffiths 2004), namely, vn(x|z) = 1√ (2pi)1/2an!2n exp [ −(x− z) 2 4a2 ] Hn ( x− z√ 2a ) , (8) where Hn are the nth-order Hermite polynomials for n = 0, 1, 2, . . ., which can be expressed as the derivatives of the Gaussian function (Griffiths 2004) vn(x|z) = (−1) n( √ 2a)n−1/2√ pi1/2n!2n exp [ (x− z)2 4a2 ]( d dx )n exp [ −(x− z) 2 2a2 ] . (9) This choice of basis functions has the further advantage that an abundance of mathematical properties are available for our analysis, as summarized in Appendix A. Fig. 2B pictures the first four wave functions. We see that: • The ground state of the quantum harmonic oscillator, v0(x|z) = U˜(x|z) U0 √ (2pi)1/2a , (10) corresponds to the stationary state of the network. • The first excited state of the quantum harmonic oscillator, v1(x|z) = 2a U0 √ (2pi)1/2a ∂U˜(x|z) ∂z , (11) corresponds to the positional shift of the stationary state. • The second and third excited states, v2(x|z) and v3(x|z), respectively correspond to the distortions in the width and skewness of the bump. To express δU(x, t) in terms of the eigenfunctions vn(x|z) using Eq. (6), we consider F (x, x′|z) as an operator acting on vn(x|z). Since vn(x|z) is a basis, we have ∫ ∞ −∞ dx′F (x, x′|z)vn(x′|z) = ∑ m vm(x|z)Fmn, (12) where Fmn is the representation of the kernel F (x, x ′|z) in the basis vn(x|z), given by Fmn = ∫ ∞ −∞ dx ∫ ∞ −∞ dx′vm(x|z)F (x, x′|z)vn(x′|z). (13) 7 -2 0 2 x 0 0.5 1 1.5 2 U (x) A -2 -1 0 1 2 x-1 -0.5 0 0.5 1 v n v0 v1 v2 v3 B Figure 2: (A) The Gaussian-shaped stationary states of the network. (B) The first four basis functions of the quantum harmonic oscillators, representing distortions in the height, position, width and skewness of the Gaussian bump. As derived in Appendix B for the particular choice of the output function (1) and the neural interactions (3), the elements of the matrix Fmn are given by Fmn =   1−√1− k/kc, m = n = 0; 21−n √ n! m! (−1) n−m 2 2 n−m 2 ( n−m 2 ) ! , n−m being an even integer; 0, otherwise. (14) By using the matrix Fmn, we can calculate the eigenvalues and the first four right eigenfunc- tions of the kernel F (x, x′|z), which are (see Appendix B) λ0 = 1− √ 1− k/kc, (15) 8 λn = 2 1−n, for n ≥ 1, . . . , (16) uR0 (x|z) = v0(x|z), (17) uR1 (x|z) = v1(x|z), (18) uR2 (x|z) = √ 1/2 D0 v0(x|z) + 1− 2 √ 1− k/kc D0 v2(x|z), (19) uR3 (x|z) = √ 1 7 v1(x, z) + √ 6 7 v3(x, z), (20) where D0 = [( 1− 2 √ 1− k/kc )2 + 1/2 ]1/2 . (21) We note that uR1 (x|z) = v1(x|z) and λ1 = 1. Fig. 3A illustrates the first four right eigenfunctions of F . The right eigenfunctions of F correspond to the various distortion modes of the bump. For example, uR0 (x|z) corresponds to the amplitude distortion of the bump, and uR1 (x|z) to the positional shift. Starting from n = 3, uRn (x|z) are linear combinations of vk(x|z), where k = n, n − 2, . . .. Some of them may be dominated by a specific type of distortion, such as uR3 in Eq. (20) being dominated by the skewness. However, generally speaking, various distortion features have been coupled in the eigenfunctions of the interaction kernel. To avoid confusion, we continue to use vn(x|z) as our basis functions in the rest of the paper. To identify the contributions of uRn (x|z) to the network dynamics, we express δU(x, t) in (6) as a linear combination of uRn (x|z), namely, δU(x, t) = ∑ n δUn(z, t)u R n (x|z). Using the orthonormality of the left and right eigenfunctions of F (x, x′|z), we have δUn(z, t) = ∫ ∞ −∞ dxδU(x, t)uLn (x|z). (22) Assume that the motion of the bump is slow, so that dz/dt becomes negligible in Eq. (6); as we shall see, this assumption is valid as long as the external stimulus is sufficiently weak. Then, the projection of Eq. (6) on the eigenfunctions become τ d dt δUn(z, t) = (λn − 1)δUn(z, t). (23) Hence, δUn(z, t) = δUn(z, 0) exp [ −(1− λn)t τ ] , (24) where δUn(z, 0) is the initial value of the projection. Eq. (24) tells us that, since λ1 = 1, the bump distortion δU1, which corresponds to the positional shift of the bump, is sustained under the network dynamics. This is the mathematical 9 expression of the neutral stability of the stationary states of the network. The eigenvalues for all other eigenfunctions are all less than one. Since these other modes of bump distortions decay exponentially with the time constant τ/(1 − λn), their contributions on the network dynamics are small when compared with that due to the positional shift. An intuitive illustration of the decomposition of the network dynamics is given in Fig. 3B. -2 -1 0 1 2 x-1 -0.5 0 0.5 1 1.5 u n R n = 0 n = 1 n = 2 n = 3 A δU0 δU1 B Σ n>1 δUn Figure 3: (A) The first four right eigenfunctions of the interaction kernel F (x, x′|z) with k/kc = 1/2. (B) The distortions of the network state are decomposed into three parts, namely, distortions in the height, position and all other shapes. The fact that the dynamics of a CANN is dominated by only a few motion modes suggests that we may use this property to simplify the description of the network dynamics; that is, we can project the network dynamics onto its dominating modes. By this, we can analytically quantify the tracking bahviors of a CANN. 10 The Landscape of the State Space It is instructive to illustrate the energy landscape in the state space of a CANN. In the dynamical system that we are studying, we do not have a Lyapunov function. This is evident from the fact that the interaction matrix Fmn is not symmetric. Nevertheless, at each peak position z, one can define an effective energy function E(z) = ∑ n(1 − λn)[δUn(z)]2/2, with δUn(z) being the projection of U(x) − U˜(x|z) on the eigenfunction un(x|z). Then, the dynamics in Eq. (6) or (23) can be locally described by the gradient descent of E(z) in the space of δUn(z). E(z) has the minimum value of zero when δUn(z) = 0 for n 6= 1 and δU1(z) can take any value since λ1 = 1. This corresponds to the picture that the bump’s position shifts, but the shape of the bump remains unchanged. When z varies, the solution of E(z) = 0 traces a curve in the state space, facilitating the local gradient descent dynamics of the network, with the tangent of the curve at each point defining the direction of neutral stability. We can envisage this by a canyon surrounding the curve as shown in Fig. 1, and a small force along the tangent of the canyon can move the network state easily. The External Input To proceed, we need to define the external input. Without loss of generality, we choose Iext(x, t) = αU0 exp [ −(x− z0) 2 4a2 ] + ση(x, t), (25) where α and σ indicate the strength of the signal and the noise, respectively. η(x, t) is Gaussian white noise, satisfying 〈η(x, t)〉 = 0 and 〈η(x, t)η(x′, t′)〉 = δ(x − x′)δ(t − t′). The signal term has the effect of driving the bump to the stimulus location z0, and the noise term tends to shift the bump position randomly. Our results are qualitatively independent of the exact form of the external input. We may choose other forms for the external input. The main conclusions will be the same once the chosen external input has a unimodal shape and has the effect of driving the bump to the stimulus position. The Setting of the Simulation Experiments We will use simulation experiments to confirm the theoretical analysis. Here, we describe the setting of simulation experiments to be used below. The network consists of N neurons. Their preferred stimuli x are evenly distributed in the range (−pi, pi] and satisfy the periodic condition, namely, U(x, t) = U(x + 2pi, t). In the simulation, the integration in the continuum limit is 11 changed to the summation accordingly, i.e., ρ ∫ dx → ∑Ni=1. We choose a = 0.5 ≪ 2pi, so that comparison with the theoretical analysis for x ∈ (−∞,∞) holds. 3 Tracking Dynamics in the Weak Input Limit We consider first the case when the external input is sufficiently small, i.e., α≪ 1 and σ ≪ 1. In this case, we can reasonably assume that the network dynamics is dominated by the positional shift of the bump, and we can thus ignore other motion modes, that is, U(x, t) ≈ U˜(x|z(t)), with only the bump position z(t) varying with time. As will be shown in Section 4, this actually corresponds to n = 0 perturbation in the general perturbative method. After projecting the network dynamics, Eq. (2), on the basis functions v1(x|z(t)), we obtain (see Appendix C) dz dt = −α τ (z − z0) exp [ −(z − z0) 2 8a2 ] + √ 2Dη1(t), (26) where D = √ 2aσ2/(U20 τ 2√pi) is the diffusion constant, and η1(t) is the overlap of η(x, t) with v1(x|z(t)) given by η1(t) = ∫ ∞ −∞ dxη(x, t)v1(x|z(t)), (27) which satisfies 〈η1(t)〉 = 0 and 〈η1(t)η1(t′)〉 = δ(t − t′). (28) This equation represents a one-dimensional Ornstein-Uhlenbeck process for the motion of the bump position in the state space (Tuckwell 1988). Its meaning is straightforward: the first term on the right-hand side of the equation represents the contribution of the stimu- lus, whose effect is to pull the bump to the stimulus position z0; the second term represents noise, which tends to shift the bump position randomly. This equation agrees with Eq. (3.7) in (Wu, Hamaguchi, and Amari 2008), which considers |z − z0| ≪ a and hence e−(z−z0)2/8a2 ≈ 1. The distribution of the bump position can be calculated. The pull term in Eq. (26) can be written as the gradient of a potential, V (z) = −4a 2α τ exp [ −(z − z0) 2 8a2 ] , (29) so that Eq. (26) can be written as a Langevin equation, dz dt = −∂V ∂z + √ 2Dη(t). (30) 12 This implies that z satisfies a Boltzmann distribution (Ma 1976), P (z) ∼ exp [ −V (z) D ] . (31) Since the pull is maximum at z = z0 ± 2a, two noise regimes can be identified. In the low noise regime, where 〈(z − z0)2〉 ≪ 4a2, the bump position is effectively trapped in a parabolic potential, whereas in the high noise regime with 〈(z − z0)2〉 ≫ 4a2, the pull vanishes and the motion of the bump resembles a random walk with 〈[z(t) − z0]2〉 = 2Dt loosely coupled to the stimulus. However, simulation results show that Eq. (31) underestimates the variance of the bump position. This is because we have considered only the overlap of η(x, t) with v1(x|z(t)). As will be shown in the perturbative approach, overlaps with other components cannot be neglected. By using the simplified dynamics in Eq. (26), we explore two tracking behavior of the network, namely, the tracking of a moving stimulus and the catching up of an abrupt change in the stimulus. 3.1 The Condition for Successful Tracking Suppose that the stimulus is moving at a constant velocity v and the noise is low and negligible. The dynamical equation becomes identical to Eq. (26) after the transients, with z0 replaced by vt. Denoting the lag of the bump behind the stimulus as s = z0 − z, we have ds dt = dz0 dt − dz dt = v − αs τ exp [ − s 2 8a2 ] = v − g(s). (32) The size of the lag s is determined by two competing factors: the first term represents the movement of the stimulus, which tends to enlarge the lag; and the second term represents the collective effects of the neuronal recurrent interactions, which tends to shorten the lag. The tracking is achieved when these two forces match each other, i.e., v = g(s); otherwise, s diverges. Fig. 4A shows that the function g(s) is concave and has the maximum value of gmax = 2αa/(τ √ e) at s = 2a. This means that if v > gmax, the network is unable to track the stimulus. Thus, gmax defines the maximum trackable speed of a moving stimulus to the network. Notably, gmax increases with the strength of the external signal α and the range of neuronal recurrent interactions a. This is reasonable since it is the neuronal interactions that induce the movement 13 of the bump. gmax decreases with the time constant of the network τ , as this reflects the responsiveness of the network to external inputs. On the other hand, for v < gmax, there is a stable and unstable fixed point, denoted as s1 and s2 respectively, in Fig. 4A. When the initial lag is less than s2, it will converge to s1. Otherwise, the tracking of the stimulus will be lost. Intuitively, this means that the propagation of neuronal activities in the network needs to be quick enough to compensate for the initial lag behind the stimulus. Figs. 4B and 4C show that the analytical results of Eq. (32) agree well with the simulation results. 3.2 The Reaction Time Another tracking behavior we study is the response to a stimulus experiencing an abrupt change. We measure how long it takes for the network to catch up with this change, i.e., the reaction time. Suppose that the network has reached a steady state at t < 0, and the stimulus position jumps from 0 to z0 suddenly at t = 0. This is the typical setting in experiments studying mental rotation behavior (Georgopoulos, Taira, and Lukashin 1993; Ben-Yishai, Lev Bar-Or, and Sompolinsky 1995; Wu and Amari 2005). The reaction time can be obtained by integrating Eq. (26) with the initial value z(t = 0) = 0. When the noise is low and the jump size |z0| is much smaller than the range a of neuronal interactions, we can derive an analytical expression for the reaction time. In this case, Eq. (26) is approximated as τ dz dt ≈ α(z0 − z). (33) We define the reaction time T as the time it takes for the bump to move to a small threshold distance θ from the stimulus. T is calculated to be T ≈ τ α ln ( |z0| θ ) . (34) This reveals that with small jump sizes, the reaction time of a CANN increases logarithmically with the jump size. Fig. 5 compares the simulation results with our theoretical predictions based on Eq. (26). We see that with small jump sizes, they agree very well with each other, and the reaction time indeed has a logarithmic nature in its increase with the jump size. However, we also notice that with large jump sizes, there is considerable discrepancy between the simulation results and 14 the theoretical predictions. This is because we have only considered the positional shift in the network dynamics. To improve the results, more motion modes must be included. 4 Tracking Dynamics: A Comprehensive Study In this section, we develop a time-dependent perturbation approach to systematically improve the description of the network dynamics. Since the network dynamics is primarily dominated by the positional shift of the bump, with high-order distortions from other motion modes, we consider the network states with the following form, U(x, t) = U˜(x|z(t)) + ∞∑ n=0 an(t)vn(x|z(t)), (35) where the bump position z(t) and the coefficients {an(t)} specify the network state. The external input can be similarly expressed as Iext(x, t) = ∞∑ n=0 In(t)vn(x|z), (36) with In = ∫ dxIext(x, t)vn(x|z). By substituting Eqs. (35) and (36) into Eq. (2), and making use of the orthonormality and completeness of vn(x|z), we obtain the perturbative equation for an(t) as (see Appendix D) ( d dt + 1− λn τ ) an = In τ − [ U0 √ (2pi)1/2aδn1 + √ nan−1 − √ n+ 1an+1 ] 1 2a dz dt + 1 τ ∞∑ r=1 √ (n+ 2r)! n! (−1)r 2n+3r−1r! an+2r. (37) The center-of-mass position z(t) is determined from the self-consistent condition, z(t) = ∫∞ −∞ dxU(x, t)x∫∞ −∞ dxU(x, t) . (38) By combining Eqs.(37) and (38) and denoting the product of the positive integers n, n − 2, . . . as n!!, we obtain an equation for the bump velocity (see Appendix D), dz dt = 2a τ I1 + ∑∞ n=3,odd √ n!! (n−1)!!In + a1 U0 √ (2pi)1/2a+ ∑∞ n=0,even √ (n−1)!! n!! an . (39) Eqs. (37) and (39) are the master equations of the perturbative method. For the perturbation order specified by n, we include the coefficients from a0 up to an, and the input components 15 from I0 up to In, and at least I1, which is the lowest order of input that drives the dynamics. By choosing different orders of perturbation, we can approximate the network dynamics to different accuracies. The components of the external input in vn(x|z) are calculated to be (see Appendix E) In(t) = αU0 exp [ −(z0 − z) 2 8a2 ]( z0 − z 2a )n√(2pi)1/2a n! + σηn(t), (40) where the noise term ηn(t) satisfies 〈ηn(t)〉 = 0 and 〈ηm(t)ηn(t′)〉 = δmnδ(t − t′). 4.1 The Weak Input Limit We first compare the perturbative approach with the weak input limit by ignoring all distortion modes of the bump; that is, we set an = 0 for all n. Due to the readiness of the bump to shift its position, the lowest order of the external stimulus to be included is I1 according to Eq. (39). Thus, in the n = 0 perturbation, Eq. (39) becomes dz dt = 2a τ I1 U0 √ (2pi)1/2a . (41) After substituting I1 from Eq. (40) into Eq. (41), we obtain Eq. (26), which is equivalent to the network dynamics in the limit of weak inputs. 4.2 The n = 0 Perturbation We further consider the n = 0 perturbation. This amounts to taking into account the effects of the change in the bump height on the network dynamics, apart from the positional shift. Perturbation up to n = 0 yields ( d dt + 1− λ0 τ ) a0 = αU0 τ √ (2pi)1/2a exp [ −(z0 − z) 2 8a2 ] + η0 τ , (42) dz dt = αU0 √ (2pi)1/2a(z0 − z) exp [ − (z0−z)28a2 ] + 2aση1 τ(U0 √ (2pi)1/2a+ a0) . (43) With the initial condition a0 = 0 at t = 0, the solution of a0 from Eq. (42) reduces to a0(t) = ∫ t 0 dt′ τ exp [ −1− λ0 τ (t− t′) ]{ αU0 √ (2pi)1/2a exp [ −(z0 − z) 2 8a2 ] + η0(t ′) } . (44) Substituting Eq. (44) into Eq. (43), we have dz dt = { α τ (z0 − z) exp [ −(z0 − z) 2 8a2 ] + √ 2Dη1 } R(t)−1, (45) 16 where R(t) = 1 + α ∫ t 0 dt′ τ exp [ −1− λ0 τ (t− t′)− (z0 − z(t ′))2 8a2 ] + ξ0(t) U0 √ (2pi)1/2a , (46) and ξ0(t) is a stochastic variable given by ξ0(t) = σ ∫ t 0 dt′ τ exp [ −(1− λ0)(t− t ′) τ ] η0(t ′). (47) R(t) can be interpreted to be the ratio of the bump height in the presence of the external stimulus to its weak input limit. Compared with Eq. (26), Eq. (45) has an extra term R(t)−1. R(t)−1 approaches 1 in the limit of weak inputs, but, in general, R(t)−1 < 1, implying that the increase in the amplitude of the bump has the effect of slowing down the tracking dynamics. In the following two applications, we consider the low noise limit with σ = 0. 4.2.1 Tracking a Moving Stimulus Similiar to the derivation of Eq. (32), we can determine the dynamical equation for the bump lag s after the transient period. From Eq. (45), ds dt = v − αs τ exp ( − s 2 8a2 ) R(t)−1, (48) where R(t) is given by R(t) = 1 + α ∫ t −∞ dt′ τ exp [ −1− λ0 τ (t− t′)− s(t ′)2 8a2 ] . (49) If the stimulus is not too weak, s reaches its steady state value in a short duration. The integral can then be computed precisely, yielding R(t) = 1 + α 1− λ0 exp [ − s 2 8a2 ] . (50) Compared with Eq. (32), Eq. (48) has a correction term, R(t)−1 < 1, representing the effect of the bump height distortion. 4.2.2 Tracking an Abrupt Change in the Stimulus The dynamical equation in this case is identical to Eq. (45) with the initial value z(t = 0) = 0. To compute the correction term R(t)−1, however, we need to consider that the bump height is initially at a steady value determined by the steady-state response of the network to the stimulus 17 at z = 0. This means that Eq. (44) needs to be replaced by a0(t) = αU0 √ (2pi)1/2a 1− λ0 exp [ −(1− λ0)t τ ] + αU0 √ (2pi)1/2a ∫ t 0 dt′ τ exp [ −1− λ0 τ (t− t′)− (z0 − z) 2 8a2 ] , (51) and the factor R(t) in Eq. (45) is replaced by R(t) = 1 + α 1− λ0 exp [ −(1− λ0)t τ ] + α ∫ t 0 dt′ τ exp [ −1− λ0 τ (t− t′)− (z0 − z(t ′))2 8a2 ] . (52) Indeed, R(t) represents the change in height during the movement of bump. The second and third terms respectively contribute to the heights near the initial and final positions (see Fig. 5A). Fig. 6 compares the simulation results and the theoretical predictions based on the pertur- bation approximation of Eqs. (45) and (52). We see that compared with the weak input limit, the result is improved a little bit. 4.3 The n = 5 Perturbation By increasing the order of perturbation, we get an increasingly better approximation of the network dynamics. We observe that the improvement is btter when the order of perturbation n increases by 1 to an odd integer. This is because odd distortion modes are antisymmetric, which better approximate the bump’s shape distortion when the position of the external stimulus is biased towards one side of the bump. Below, we present the results of the n = 5 perturbation, which correspond to taking into account distortions in the position, height, width, skewness and the fourth- and fifth-order distortions of the bump. By including perturbations up to n = 5, Eqs. (37) and (39) become ( d dt + 1− λ0 τ ) a0 = I0 τ + a1 2a dz dt − √ 2 4τ a2 + √ 6 32τ a4, (53) da1 dt = I1 τ − (√ (2pi)1/2aU0 + a0 − √ 2a2 ) 1 2a dz dt − √ 6 8τ a3 + √ 30 64τ a5, (54)( d dt + 1 2τ ) a2 = I2 τ − √ 2a1 − √ 3a3 2a dz dt − √ 3 8τ a4, (55)( d dt + 3 4τ ) a3 = I3 τ − √ 3a2 − √ 4a4 2a dz dt − √ 5 16τ a5, (56)( d dt + 7 8τ ) a4 = I4 τ − √ 4a3 − √ 5a5 2a dz dt , (57) 18 dz dt = 2a ( I1 + √ 3 2I3 + √ 15 8 I5 + a1 ) τ (√ (2pi)1/2aU0 + a0 + √ 1 2a2 + √ 3 8a4 ) , (58) and a5 is determined by the condition a1 + √ 3/2a3 + √ 15/8a5 = 0. Fig. 7 compares the theoretical predictions with the simulation results. They agree very well even when the jump sizes are large. 4.4 Shape Distortion Besides the reaction time, it is instructive to study the distortion in the bump’s shape during tracking. As shown in Fig. 8, there is an abrupt change in the height, position, width and skewness immediately after the external stimulus is abruptly removed at t = 0. a0(t) approaches 0 roughly, as if there were no external stimulus, and there is even a slight overshoot. a1(t) also drops abruptly, showing that the peak position lags behind the center of mass, an indication that the bump is abruptly distorted. At the same time, distortions in the width and skewness increase abruptly, showing that the bump is suddenly pulled by the external stimulus from its new position. After the initial abrupt changes, the distorted components undergo rather smooth changes on the journey to the new position. Finally, when the bump approaches its destination, the height returns to its equilibrium value, while the other components approach 0. We also compare the perturbation and simulation results in Fig. 8. The agreement is very good. Discrepancies arise from linearizing the interaction term in going from Eq. (2) to (6). Their effects are mainly seen in the moments immediately after the abrupt change in the external stimulus. Figure 9 shows the distorted bump in the center-of-mass frame during tracking. During the process, the lowering of the height, the retarding of the peak position, the broadening of the width, and the skewing due to the attraction of the newly positioned stimulus are clearly visible. 5 Dynamics of a Two-Dimensional CANN We can extend our analysis to a two-dimensional(2D) CANN. Consider a neural ensemble encoding a 2D continuous stimulus x = (x1, x2), with its components x1 ∈ (−∞,∞) and x2 ∈ (−∞,∞). We denote by U(x) the synaptic input to neurons having the preferred stimulus 19 x and r(x) the corresponding firing rate. For a solvable model, we consider the network whose dynamics is governed by τ ∂U(x, t) ∂t = Iext(x, t) + ρ ∫ ∞ −∞ ∫ ∞ −∞ dx′J(x,x′)r(x′, t)− U(x′, t), (59) r(x, t) = U(x, t)2 1 + kρ ∫∞ −∞ ∫∞ −∞ dx ′U(x′, t)2 , (60) where τ is the time constant and ρ the neural density, and the recurrent interactions are chosen to be J(x,x′) = A 2pia2 exp [ −(x− x ′)2 2a2 ] , (61) where (x− x′)2 = (x1 − x′1)2 + (x2 − x′2)2 is the Euclidean distance between x and x′. It can be checked that when Iext(x, t) = 0 and 0 < k < kc ≡ A2ρ/(32pia2), the network holds a continuous family of stationary states given by U˜(x|z) = U0 exp [ −(x− z) 2 4a2 ] , (62) r˜(x|z) = r0 exp [ −(x− z) 2 2a2 ] , (63) where U0 = [1+ (1− k/kc)1/2]A/(8pia2k) and r0 = [1+ (1− k/kc)1/2]/(4pia2kρ). The parameter z = (z1, z2) is a free variable, indicating the location of the bump. We note the differences in J(x,x′), kc, U0 and r0 when compared with their values in the 1D case. We analyze the stability of the bump states, and consider the network state U(x, t) = U˜(x|z) + δU(x, t). After linearizing Eq. (59) at U˜(x|z), we get τ ∂ ∂t δU(x, t) = ∫ ∞ −∞ ∫ ∞ −∞ dx′F (x,x′|z)δU(x′, t)− δU(x, t), (64) where the interaction kernel is given by F (x,x′|z) = 2 a2pi exp [ −(x− x ′)2 2a2 ] exp [ −(x ′ − z)2 4a2 ] , − 1 + √ 1− k/kc 2pia2 exp [ −(x− z) 2 4a2 ] exp [ −(x ′ − z)2 4a2 ] . (65) We can construct the right eigenfunction of F (x,x′|z) by using the product of the eigenfunctions un(x|z) in the 1D case, i.e., um,n(x|z) = um(x1|z1)un(x2|z2), m, n = 0, 1, 2, . . . . (66) 20 The corresponding eigenvalues are given by (see Appendix F) λ0,0 = λ0 (67) λm,0 = λm, m 6= 0, (68) λ0,n = λn, n 6= 0, (69) λm,n = λmλn, m 6= 0, n 6= 0, (70) where λm are the eigenvalues in the 1D case given by Eqs. (15) and (16). The eigenfunction u0,0(x|z) corresponds to the amplitude mode of the 2D bump state of the network. u1,0(x|z) corresponds to the displacement of the bump position along the coordinate z1 and u0,1(x|z) the positional shift along z2. A linear combination of u1,0(x|z) and u0,1(x|z), i.e., c1u1,0(x|z)+c2u0,1(x, z), with c1 and c2 being constants and satisfying c21+c22 = 1, corresponds to the positional shift of the bump along the c = (c1, c2) direction. Interestingly, the eigenvalue for c1u1,0(x|z)+c2u0,1(x, z) is 1, indicating that the bump is neutrally stable in the two-dimensional space z. Analogous to the 1D case, we can develop a perturbation method to solve the network dynamics, e.g., by using the 2D wave functions, vm,n(x|z) = vn(x1|z1)vn(x2|z2), of the quantum harmonic oscillators as the basis functions. A more comprehensive treatment of the problem involves transforming the basis functions to polar coordinates; it will be presented elsewhere. Here, as an illustration, we will present only the result for the weak input limit in the rectilinear coordinates, which corrsponds to projecting the network dynamics on a single displacement mode. We choose the external input to be Iext(x, t) = αU0 exp [ −(x− z 0)2 4a2 ] + ση(x, t), (71) where z0 represents the stimulus location. Suppose the bump position is z(t) at time t. Then, the stimulus will pull the bump towards the location z0. In the limit of weak inputs, the network dynamics is dominated by the positional shift of the bump along the straight line through z0. As derived in Appendix F, d(z− z0) dt = α τ (z− z0) exp [ −(z− z 0)2 8a2 ] + √ 2Dη(t), (72) where D = σ2/(U20 τ 2pi). This equation describes how the distance between the bump and the stimulus varies with time. It has the same form as Eq. (26) except that z − z0 is replaced by z − z0. Thus, the network exhibits the same dynamical properties as observed in the 1D case. 21 Fig. 10 compares the theoretical predictions and the simulation results of the reaction time of the 2D CANN. As expected, they agree with each other very well with small jump sizes. 6 Discussion and Conclusions The idea that the landscape of the energy function of a CANN has a canyon structure, which fa- cilitates the tracking capacity of a neural system has long been recognized (see, e.g., (Amari 1977; Hansel and Sompolinsky 1998; Brody, Romo, and Kepecs 2003; Wu and Amari 2005)). How- ever, a detailed and rigorous analysis of this property has been lacking. The main challenge here is to solve the network dynamics analytically, elucidating how the shape of the network state is affected during the tracking. In this study, by using a simple analytically solvable model, we systematically investigate this issue. There are two parts to our main results. First, we show that the dynamics of a CANN can be decomposed into different motion modes, corresponding to distortions in the height, position, width, skewness and other higher-order distortions in the shape of the bump states. We demonstrate that with a CANN, it is the positional shift that dominates the network dynamics, with other motion modes having high-order contributions. Second, we develop a time-dependent perturbation approach to approximate the network dy- namics. Geometrically, this corresponds to projecting the network dynamics onto its dominating motion modes. Simulation results confirm that our method works very well. Two interesting phenomena in the tracking performance of a CANN are observed, namely, the maximum track- able speed and the logarithmic reaction time of the network. Since both properties are associated with the unique dynamics of a CANN, they may be tested in practice and serve as clues for checking the application of a CANN in neural systems. To facilitate an analytical solution and hence to describe the network dynamics clearly, we have used a simple firing-rate-based model with the inhibition effect in the form of divisive normalization, and the generality of our results may thus be a concern. In particular, we should consider whether the mechanism of divisive normalization is plausible, whether tracking behavior can be observed in models with different inhibitory mechanisms, and how the perturbation method developed in this paper can be used in other types of networks. First, we argue that a neural system can have resources to implement divisive normal- ization (Grossberg 1988; Heeger 1993; Deneve, Latham, and Pouget 1999). Let us consider a network in which all excitatory neurons are connected to a pool of inhibitory neurons. If the 22 time constant of inhibitory neurons is much smaller than that of excitatory neurons, and if these inhibitory neurons inhibit the activity of the excitatory neurons in a uniform shunting way, then the effect of divisive normalization can be achieved. Second, even if there exists uncertainty of the model, the main conclusions of tracking be- havior in this work are applicable to general cases. This is because our calculation is based on the fact that the dynamics of a CANN is largely dominated by the positional shift of the network states, a property coming from the translational invariance of the neuronal interactions, rather than from the inhibition mechanism. To see this point clearly, let us consider the dynamics of a network having the same form as Eq. (2), but the exact form of the inihibition mechansim remains unspecified. We consider J(x− x′) and r(x) that are properly chosen (e.g., J(x− x′) is of the Mexcian-hat shape and r(x) a sigmoid function of the inputs), so that the network holds a continuous family of stationary states, denoted as U˜(x|z) and r˜(x|z), with z representing the bump position. This implies that U˜(x|z) = ρ ∫ ∞ −∞ dx′J(x− x′)r˜(x′|z). (73) By differentiating both sides of the above equation with respect to z, we get ∂U˜(x|z) ∂z = ρ ∫ ∞ −∞ dx′ ∫ ∞ −∞ dx′′J(x− x′) ∂r˜(x ′|z) ∂U˜ (x′′|z) ∂U˜(x′′|z) ∂z = ∫ ∞ −∞ dx′′F (x, x′′|z)∂U˜ (x ′′|z) ∂z . (74) Thus, the mode of the positional shift of the bump, ∂U˜ (x|z)/∂z, is the eigenfunction of the interaction kernel F with the eigenvalue equal to 1. We should observe similar tracking behavior of the network, once J(x−x′) and r(x) are properly chosen to ensure that all the other eigenvalues are less than one. Third, the perturbation method developed in this paper can be used to analyze other types of networks with tracking behavior. Consider appropriate choices of J(x−x′) and r(x) that can hold a continuous family of bump states centered at position z (for example, J(x − x′) having a Mexican-hat shape and r(U) a sigmoid function). Then, the dynamics of the bump can be described by Eq. (6), with F (x, x′|z) = ∫ ∞ −∞ dx′J(x− x′)∂r(U˜ (x ′|z)) ∂U˜(x′|z) . (75) Using the basis functions of the quantum harmonic oscillator, we can obtain the matrix elements Fmn from Eq. (13). While elegant expressions such as those in Eq. (14) may not be available in 23 the general case, we can nevertheless compute Fmn numerically. Perturbation dynamics similar to Eqs. (37) and (39) can be worked out following Appendix D. Indeed, we have considered networks with Mexican-hat-shaped interactions. Tracking dynamics with a logarithmic reaction time and a maximum trackable speed is observed, and the convergence of the perturbation method is equally remarkable. These results will be reported elsewhere. The tracking dynamics of a CANN has also been studied by other authors. In particu- lar, Zhang proposed a mechanism of using asymmetrical recurrent interactions to drive the bump, so that the shape distortion would be minimized (Zhang 1996). Xie et al further pro- posed a double ring network model that uses the inputs from the vestibular system in mammals to achieve these asymmetrical interactions (Xie, Hahnloser, and Seung 2002). This mechansim works well for the head-direction system, but it is unclear if it can be achieved in other neural systems. For instance, in the visual and hippocampal systems, it is often assumed that the bump movement is directly driven by external inputs (see, e.g., (Samsonovich and McNaughton 1997; Berry II, Brivanlou, Jordon, and Meister 1999; Fu, Shen, and Dan 2001)), and the distortion of the bump is inevitable (indeed the bump distortions in (Berry II, Brivanlou, Jordon, and Meister 1999; Fu, Shen, and Dan 2001) are associated with visual perception). The contribution of this study is that we quantify how the distortion of the bump’s shape affects the network tracking per- formance, and we obtain the maximum trackable speed of the network. We hope that this study will enrich our knowledge about the tracking dynamics of a CANN, which may differ in different brain functions. Apart from the tracking capacity, recent experimental observa- tions have suggested that CANNs may have other important roles in neural cognitive func- tions. For example, the highly structured state space of a CANN may provide a neural ba- sis of encoding the categorization relationships of objects (Jastorff, Kourtzi, and Giese 2006; Graf, Wichmann, Bulthoff, and Scholkopf 2006). It is quite possible that the distance between two memory states in the canyon defines the perceptual similarity between the two objects. We will explore these issues in the future, and we expect that the theoretical model and approaches developed in this work will provide valuable tools. Acknowledgment This work was partially supported by the Research Grants Council of Hong Kong (Grant Nos. HKUST 603606, 603607 and 604008), and BBSRC (BB/E017436/1) and the Royal Society of 24 UK. Appendix A: The Hermite Polynomials Since the Hermite polynomials are frequently used in the calculations of this paper, here we summarize their main properties for the convenience of the reader. The nth-order Hermite polynomial is defined as Hn(x) = (−1)n exp(x2) ( d dx )n exp(−x2). (76) The first five Hermite polynomials are given by H0(x) = 1, H1(x) = 2x, H2(x) = 4x 2 − 2, H3(x) = 8x 3 − 12x, H4(x) = 16x4 − 48x2 + 12. For the analysis in this paper, it is instructive to note that the Hermite polynomials satisfy the following properties. • Recursion relation: Hn+1(x) = 2xHn(x)−H ′n(x), (77) H ′n(x) = 2nHn−1(x), (78) Hn(x+ y) = n∑ k=0   n k  Hk(x)(2y)n−k. (79) • Contour integral representation: Hn(x) = n! ∮ dt 2piitn+1 e2tx−t 2 , (80) with the contour encircling the origin. Appendix B: The Interaction Kernel F (x, x′|z) Linearizing Eq. (2) at U˜(x|z), we arrive at τ ∂ ∂t δU(x) = ρ ∫ ∞ −∞ dx′J(x, x′) [ 2U(x′) B δU(x′) − U(x ′)2 B2 ∫ ∞ −∞ dx′′2kρU(x′′)δU(x′′) ] − δU(x), (81) where we have dropped the dependence on t for brevity, and B = 1 + kρ ∫∞ −∞ dx ′U˜(x′)2 = ρAU0/ √ 2. Interchanging the dummy variables x′ and x′′ in the last term, we obtain Eqs. (6) 25 and (7), with F (x, x′|z) = 2 a √ pi exp [ −(x− x ′)2 2a2 ] exp [ −(x ′ − z)2 4a2 ] − 1 + √ 1− k/kc√ 2pia exp [ −(x− z) 2 4a2 ] exp [ −(x ′ − z)2 4a2 ] . (82) By substituting the expressions of U(x), J(x, x′) and r(x), we get Eq. (7). To calculate the matrix elements Fmn in Eq. (13), we denote gn(x|z) = ∫ ∞ −∞ dx′F (x, x′|z)vn(x′|z). (83) From Eqs. (7) and (8), gn(x|z) = (−1) n2( √ 2a)n− 1 2 a √ pi3/2n!2n ∫ ∞ −∞ dx′ exp [ −(x− x ′)2 2a2 ]( d dx′ )n exp [ −(x ′ − z)2 2a2 ] −(1 + √ 1− k/kc)(−1)n( √ 2a)n− 1 2 a √ pi3/2n!2n+1 exp [ −(x− z) 2 4a2 ] × ∫ ∞ −∞ dx′ ( d dx′ )n exp [ −(x ′ − z)2 2a2 ] . (84) The second term vanishes for n ≥ 1. Hence, by evaluating the two integrals explicitly for n = 0, we have g0(x|z) = (1− √ 1− k/kc)v0(x|z). (85) Making use of the orthonormality of the basis vn(x|z), we obtain the first line of Eq. (14). Next, we consider the case n ≥ 1, focusing on the first term. Utilizing the identity (−d/dz)ne−(x−z)2/2a2 = (d/dx)ne−(x−z)2/2a2 , we have gn(x|z) = (−1) n2( √ 2a)n− 1 2 a √ pi3/2n!2n ( − d dz )n ∫ ∞ −∞ dx′ exp [ −(x− x ′)2 2a2 ] exp [ −(x ′ − z)2 2a2 ] . (86) Evaluating the integral explicitly, we find that gn(x|z) = (−1) n2( √ 2a)n− 1 2√ pi1/2n!2n ( d dx )n exp [ −(x− z) 2 4a2 ] . (87) We are now ready to derive Fmn from gn(x|z): Fmn = ∫ ∞ −∞ dxvm(x|z)gn(x|z). (88) From Eqs. (8) and (86), Fmn = 2 (−1)m(√2a)m− 12√ pi1/2m!2m (−1)n(√2a)n− 12√ pi1/2n!2n × ∫ ∞ −∞ dx exp [ (x− z)2 4a2 ]( d dx )m exp [ −(x− z) 2 2a2 ]( d dx )n exp [ −(x− z) 2 4a2 ] . (89) 26 Using the definition of the Hermite polynomials in Eq. (76), this simplfies to Fmn = 2 (−1)m(√2a)m+n−1√ pim!n!2m+n 1 (2a)n ∫ ∞ −∞ dxHn ( x− z 2a )( d dx )m exp [ −(x− z) 2 2a2 ] . (90) Applying integration by parts m times yields Fmn = 2 ( √ 2a)m+n−1√ pim!n!2m+n 1 (2a)n ∫ ∞ −∞ dx exp [ −(x− z) 2 2a2 ]( d dx )m Hn ( x− z 2a ) . (91) Applying Eq. (78) m times yields, for n ≥ m, Fmn = 2 ( √ 2a)m+n−1√ pim!n!2m+n 1 (2a)n n! am(n−m)! ∫ ∞ −∞ dx exp [ −(x− z) 2 2a2 ] Hn−m ( x− z 2a ) , (92) and Fmn = 0 otherwise. Introducing the contour integral representation in Eq. (80) leads to Fmn = √ 2 √ n! 2na √ pim! ∫ ∞ −∞ dx exp [ −(x− z) 2 2a2 ] ∮ dt 2piitn−m+1 exp [ −t2 + t ( x− z a )] . (93) Interchanging the order of integrating x and t, we have Fmn = 2 1−n √ n! m! ∮ dt 2piitn−m+1 exp ( − t 2 2 ) . (94) According to the residue theoreom in complex analysis, the contour integral can be reduced to the coefficient of tn−m in the Laurent expansion of exp(−t2/2), which is equal to (−1/2)(n−m)/2/[(n− m)/2]! when n −m is even and 0 otherwise. Hence, we have derived Fmn in the second line of Eq. (14). Other than the above cases, all other elements Fmn are zero. Hence we have derived Eq. (14) completely. As an illustration, we write down the elements of F in the first four rows and columns, F =   1−√1− k/kc 0 −√24 0 . . . 0 1 0 − √ 6 8 . . . 0 0 12 0 . . . 0 0 0 14 . . . . . . . . . . . . . . . . . .   . (95) Since all elements of the matrix F in the lower half of the diagonal line vanish, the eigenvalues of F are given by the diagonal elements, namely, Eqs. (15) and (16). It is straightforward to check that the first four right eigenvectors of Fmn are given by q0 = (1, 0, 0, . . .) T , (96) 27 q1 = (0, 1, 0, . . .) T , (97) q2 = ( √ 1/2 D0 , 0, 1− 2√1− k/kc D0 , 0, . . .)T , (98) q3 = (0, √ 1 7 , 0, √ 6 7 , 0, . . .)T , (99) where D0 is given in Eq. (21) and all the omitted elements are zero. The right eigenfunctions of F (x, x′|z), with eigenvalues λn, are given by uRn (x|z) = ∑ l qnlvl(x|z). (100) This can be verified by considering the inner product ∫ ∞ −∞ dx′F (x, x′|z)uRk (x′|z) = ∫ ∞ −∞ dx′ ∑ mn vm(x|z)Fmnvn(x′|z) ∑ l qklvl(x ′|z). (101) Making use of the orthonormality of vl(x ′|z), we have ∫ ∞ −∞ dx′F (x, x′|z)uRk (x′|z) = ∑ ml vm(x|z)Fmlqkl. (102) Since qk is an eigenvector of F with eigenvalue λk,∫ ∞ −∞ dx′F (x, x′|z)uRk (x′|z) = λk ∑ m vm(x|z)qkm = λkuRk (x|z). (103) The first four right eigenfunctions of F are presented in Eqs. (17)-(20). Appendix C: The Network Dynamics in the Weak Input Limit We let U(x, t) = U˜(x|z(t)) and multiply both sides of Eq. (2) by v1(x|z(t)), and then we integrate over x. Since ∂U˜(x|z(t)) ∂t = U0 √ (2pi)1/2a 2a dz dt v1(x|z(t)), (104) the left-hand side of the resultant equation becomes L.H.S. = τU0 √ (2pi)1/2a 2a dz dt . (105) On the right hand side of Eq.(2), the first term arises from the external input, consisting of both signal and noise components according to Eq. (25). The signal component, when projected onto the component v1(x|z(t)), leads to R.H.S.signal = αU0√ (2pi)1/2a ∫ ∞ −∞ dx exp [ −(x− z0) 2 4a2 ]( x− z a ) exp [ −(x− z) 2 4a2 ] . (106) 28 Explicit integration yields R.H.S.signal = αU0 √ (2pi)1/2a z0 − z 2a exp [ −(z0 − z) 2 8a2 ] . (107) The noise component leads to R.H.S.noise = ση1(t), (108) where η1 is given in Eq. (27) and satisfies Eq. (28). The second and third terms of Eq. (2) vanish owing to the fact that U˜(x|z(t)) is the stationary state solution of Eq. (2) in the weak input limit, independent of the bump position z(t). Combining the above results, we get Eq. (26). Appendix D: The Perturbation Method D.1 The Perturbation Equation Using Eq. (8), we have d dt vn(x|z) = 1√ (2pi)1/2an!2n exp [ −(x− z) 2 4a2 ] [ x− z 2a2 Hn ( x− z√ 2a ) − 1√ 2a H ′n ( x− z√ 2a )] dz dt . (109) By virtue of Eq. (77) and (78), d dt vn(x|z) = [√ n+ 1vn+1(x|z)− √ nvn−1(x|z) ] 1 2a dz dt . (110) Similarly, since U˜(x|z) = U0 √ (2pi)1/2av0(x|z), ∂ ∂t U˜(x|z) = U0 √ (2pi)1/2av1(x|z) 1 2a dz dt . (111) The neuronal interaction and relaxation terms cancel each other at the steady state. Hence, in determining the dynamics, only the linearized terms need to be considered, ρ ∫ ∞ −∞ dx′J(x, x′)r(x′|z)− U(x|z) ≈ ∑ n an ∑ m vm(x|z)Fmn − ∑ n anvn(x|z). (112) Substituting Eqs. (110-112) into Eq. (2), we obtain τ ∑ n dan dt vn + τ 2a { U0 √ (2pi)1/2av1 + ∑ n an [√ n+ 1vn+1 − √ nvn−1 ]} dz dt = ∑ n an ∑ m vmFmn − ∑ n anvn + ∑ n Invn. (113) 29 Making use of the orthonormality and completeness of vn(x|z), the coefficients in Eq. (113) can be equated term by term. Hence, we get τ dan dt + τ 2a [ U0 √ (2pi)1/2aδn1 + √ nan−1 − √ n+ 1an+1 ] dz dt = ∑ k Fnkak − an + In. (114) By further using Eq. (14), the above equation is re-organized as Eq. (37). D.2 The Velocity Equation The peak position z(t) of the bump is determined from the self-consistent condition in Eq. (38). Without loss of generality, we consider z = 0 and substitute the expression of vn(x|z) into Eq. (38), ∑ n=1,odd an√ n!2n ∫ ∞ −∞ dx exp ( − x 2 4a2 ) xHn ( x√ 2a ) = 0. (115) Integrating by parts and using the identity in Eq. (78), we have ∑ n=1,odd nan√ n!2n ∫ ∞ −∞ dx exp ( − x 2 4a2 ) Hn−1 ( x√ 2a ) = 0. (116) The integral can be performed by using the contour integral representation of the Hermite polynomial in Eq. (80), ∑ n=1,odd n!an√ n!2n ∫ ∞ −∞ dx ∮ dt 2piitn exp ( −t2 − x 2 4a2 + 2tx√ 2a ) = 0. (117) Interchanging the order of integration yields ∑ n=1,odd n!an√ n!2n ∮ dt 2piitn exp(t2) = 0. (118) The contour integral is equal to the coefficient of tn−1 in the series expansion of exp(t2), which is [(n − 1)/2]!. Hence, ∑ n=1,odd √ n!! (n− 1)!!an = 0. (119) Multiplying both sides of Eq. (37) by √ n!!/(n − 1)!!, and summing over odd n, the time deriva- tives of an disappear, leaving behind 0 = 1 τ ∑ n=1,odd √ n!! (n− 1)!!In −  U0√(2pi)1/2a+ ∑ n=1,odd √ n!! (n− 1)!! (√ nan−1 − √ n+ 1an+1 ) 1 2a dz dt + 1 τ ∑ n=1,odd √ n!! (n − 1)!! ∞∑ r=0 √ (n+ 2r)! n! (−1)r 2n+3r−1r! an+2r. (120) 30 Collecting terms in the summations, we have U0 √ (2pi)1/2a+ ∑ n=0,even [√ (n+ 1)!! (n)!! √ n+ 1− √ (n− 1)!! (n− 2)!! √ n ] an   12a dzdt = 1 τ ∑ n=1,odd √ n!! (n− 1)!! (In−an)+ 1 τ ∑ m=1,odd √ m!  (m−1)/2∑ r=0 (−1)r 2m+r−1(m− 2r − 1)!!r!   am.(121) Note that for odd m, (m− 2r − 1)!! = 2(m−2r−1)/2[(m− 1)/2 − r]!, so that the summation over r in the last term is proportional to the binomial expansion of [1 + (−1)](m−1)/2, which is equal to 1 for m = 1, and vanishes otherwise. Therefore, Eq. (39) is obtained. Appendix E: The Components of the External Input We first calculate the component of the signal term in Eq. (25), In = αU0 ∫ ∞ −∞ dx exp [ −(x− z0) 2 4a2 ] vn(x|z). (122) After substituting Eq. (8) and completing square in the exponential argument, we have In = αU0√ (2pi)1/2an!2n exp [ −(z0 − z) 2 8a2 ] ∫ ∞ −∞ dx exp [ −(x− (z0 + z)/2) 2 2a2 ] Hn ( x− z√ 2a ) . (123) Using the identity in Eq. (79), denoting (z0 + z)/2 as z¯, and rewriting the integrand in terms of the basis functions, we obtain In = αU0√ (2pi)1/2an!2n exp [ −(z0 − z) 2 8a2 ] ∫ ∞ −∞ dx √ (2pi)1/2av0(x|z¯) × n∑ k=0   k n  √(2pi)1/2ak!2kvk(x|z¯) ( z0 − z√ 2a )n−k . (124) By the orthonormality of vn, only the k = 0 term remains non-vanishing, leading to the signal term in Eq. (40). For the noise component, ηn(t) = ∫ ∞ −∞ dxη(x, t)vn(x|z). (125) It is straightforward to check that 〈ηn(t)〉 = ∫ ∞ −∞ dx〈η(x, t)〉vn(x|z) = 0, (126) 〈ηn(t)ηm(t)〉 = ∫ ∞ −∞ dx ∫ ∞ −∞ dx′〈η(x, t)η(x′, t′)〉vm(x|z)vn(x′|z) = δmnδ(t− t′). (127) 31 Appendix F: The Dynamics of the 2D CANN F.1 The Eigenvalues and Eigenfunctions of F (x,x′|z) Define um,n(x|z) = um(x1|z1)un(x2|z2). (128) We check that um,n is the right eigenfunction of F (x,x′|z) and calculate the corresponding eigenvalue. We distinguish three different cases: • m = n = 0. Following the calculation in Eq. (85) and noting the integrations with respect to x′1 and x ′ 2 can be carried out separately, we have∫ ∞ −∞ ∫ ∞ −∞ dx′F (x,x′|z)u0,0(x′|z) = ∫ ∞ −∞ ∫ ∞ −∞ dx′1dx ′ 2F (x, x ′ 1, x ′ 2|z1, z2)u0(x′1|z1)u0(x′2|z2) = (1− √ 1− k/kc)u0,0(x|z). (129) • m 6= 0, n = 0 (the case for m = 0, n 6= 0 is the same). An important observation in the calculation of Eq. (84) is that for m ≥ 1, the integral in the second term vanishes. Therefore ∫ ∞ −∞ ∫ ∞ −∞ dx′F (x,x′|z)um,0(x′|z) = λmum,0(x|z). (130) • m 6= 0, n 6= 0. ∫ ∞ −∞ ∫ ∞ −∞ dx′F (x,x′|z)um,n(x′|z) = λmλnum,n(x|z). (131) F.2 The network dynamics in the weak input limit In the limit of weak inputs, we assume that the shape of the bump is unchanged and only the position of the bump varies with time, i.e., U(x, t) ≈ U˜(x|z(t)). We project the network dynamics in Eq. (59) on a single primary displacement mode, which is the movement of the bump along the straight line towards the stimulus location z0. Expressing the left-hand side of Eq. (59) in terms of the basis functions leads to τ ∂ ∂t U˜(x|z, t) = τU0(2pi) 1/2 2 [ u1,0 dz1 dt + u0,1 dz2 dt ] . (132) On the right-hand side of Eq. (59), we note that the neuronal interaction and relaxation terms cancel each other at the steady state, leaving behind the external input. Consider first the signal 32 part, projected onto the components u1,0 and u0,1: R.H.Ssignal = αU0 ∫ ∞ −∞ ∫ ∞ −∞ dx exp [ −(x− z 0)2 4a2 ] [u1,0(x|z) + u0,1(x|z)]. (133) Explicit integration yields R.H.S.signal = −αU0(2pi) 1/2 2 exp [ −(z− z 0)2 8a2 ] [(z1 − z01)u1,0(x|z) + (z2 − z02)u0,1(x|vz)]. (134) For the noise component, R.H.S.noise = ση1,0(t)u1,0(x|z) + ση0,1(t)u0,1(x|z), (135) where 〈η1,0(t)〉 = 〈η0,1(t)〉 = 0, (136) 〈η1,0(t)η1,0(t′)〉 = 〈η0,1(t)η0,1(t′)〉 = δ(t− t′). (137) Using the orthonormality of um,n and expressing the result in vector form, we arrive at Eq. (72). 33 References Amari, S. (1977). Dynamics of pattern formation in lateral-inhibition type neural fields. Bio- logical Cybernetics 27, 77–87. Ben-Yishai, R., R. Lev Bar-Or, and H. Sompolinsky (1995). Theory of orientation tuning in visual cortex. Proc. Natl. Acad. Sci. USA 92, 3844–3848. Berry II, M., I. Brivanlou, T. Jordon, and M. Meister (1999). Anticipation of moving stimuli by the retina. Nature 398, 334–338. Brody, C. D., R. Romo, and Kepecs (2003). Basic mechanisms for graded persistent activity: discrte attractors, continuous attractors, and dynamic representations. Current Opinion in Neurobiology 13, 204–211. Camperi, M. and X.-J. Wang (1998). A model of visuospatial short-term memory in prefrontal cortex: recurrent network and cellular bistability. J. Comput. Neurosci. 5, 383–405. Chow, C. and S. Coombes (2006). Existence and wandering of bumps in a spiking neural network model. SIAM Journal of Applied Dynamical Systems 5, 552–574. Dayan, P. and L. Abbott (2001). Theoretical Neuroscience. MIT Press, Cambridge. Deneve, S., P. E. Latham, and A. Pouget (1999). Reading population codes: a neural imple- mentation of ideal observers. Nature Neuroscience 2, 740–745. Domany, E., J. van Hemmen, and K. Schulten (1995).Models of Neural Networks III. Springer, Berlin. Erlhagen, W. (2003). Internal models for visual perception. Biol. Cybern. 88, 409–417. Ermentrout, B. (1998). Neural networks as spatial-temporal pattern-forming systems. Reports on Progress in Physics 61, 353–430. Folias, E. and P. Bressloff (2004). Breathing pulses in an excitatory neural network. SIAM J. Appl. Dyn. Syst. 3, 378–407. Fu, Y., Y. Shen, and Y. Dan (2001). Motion-induced perceptual extrapolation of blurred visual targets. J. Neuroscience 21, 1–5. Funahashi, S., C. Bruce, and P. Goldman-Rakic (1989). Mnemonic coding of visual space in the monkey’s dorsolateral prefrontal cortex. J. Neurophysiology 61, 331–349. 34 Fung, C. C. A., K. Y. M. Wong, and S. Wu (2008). Dynamics of neural networks with continuous attractors. Europhys. Lett. 84, 18002. Georgopoulos, A. P., J. F. Kalaska, R. Caminiti, and J. T. Massey (1982). On the relations between the direction of two-dimensional arm movements and cell discharge in primate motor cortex. J. Neurosci. 2, 1527–1537. Georgopoulos, A. P., M. Taira, and A. Lukashin (1993). Cognitive neurophysiology of the motor cortex. Science 260, 47–52. Graf, A., F. Wichmann, H. Bulthoff, and B. Scholkopf (2006). Classification of faces in man and machines. Neural Computation 18, 143. Griffiths, D. (2004). Introduction to Quantum Mechanics. Prentice Hall. Grossberg, S. (1988). Nonlinear neural networks: principles, mechanisms and architectures. Neural Network 1, 17. Gutkin, B. S., D. Pinto, and B. Ermentrout (2003). Mathematical neuroscience: from neurons to circuits to system. J. Physiol. Paris 97, 209–219. Hansel, D. and H. Sompolinsky (1998). Modeling feature selectivity in local cortical circuits. In C. Koch and I. Segev (Eds.), Methods in Neuronal Modeling: From Ions to Networks. MIT Press, Cambridge. Heeger, D. (1993). Modeling simple-cell direction selectivity with normalized, half-squared, linear operators. J. Neurophysiology 70, 1885. Hopfield, J. J. (1984). Neurons with graded responses have collective computational properties like those of two-state neurons. Proc. Natl. Acad. Sci. USA 81, 3088–3092. Jastorff, J., Z. Kourtzi, and M. Giese (2006). Learning to discriminate complex movements: biological versus artificial trajectories. J. Vision 6, 791. Laing, C. R. and C. Chow (2001). Stationary bumps in networks of spiking neurons. Neural Computation 13, 1473–1494. Ma, S. K. (1976). Modern Theory of Critical Phenomena. Benjamin, Reading, Massachusetts. Machens, C. and C. Brody (2008). Design of continuous attractor networks with monotonic tuning using a symmetry principle. Neural Computation 20, 452–485. Maunsell, J. H. R. and D. C. Van Essen (1983). Functional properties of neurons in middle 35 temporal visual area of the macaque monkey. i. selectivity for stimulus direction, speed, and orientation. J. Neurophysiology 49, 1127–1147. Miller, P. (2006). Analysis of spike statistics in neuronal systems with continuous attractors or multiple, discrete attractor states. Neural Computation 18, 1268–1317. Renart, A., P. Song, and X. Wang (2003). Robust spatial working memory through homeo- static synaptic scaling in heterogeneous cortical networks. Neuron 38, 473–485. Rolls, E. T., R. G. Robertson, and P. Georges-Francois (1995). The representation of space in the primate hippocampus. Soc Neurosci Abstr 21, 1494. Samsonovich, A. and B. L. McNaughton (1997). Path integration and cognitive mappping in a continuous attractor neural network model. J. Neurosci. 7, 5900–. Seung, H. S. (1996). How the brain keeps the eyes still. Proc. Acad. Sci. USA 93, 13339–13344. Stringer, S. M., T. P. Trappenberg, E. Rolls, and I. Aranjo (2002). Self-organizing continuous attractor networks and path integration: One-dimensional models of head direction cells. Network: Computation in Neural Systems 13, 217–242. Taube, J. S. (1998). Head direction cells and the neurophysiological basis for a sense of direction. Prog. Neurobiol. 55, 225–256. Trappenberg, T. (2003). Continuous attractor neural networks. In L. N. de Castro and F. J. V. Zuben (Eds.), Recent Developments in biologically inspired computing. Tuckwell, H. (1988). Introduction to theoretical neurobiology. Cambridge University Press, Cambridge. Wang, X. J. (2001). Synaptic reverberation underlying mnemonic persistent activitity. Trends in Neuroscience 24, 455–463. Wennekers, T. (2002). Dynamic approximation of spatio-temporal receptive fields in nonlinear neural field models. Neural Computation 14, 1801–1825. Wilson, H. and J. Cowan (1972). Excitatory and inhibitory interactions in localized popula- tions of model neurons. Biophysical Journal 12, 1–24. Wilson, M. A. and B. L. McNaughton (1993). Dynamics of hippocampal ensemble code for space. Science 261, 1055–1058. Wu, S. and S. Amari (2005). Computing with continuous attractors: Stability and on-line aspects. Neural Computation 17, 2215–2239. 36 Wu, S., S. Amari, and H. Nakahara (2002). Population coding and decoding in a neural field: a computational study. Neural Computation 14, 999–1026. Wu, S., K. Hamaguchi, and S. Amari (2008). Dynamics and computation of continuous at- tractors. Neural Computation 20, 994–1025. Xie, X., R. Hahnloser, and S. Seung (2002). Double-ring network model of the head-direction system. Physical Review E 66, 041902. Zhang, K.-C. (1996). Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: a theory. J. Neuroscience 16, 2112–2126. 37 0 0.5 1 1.5 2 2.5 3 s 0 0.01 0.02 0.03 0.04 g(s ) v max A s1 s2 v 0 50 100 150 200 250 t 0 1 2 3 4 s Simulation Theory s1 s2 B 0 0.01 0.02 0.03 0.04 v 0 0.2 0.4 0.6 0.8 1 s Simulation Theory v max C Figure 4: (A) The function g(s) and the stable and unstable fixed points of Eq. (32). (B) The time dependence of the separation s starting from different initial values. v = 0.025. (C) The dependence of the terminal separation s on the speed v. Parameters: N = 200, k = 0.5, a = 0.5, τ = 1, A = √ 2pia, ρ = N/2pi and α = 0.05. 38 -2 0 2 x 0 0.5 1 1.5 2 U (x) A 0 0.5 1 1.5 2 2.5 3 z0 0 100 200 300 400 T Simulation Weak input limit B Figure 5: (A) The tracking process of the network. (B) The reaction time vs. the jump size |z0|. Parameters: θ = pi/200 and the rest are the same as Fig. 4. 39 0 0.5 1 1.5 2 2.5 3 z0 0 100 200 300 400 T Simulation n=0 perturbation Weak input limit Figure 6: The reaction time vs. the jump size. Parameters: the same as Fig. 5. 0 0.5 1 1.5 2 2.5 3 z0 0 100 200 300 400 T Simulation Weak input limit n=1 perturbation n=3 perturbation n=5 perturbation Figure 7: The reaction time vs. the jump size. The simulation result is compared with the n = 5 perturbation approximation. Parameters: the same as Fig. 5. Note that the predictions of the n = 0 and n = 1 perturbations are the same because of the self-consistent condition in Eq. (119). Also, the predicted reaction time given by the n = 1 and n = 2 perturbations are almost the same. So are the n = 3 and n = 4 perturbations. 40 0 50 100 150 t -0.02 0 0.02 0.04 0.06 0.08 a 0 (t) 0 50 100 150 t -0.1 -0.05 0 a 1 (t) 0 50 100 150 t 0 0.02 0.04 0.06 0.08 a 2 (t) 0 50 100 150 t 0 0.01 0.02 0.03 0.04 0.05 a 3 (t) Figure 8: The first four an(t) obtained by the n = 10 perturbation (dashed lines) and by projecting U(x, t) onto the corresponding vn(x|z) during tracking (symbols). Parameters: N = 200, α = 0.05, a = 0.5, τ = 1, k = 0.5, ρ = N/2pi and A = √ 2api. -2 0 2 x - z(t) 0 0.5 1 1.5 2 U (x) -2 0 2 x - z(t) 0 0.5 1 1.5 2 U (x) -2 0 2 x - z(t) 0 0.5 1 1.5 2 U (x) -2 0 2 x - z(t) 0 0.5 1 1.5 2 U (x) -2 0 2 x - z(t) 0 0.5 1 1.5 2 U (x) -2 0 2 x - z(t) 0 0.5 1 1.5 2 U (x) t = 0 t = 5τ t = 10τ t = 15τ t = 20τ t = 25τ Figure 9: Snapshots of the neuronal inputs in the center-of-mass frame of the bump. Symbols: simulations. Solid lines: predicted synaptic input with the n = 10 perturbation. Parameters: N = 200, α = 0.15, a = 0.5, τ = 1, k = 0.5, ρ = N/2pi and A = √ 2api. 42 U( x,y ) A -3 -2 -1 0 1 2 3x -3 -2 -1 0 1 2 3 y 0 0.2 0.4 0.6 0.8 1 1.2 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 0 0.2 0.4 0.6 0.8 1 1.2 U( x,y ) B x y 0 0.5 1 1.5 2 2.5 3|z0| 0 100 200 300 400 T Simulation Theory C Figure 10: (A) The stationary state of the 2D CANN. (B) Illustrating the tracking process of the network; (C) The reaction time vs. the jump size. The simulation result is compared with the theoretical prediction. Parameters: N = 40 × 40, k = 0.5, a = 0.5, τ = 1, A = √2pia2, ρ = N/(2pi)2, α = 0.05 and θ = √ 2pi/N . 43 