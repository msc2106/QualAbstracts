ABSTRACT: 
Neurons that sustain elevated firing in the absence of stimuli have been found in many neural systems. In graded persistent activity, neurons can sustain firing at many levels, suggesting a widely found type of network dynamics in which networks can relax to any one of a continuum of stationary states. The reproduction of these findings in model networks of nonlinear neurons has turned out to be nontrivial. A particularly insightful model has been the "bump attractor," in which a continuous attractor emerges through an underlying symmetry in the network connectivity matrix. This model, however, cannot account for data in which the persistent firing of neurons is a monotonic-rather than a bell-shaped-function of a stored variable. Here, we show that the symmetry used in the bump attractor network can be employed to create a whole family of continuous attractor networks, including those with monotonic tuning. Our design is based on tuning the external inputs to networks that have a connectivity matrix with Toeplitz symmetry. In particular, we provide a complete analytical solution of a line attractor network with monotonic tuning and show that for many other networks, the numerical tuning of synaptic weights reduces to the computation of a single parameter. 
 
PREDICTION: 
This paper presents a a complete analytical solution of a line attractor network with monotonic tuning. We show that the symmetry used in the bump attractor network can be employed to create a whole family of continuous attractor networks, including those with monotonic tuning. Our design is based on tuning the external inputs to networks that have a connectivity matrix with Toeplitz symmetry. In particular, we provide a complete analytical solution of a line attractor network with monotonic tuning. We show that for many other networks, the numerical tuning ofsynaptic weights reduces to the computation of a single parameter. 
 
FULL TEXT: 
 LETTER Communicated by Xiao-Jing WangDesign of Continuous Attractor Networks with MonotonicTuning Using a Symmetry PrincipleChristian K. Machensmachens@zi.biologie.uni-muenchen.deCarlos D. Brodybrody@princeton.eduCold Spring Harbor Laboratory, Cold Spring Harbor, NY 11724, U.S.A.Neurons that sustain elevated firing in the absence of stimuli have beenfound in many neural systems. In graded persistent activity, neurons cansustain firing at many levels, suggesting a widely found type of networkdynamics in which networks can relax to any one of a continuum of sta-tionary states. The reproduction of these findings in model networks ofnonlinear neurons has turned out to be nontrivial. A particularly insight-ful model has been the “bump attractor,” in which a continuous attractoremerges through an underlying symmetry in the network connectivitymatrix. This model, however, cannot account for data in which the per-sistent firing of neurons is a monotonic—rather than a bell-shaped—function of a stored variable. Here, we show that the symmetry used inthe bump attractor network can be employed to create a whole family ofcontinuous attractor networks, including those with monotonic tuning.Our design is based on tuning the external inputs to networks that havea connectivity matrix with Toeplitz symmetry. In particular, we provide acomplete analytical solution of a line attractor network with monotonictuning and show that for many other networks, the numerical tuning ofsynaptic weights reduces to the computation of a single parameter.1 IntroductionAnimals are constantly confronted with the task of storing and manipu-lating continuous variables, such as the spatial position of objects or bodyparts. The short-term storage of such information is thought to bemediatedby persistently active neurons in various areas of the brain (Major & Tank,2004; Wang, 2001). For instance, neurons in the oculomotor system fire inproportion to an animal’s eye position (Aksay, Gemkrelidze, Seung, Baker,& Tank, 2001), neurons in the head-direction system fire with respect to ananimal’s directional heading (Taube & Bassett, 2003), and neurons in thehippocampus of rats fire with respect to the location of an animal (O’Keefe,1979; Quirk, Muller, & Kubie, 1990). All of these firing rates persist in theNeural Computation 20, 452–485 (2008) C© 2008 Massachusetts Institute of TechnologyDesign of Continuous Attractor Networks 453absence of proprioreceptive or other sensory information, suggesting thatthey store information about the respective analog variables. Further exam-ples come frommany cortical areas in which neural activities can representinformation about remembered sensory variables, such as the frequencyof a vibratory stimulus (Romo, Brody, Hernandez, & Lemus, 1999) or aspatial location (Smyrnis, Taira, Ashe, & Georgopoulos, 1992; Nakamura,1999).In all of the cited examples, the firing rates of the neurons are continuous,smoothly varying functions of the stored analog variable. These functions(or tuning curves) seem to come in two distinct classes. In one class, thefiring rates are bell-shaped or bump-shaped functions of the stored variable.Such tuning curves have been prominent for periodic angular variablessuch as head direction (Taube & Bassett, 2003), but have also been found fornonperiodic, nonangular variables such as the spatial position of an animal(O’Keefe, 1979; Quirk et al., 1990). In the other class, the firing rates aremonotonic functions of the stored variable. Such tuning curves have beenobserved in the oculomotor system (Seung, Lee, Reis, & Tank, 2000) and theprefrontal cortex (Romo et al., 1999).The ability to maintain firing rates at different constant levels, often re-ferred to as graded persistent activity, suggests that the respective neuralsystems are able to maintain a continuum of stationary states, each corre-sponding to a particular pattern of firing rates and a particular memoryvalue. Such a continuum of stationary states is usually called a continuousattractor or line attractor of the respective neural dynamical system.Neural network models of line attractors, built to account for the experi-mental data described above, are often based on control of precise feedbackin the recurrent connections between neurons. (But see Loewenstein &Sompolinsky, 2003, and Fransen, Tahvildari, Egorov, Hasselmo, & Alonso,2006, for single-cell models of persistent activity.) Designing such attrac-tor networks using neurons with linear input-output functions is fairlystraightforward.However,when realistic nonlinear neurons are considered,finding network connectivities that yield continuous attractors becomes anontrivial problem. A particularly elegant solution exists for the case ofbell-shaped tuning curves. In the so-called bump attractor, the attractorproperty is derived from an underlying symmetry in the network connec-tivity, a translational invariance in the ordering of synaptic weights (Amari,1977; Ben-Yishai, Bar-Or,&Sompolinsky, 1995; Skaggs,Knierim,Kudrimoti,& McNaughton, 1995; Zhang, 1996; Samsonovich & McNaughton, 1997;Tsodyks, 1999; Compte, Brunel, Goldman-Rakic, & Wang, 2000). In con-trast, an exact solution for the construction of realistic networkmodels withmonotonic tuning curves has been evasive. Instead, researchers buildingsuch network models have relied on computationally intensive numeri-cal tuning to approximate a line attractor (Seung et al., 2000; Koulakov,Raghavachari, Kepecs, & Lisman, 2002; Miller, Brody, Romo, &Wang, 2003;Eliasmith, 2005).454 C. Machens and C. BrodyIn this letter, we show that the construction of monotonic line attractorscan be based on the same symmetry principles used in the bumpattractor. Inparticular, we provide an exact and analytical solution of a monotonic lineattractor network built out of nonlinear neurons and show that this networkand the bump attractor network are instantiations of a whole family of lineattractor networks based on weight matrices with a Toeplitz symmetry.By connecting the emergence of a continuous attractor with an underlyingsymmetry in the network, we gain insights that are less easily obtainedwithnumerical approaches. We finally show how the monotonic line attractorsbased on Toeplitz symmetry are related to themonotonic line attractors thathave been designed earlier (Seung, 1996; Koulakov et al., 2002; Miller et al.,2003; Eliasmith, 2005).2 Network Dynamics in The Mean Field LimitIn the experimental data described above, analog variables are representedby the firing rates of neurons, and these firing rates stay approximately con-stant over several seconds. We will therefore use firing-rate-based modelsto approach the network design problem (Wilson & Cowan, 1973; Amari,1977; Dayan & Abbott, 2001). We do note that in some experimental sys-tems, persistent neural activity can be seen to be slowly but significantlytime varying (Brody, Hernandez, Zainos, & Romo, 2003; Rainer & Miller,2002), and some network models have begun to incorporate this aspect(Miller et al., 2003; Durstewitz, 2003; Singh & Eliasmith, 2006). However,for simplicity we will focus here on the case of networks with non-time-varyingpersistent activity. In otherwords,we are concernedwith stationarystates—fixed points of the network dynamics.We label each neuron by a position label x and define stot(x, t) to be thetotal synaptic input to neuron x at time t. We further define r (x, t) to be thefiring rate of neuron x at time t. Its dependence on synaptic input is de-scribed by a function h(·), determined by the biophysics of single neurons,such that r (x, t) ≡ h(stot(x, t)). The synaptic output of neuron x, defined asthe fraction of themaximal conductance of its postsynaptic targets, dependson the neuron’s firing rate r and is given by a function g(·) that is deter-mined by synaptic properties: s(x, t) ≡ g(r (x, t)) ∈ [0, 1]. (see appendix Afor details). We label the composition of these two functions, which goesfrom synaptic input to synaptic output, as f (·) ≡ g(h(·)).Let w(x, x′) be the total strength of connections from neuron x′ to neu-ron x. We assume that different inputs to a neuron combine additively.For simplicity, we assume that the effect of different synapse types can beincorporated in the weight matrix w(x, x′). In particular, we assume thatexcitatory inputs correspond to positive weights and inhibitory inputs tonegativeweights. Finally, let E(x) denote external synaptic inputs to neuronx, coming from neurons or areas of the brain that are outside this network.Design of Continuous Attractor Networks 455We will assume that these external inputs are constant on the timescalesthat interest us.Under these assumptions, the standard mean field equation for continu-ous neural networks is given by (Wilson & Cowan, 1973; Dayan & Abbott,2001)∂∂ts(x, t) = −s(x, t) + f(∫ 1−1dx′w(x, x′)s(x′, t) + E(x)). (2.1)In the usual interpretation of equation 2.1, s(x) is the firing rate of neuron x.In our interpretation, however, swill be the synaptic output of a neuron andis related to the firing rate r through s = g(r ). With this interpretation, theabove formulation of the network dynamics is equivalent to the dynam-ics of a conductance-based network of asynchronously spiking neurons(Ermentrout, 1994; Seung et al., 2000; Shriki, Hansel, & Sompolinsky, 2003;Renart, Brunel, & Wang, 2003).3 Line Attractors Reconsidered3.1 Definition. Stationary network states correspond to the case∂ts(x, t) = 0 in equation 2.1 so thats(x) = f(∫ 1−1dx′w(x, x′)s(x′) + E(x)). (3.1)Given a net input-output function f (·), defined by the biophysical proper-ties of neurons, our design goal is to find weight matrices w(x, x′) andexternal inputs E(x) that generate a continuum of solutions satisfyingequation 3.1. If we denote these solutions with a parameter m ∈ [m0,m1],thens(x,m) = f(∫ 1−1dx′w(x, x′)s(x′,m) + E(x)). (3.2)The value of m corresponds to the memory stored in the network. As afunction of m, the network states s(x,m) satisfying equation 3.2 trace outa continuous line in the space of all network states. A one-dimensionalcontinuous attractor is therefore also called a line attractor.Since f (·) is a nonlinear function, the design problem—that of findingweight matrices w(x, x′) and external inputs E(x) that fulfill equation 3.2—is nontrivial. In contrast, for linear input-output functions f (·), the class ofpossible solutions, not treated here, is readily obtained (Dayan & Abbott,2001). Finding weight matrices and external inputs so that equation 3.2holds does not guarantee that the resulting continuum of stationary states456 C. Machens and C. Brodyis dynamically stable. However, we will treat the stability of these solutionsas a separate issue.For a given neuron at position x = x0, the function s0(m) = s(x0,m) cor-responds to the synaptic output of that neuron as a function of the memorym. Accordingly, the firing rater0(m) = h(∫ 1−1dx′w(x0, x′)s(x′,m) + E(x))(3.3)corresponds to the tuning curve of the neuron (i.e., its firing rate as afunction of m). Note that the tuning curves are defined via the functionr = h(stot), the mapping of synaptic inputs into the firing rate, and not theinput-output function s = f (stot) used in equations 3.1 and 3.2. The tuningcurves provide the link to experimental data since they can be measured inelectrophysiological experiments. An important constraint on line attractordesign is therefore to find solutions to equations 3.2 and 3.3 that reproducethe distribution of tuning curves found in actual neural systems.3.2 TheBumpAttractor. Thebumpattractor is oneof thefirst line attrac-tor networks constructed with nonlinear elements (Amari, 1977; Ben-Yishaiet al., 1995; Skaggs et al., 1995; Zhang, 1996; Samsonovich & McNaughton,1997; Tsodyks, 1999; Compte et al., 2000; see also Figure 1). It has beenthoroughly studied and yields some crucial insights into how a continuumof stationary states can emerge through network interactions. We brieflyreview the ideas that underlie the construction of the bump attractor herein order to pave theway for our own developments explored further below.In its simplest form, the bump attractor consists of an infinite one-dimensional layer of neurons, x ∈ R, that receive a spatially homogeneousexternal input E(x) = Ec and are excited by their immediate neighbors andinhibited by neurons farther away. The weight matrix w(x, x′) is thus afunction of the difference x − x′ only (see Figure 1A),w(x, x′) = k(x − x′),and a typical weighting function or connectivity kernel k(y) has the shapeof a Mexican hat (see Figure 1B), for example,k(y) = Aσ1exp(− y22σ 21)− Aσ2exp(− y22σ 22),where σ1 is smaller than σ2.This network canmaintain stable bumps of activity at any position x (seeFigure 1D). Intuitively, individual activity bumps are stable in the absenceof inputs because the short-range excitatory connections provide sufficientDesign of Continuous Attractor Networks 457Figure 1: Bump attractor network. (A) The weight or connectivity matrix de-pends on only the difference y = x − x′ between the positions x and x′ of twoneurons. (B) The connectivity kernel k(y) shows that nearby neurons excite eachother, and neurons farther away inhibit each other. (C) All neurons receive thesame constant external input. (D) The stationary activity profiles s(x,m) are bellshaped, as a function of both x and m. The thick black curve shows the synap-tic output of neuron x = 0 as a function of the memory m. (E) Tuning curvesfor neurons at different positions x. The thick black curve corresponds to theneuron at position x = 0.excitatory feedback to maintain the neural activities within the bump. Thelong-range inhibitory connections counterbalance the uncontrolled growthof the activity bump. Since the network is infinitely large, this activity bumpcan be generated at any position x due to the translational symmetry of thenetwork connectivity matrix.For arbitrary input-output functions f (·), an analytical solution of thestationary network states cannot be obtained. Nonetheless, self-maintainedactivity bumps can exist in many networks, even if details of the connectiv-ity kernel or the input-output function f (·) are changed or if the network isbounded (e.g., ranges from positions x = −1 to x = 1). As a function of thememoriesm, the tuning curves of individual neurons, equation 3.3, are bellshaped (see Figure 1E).In its most common formulation, the neurons in a bump attractor areassumed to be organized on a ring, rather than a straight line, so that po-sition x = −1 corresponds to position x = 1 (Ben-Yishai et al., 1995; Skaggs458 C. Machens and C. Brodyet al., 1995; Zhang, 1996). A possible weight matrix is given by (Ben-Yishaiet al., 1995)—the connectivity kernel k(y) = −J0 + J2 cos(y). Note that thiskernel has the additional property k(y+ 1) = k(y− 1) for y = [−1, 1], whichenforces the network’s ring topology. Ring attractors have proven usefulto explain the tuning curves found in systems that store information aboutangles, such as the head-direction system (Skaggs et al., 1995; Zhang, 1996;Taube & Bassett, 2003).4 Line Attractor Design I: Translationally Invariant Activity PatternsIn both the bump and ring attractor model, the continuum of stable fixedpoints of the network is readily understood through the translational sym-metry of the activity profile on the spatial axis. Similar geometric principlesare not known for line attractors with monotonic tuning; instead, theirconstruction has so far been based on numerical approximations of appro-priate weight matrices (Seung, 1996; Seung et al., 2000; Miller et al., 2003;Eliasmith, 2005). We will now show that the class of solutions with thisproperty—translational symmetry—can be significantly extended and, inparticular, includes monotonic line attractors.To investigate the class of translationally invariant solutions toequation 3.2, we set s(x,m), the family of fixed-point solutions parame-terized by m, tos(x,m) = q (x −m),with x ∈ [−1, 1] and m ∈ [m0,m1], and observe that the following equationmust hold:∂∂xq (x −m) = − ∂∂mq (x −m). (4.1)Inserting equation 3.2 into equation 4.1, we obtainf ′(stot(x,m))∂∂x[ ∫ 1−1dx′w(x, x′)q (x′ −m) + E(x)]= − f ′(stot(x,m)) ∂∂m[ ∫ 1−1dx′w(x, x′)q (x′ −m) + E(x)].This equation is trivially fulfilled for all areas in the (x,m)-plane in whichf ′(stot(x,m)) = 0, that is, areas where the synaptic outputs are either zero,q (x −m) = 0 or saturated, q (x −m) = 1. When f ′(stot(x,m)) = 0 (corre-sponding to the area under the bump in Figure 1D), the synaptic inputsDesign of Continuous Attractor Networks 459must obey the translation rules:∂∂x[ ∫ 1−1dx′w(x, x′)q (x′ −m) + E(x)]= − ∂∂m[ ∫ 1−1dx′w(x, x′)q (x′ −m) + E(x)].Using equation 4.1, we can reformulate the right-hand side to obtain∂∂x[ ∫ 1−1dx′w(x, x′)q (x′ −m) + E(x)]=∫ 1−1dx′w(x, x′)∂∂x′q (x′ −m).Using partial integration of the right-hand side and solving for ∂x E(x), weobtain∂∂xE(x)=w(x, 1)q (1 −m) − w(x,−1)q (−1 −m)−∫ 1−1dx′(∂w(x, x′)∂x+ ∂w(x, x′)∂x′)q (x′ −m). (4.2)Equation 4.2 will be the central basis of our developments. Althoughit need hold only when f ′(s(x,m)) = 0, for simplicity we will confine our-selves here to solutions where equation 4.2 holds in the entire (x,m) plane.Below we describe some further simplifying assumptions that allow us touse equation 4.2 to design a wide variety of line attractors. Before we do so,however, we illustrate the simplest possible monotonic line attractor thatwe can design using equation 4.2.5 A Basic Monotonic Line AttractorLet us assume that the network has all-to-all excitatory connectionswith equal weights, w(x, x′) = wE (see Figures 2A and 2B). According toequation 4.2, and noting that the derivatives of w(x, x′) vanish, the externalinputs must fulfill the equation∂∂xE(x) = wE (q (1 −m) − q (−1 −m)).If we assume that the neuron at the left end, position x = −1, is inactivefor all memories m, so that s(−1) = q (−1 −m) = 0, and the neuron at theright end, position x = 1, is saturated so that s(1) = q (1 −m) = 1, then theexternal inputs must be (see Figure 2C)E(x) = wE x + Ec . (5.1)460 C. Machens and C. BrodyFigure 2: Monotonic line attractor network. (A) The network is all-to-all con-nected with equal excitatory weights, w(x, x′) = wE . (B) Accordingly, the con-nectivity kernel k(y) is constant. (C)Neurons receive external inputs that dependlinearly on their position x. (D) The stationary activity profiles s(x,m) are sig-moidal functions of both x and m. The thick black curve shows the synapticoutput of neuron x = 0 as a function of the memory m. (E) Tuning curves forneurons at different positions x. The thick black curve corresponds to the neu-ron at position x = 0. Although the synaptic outputs in panel D saturate, thefiring rates of individual neurons do not.Note that the external inputs must have this form if a translationally invari-ant steady-state s(x,m) = q (x −m) is to exist. While a necessary condition,this choice of E(x) guarantees neither that steady states exist nor that theyare nontrivial: as an extreme example, equation 5.1 can be trivially satis-fied with wE = 0 and E(x) = Ec, but in that case, there is only a singlestable state, and that is the trivial solution with all neurons having the sameoutput, s(x) = f (Ec).For large enough wE and a specific value of Ec, determined below, acontinuum of different stable states does exist and is shown in Figure 2D.Notice that by design, neurons at the left end always have inputs belowthreshold to produce an output s(−1) = 0, while neurons at the right endalways have their synaptic outputs saturated, s(1) = 1. Nonetheless, thislast fact does not imply that neuronal firing rates at x = 1 must always besaturated, since g(·), the mapping of firing rate into synaptic output, mayhave reached saturation, evenwhile h(·), themapping of synaptic input intoDesign of Continuous Attractor Networks 461Figure 3: Design of a simple monotonic line attractor. (A) Stationary activityprofile of the basic monotonic line attractor. (B)Whenwe shift the network stateby x to the left, the neuron at position x0 − x will have the same synapticoutput that neuron x0 had in panel A. However, the recurrent inputs to theneuron x0 − x are not the same, since the shift of the network state causes anoverall increase in recurrent excitation (black area). (C) If the external inputsto neuron x0 − x exactly compensate for the gain in recurrent inputs, thenneuron x0 − x in panel B will also be in steady state.the firing rate, has not (compare Figures 2D and 2E and see also appendixA and Figure 6).How does this network achieve its stability for a continuum of states?A particular stable state of the network is shown in Figure 3A. When thememory m is decreased, the activity pattern s(x,m) is translated leftward,and the number of neurons in the network with saturated output increases462 C. Machens and C. Brody(see Figure 3B). Consequently, the total recurrent excitation received by anyneuron will also increase, leading to a gain in recurrent excitation that isproportional to the shift x as shown by the black area in Figure 3B. Nowconsider the neuron at position x0 in Figure 3A. Let us denote this neuron’ssynaptic output as s0. As m decreases and the activity pattern is translatedtoward the left, the value of x identifying the neuronwith synaptic output s0decreases (see Figure 3B). For lower values of x, the external excitation E(x)is smaller (see Figure 3C).When the gain in recurrent excitationmatches thisloss in external excitation, the forces on the neurons identified by s(x,m) =s0 remain constant evenwhenm changes. Equation 5.1 enforces this balanceso that translations of any steady state obeying the boundary conditionswillalso be a steady state.The intuition illustrated in Figure 3 aside, let us now determine theconditions on wE and Ec that lead to nontrivial translationally invariantsolutions. Define the integrated activity in the network asu ≡∫ 1−1dx s(x),so that the steady state, equation 3.1, can be written ass(x) = f (wEu+ wE x + Ec).To be consistent with the boundary conditions, the output s(x) of theleft-most (x = −1) neuron must be zero, and the output of the right-mostneuron (x = 1) must be saturated at one. These conditions are reflected intwo inequalities. For the left border neuron, the total synaptic input mustbestot(−1) = wEu− wE + Ec ≤ sth, (5.2)where sth is the value of synaptic input below which there is zero output.Note that this equation must hold for all points on the line attractor—thatis, for all memories m. These different points are distinguished throughdifferent values of the integrated activity u. Equality holds in equation 5.2for the activity profile with the largest integrated activity, u = uL . This is thestate on the line attractor for which the activity of the left border neuron,s(−1), is still zero, yet the activities of its neighbors are not (compare inFigure 2D, position x = −1, memory m = 0).For the right border neuron (position x = 1), we obtainstot(1) = wEu+ wE + Ec ≥ ssat, (5.3)Design of Continuous Attractor Networks 463where ssat is the synaptic input above which the neuron is in saturation.Equality holds for the steady state with the smallest integrated activity,u = uR. Here, the right border neuron, s(1), is still in saturation, yet itsneighbors no longer are (compare in Figure 2D, position x = 1, memorym = 1.25). Using the cases u = uL in equation 5.2 and u = uR in equation 5.3,we can solve these equations for wE to obtainwE = ssat − sth2 + uR − uL>12(ssat − sth), (5.4)where the last inequality follows because uL > uR. Accordingly, only whenwE exceeds the right-hand side of equation 5.4 are we ensured that theleft and right border neurons fulfill the boundary conditions. When wE =(ssat − sth)/2, then uR = uL , implying that there is only one stable point. IfwE is greater than (ssat − sth)/2, then uL − uR > 0, and there is a range of uvalues that satisfies the boundary conditions. Themaximumpossible range,with uL − uR = 2, occurs when wE = ∞. In a manner similar to the case ofthe bump attractor, the line attractor solutions require a large enough wEbut are independent of the detailed form of f (·).These considerations leave only the parameter Ec unconstrained. Inappendix B, we show that when wE satisfies equation 5.4 and if Ec hasthe valueEc = −wE + ssat −∫ ssatsthds ′ f (s ′), (5.5)the network has a continuumof stationary states. Furthermore, in appendixE, we show that these stationary states are stable, so that a line attractornetwork is obtained.6 Line Attractor Design II: Boundary ConditionsWhile a uniform connectivity matrix,w(x′, x) = wE , provides a particularlytreatable simplification of equation 4.2, it is not the only one. Themain issueto be addressed with respect to equation 4.2 is that E(x) depends on boththe weight matrix, w(x, x′), and the activity profiles, q (x −m). However, bydefinition E(x) cannot depend onm, so in order for the equation to be valid,the m-dependence on the right-hand side needs to drop out.We will make two constraining assumptions that let m drop out of theright-hand side of equation 4.2. First, we will confine ourselves to solutionswhere equation 4.2 is valid for the entire (x,m) plane. Second, we furtherconfine ourselves to weight matrices with Toeplitz symmetry, w(x, x′) =k(x − x′). For these weight matrices, the integral on the right-hand side464 C. Machens and C. Brodyof equation 4.2 vanishes. Note that Toeplitz-type weight matrices are alsoused in the bump and ring attractor; here, however, we do not require theadditional symmetry constraint, w(x, x′) = w(x′, x), that is used in thosemodels.With these assumptions, we integrate equation 4.2 to obtainE(x) = Ec + q (1 −m)∫ x−1dx′k(x′ − 1) − q (−1 −m)∫ x−1dx′k(x′ + 1). (6.1)While the right-hand side of this equation still depends on m, this depen-dence is now reduced to the activities of the border neurons—the neurons atthe positions x = −1 and x = 1. There are only two possibilities to eliminatethe remaining dependence on m and render a unique equation for E(x): Constant boundaries. If the activity of the border neurons remainsconstant for all m, so that q (−1 −m) = const1 and q (1 −m) = const2,then the m-dependence drops out trivially. The simplest conditionsare those forwhich the border neurons are either below threshold or insaturation. Themonotonic line attractor designed above (see section 5)falls into this class, as does the bump attractor (see section 3). For thelatter, both border neurons are below threshold. Periodic boundaries. If the connectivity kernel fulfills the addi-tional constraint k(x − 1) = k(x + 1) and if the boundaries are peri-odic, q (−1 −m) = q (1 −m), then them-dependence will drop out be-cause the integrals in equation 6.1 cancel exactly. In this case, we haveE(x) = Ec . This choice of external inputs, together with the conditionk(x − 1) = k(x + 1), does indeed create a network with ring topologysince the neuron at position x = −1 and the neuron at position x = 1become identical. (We note that Toeplitz matrices with this additionalproperty are also called circulant matrices; Horn & Johnson, 1985.)The ring attractor discussed in section 3 falls into this class.The choice of a weight matrix and a boundary condition therefore fullydetermines the x-dependence of the external inputs E(x), up to an additiveconstant. Nonetheless, several uncertainties remain in the line attractordesign since the above conditions are necessary but not sufficient. Morespecifically, for an arbitrary Toeplitz weight matrix and a choice of theboundary conditions, there is no guarantee that a value Ec exists such thata network with external inputs tuned according to equation 6.1 will have anontrivial, stable activity profile consistent with the boundary conditions.While we have not been able to specify all Toeplitz weight matrices thatallow the design of line attractors, we outline several conditions belowthat allow constructing a large variety of line attractors (see also Figure 4).We will not consider the periodic boundary conditions or the boundaryconditions q (1 −m) = q (−1 −m) = 0 since these correspond to the bumpattractor and have already been discussed in great detail in the literature.Design of Continuous Attractor Networks 4657 A Potpourri of Line Attractors Based on Toeplitz Symmetry7.1 More Line Attractor Networks. A large set of line attractors can bedesigned with the boundary conditions q (−1 −m) = 0 and q (1 −m) = 1.All we need to do is follow the same principles as in section 5.First, since not all connectivity kernels can support a line attractor withthese boundary conditions, we need to find a condition that tells us whichkernels do. As shown in appendix C, we can follow a similar line of thoughtas in the derivation of equation 5.4. If k(y) fulfills the inequality∫ 1−1dx min{k(1 − x), k(−1 − x)} ≥ ssat − sth, (7.1)then a line attractor network can be obtained by tuning Ec . Note that weretrieve equation 5.4 when k(y) = wE . A simple way to obtain kernels thatobey equation 7.1 is to make sure that excitation dominates the networkconnectivities. As a direct consequence, we note that noise correlations ofsimultaneously recorded neurons in such networks must be positive onaverage. This is indeed the case for simultaneously recorded neurons inthe goldfish oculomotor system (Aksay, Baker, Seung, & Tank, 2003) or themonkey prefrontal cortex (Machens, Romo, & Brody, 2005), if the slope ofthe respective tuning curves has the same sign.Second, we need to tune the constant Ec in equation 6.1. For arbi-trary connectivity kernels that fulfill equation 7.1, this parameter has tobe tuned numerically. In contrast to previous monotonic line attractor de-signs, numerical tuning has been reduced to the tuning of a single scalarparameter. Fortunately, as we now describe, a simple procedure to achievethis tuning can be devised. Let us assume that we know a steady-states0(x) on the line attractor. For the correct value of Ec , all time deriva-tives, equation 2.1, will vanish if the network is in this steady state. If,on the other hand, Ec is too large, then the time derivatives will be pos-itive at any position x, and if Ec is too small, then the time derivativeswill be negative. Due to the monotonicity of f (·), the integral over the timederivatives,D(Ec, s0(·)) =∫ 1−1dx[− s0(x)+ f(∫ 1−1dx′k(x − x′)s0(x′)+E(x|Ec))](7.2)is therefore also a monotonic function of Ec , and its root yields the correctvalue for Ec . The notation E(x|Ec) reminds us that the external inputs E(x)depend on Ec , as defined in equation 6.1. The correct value for Ec canthen be determined with simple root-finding methods (Press, Teukolsky,Vetterling, & Flannery, 1992).466 C. Machens and C. BrodyThe only remaining problem is that we do not know the precise form ofs0(x) until we know Ec . In practice, though, if s0(x) is sufficiently close to anactual steady state, such as s0(x) = (x), the root of equation 7.2 will still liein the neighborhood of the correct Ec . In this neighborhood, the dynamicsof the network are usually slow along the line that will give rise to the lineattractor and fast along all other directions. Consequently, ifwe simulate thenetwork differential, equation 2.1, starting with a state that is sufficientlyclose to the actual line attractor, such as s0(x) = (x), the shape of s0(x) willrapidly approximate the shape of the steady-state activity profiles and onlyslowly drift along the line attractor. By determining the root of equation 7.2at every time step, we can adjust Ec during the simulation. While thenetwork dynamics falls into the line attractor, Ec will then converge ontoits correct value (see also appendix A).Several examples designed with this recipe are shown in Figures 4A to4F.Whereasmost of these line attractors havemonotonic tuning curves, it isalso possible to obtain nonmonotonic tuning curves, as shown in Figure 4F.7.2 Positive and Negative Monotonic Tuning Curves I: Revisiting theBump Attractor. Neurons in the networks shown in Figures 4A to 4Ehave tuning curves with positive monotonic slopes. In electrophysiolog-ical recordings from the prefrontal cortex, however, one finds both neuronswith positive and neurons with negative monotonic tuning curves withinthe same system (Romo et al., 1999;Machens et al., 2005).Wewill show nextthat both types of tuning curves can be obtained by choosing a networkwithboundary conditions q (−1 −m) = 1 and q (1 −m) = 1.As previously, the general problem is to find weight matrices that matchthis particular choice of saturated synaptic output boundary conditions.Here, our approach is heuristic: we choose weight matrices such that thetuning of the external inputs E(x), through equation 6.1, creates externalinputs at the boundaries, E(x = −1) and E(x = 1), that support the chosenboundary conditions. In other words, to guarantee nontrivial steady states,we seek a U-shaped function E(x) such that neurons close to the bordersreceive large external inputs and neurons in the center (positions aroundx = 0) receive small external inputs. According to equation 6.1, a simpleway to obtain this shape of the external inputs is to choose a Toeplitzweight matrix such that the kernel k(y) is negative around its ends (y ≈ −2and y ≈ 2) and positive in the center (y ≈ 0).An example of such a line attractor is shown in Figure 4I. Note that theweight matrix of this network is identical to the one for the bump attractornetwork in Figure 4H.Nonetheless, simply by changing the external inputs,without changing the internal recurrent connectivity at all, it is possible totransform a bump attractor into a line attractor with monotonic tuningcurves. Individual neurons are tuned to the memory m with both positiveand negative monotonic slopes.Design of Continuous Attractor Networks 467Figure 4: Tuning curves obtained from line attractor networks with differentkernels k(y) or different boundary conditions. The thick black lines correspondto the tuning curve of the neuron at position x = 0. The insets show the con-nectivity kernels k(y) and the external inputs E(x). (A) Line attractor withk(x − x′) = wE and boundary conditions s(−1) = 1 and s(1) = 0. This is the basicmonotonic line attractor (see section 5) with inverted boundary conditions andtuning curves. (B) Line attractorwith a symmetric connectivity kernel and short-range excitatory connections. The boundary conditions are again s(−1) = 1 ands(1) = 0. (C) Line attractor with the same boundary conditions but an asym-metric kernel. (D–F) Line attractors with the same boundary conditions butrandomized connectivity kernels. Note that in all of these kernels, excitatoryconnections dominate. Panel F shows that nonmonotonic tuning curves arepossible as well. (G) Ring attractor (see section 3). (H) Variant of the bump at-tractor with short-range excitation and global inhibition. Boundary conditionsare s(−1) = 0 and s(1) = 0. (I) By changing the external inputs to the network inH (boundary conditions are now s(−1) = 1 and s(1) = 1), we can obtain a net-work with both positive and negative monotonic tuning curves. Here neuronsin the center (black line; position x = 0) are not active.468 C. Machens and C. BrodyWe remark on this line attractor simply to demonstrate the range ofpossibilities obtainable by tuning the external inputs.However,we have notinvestigated this family of line attractors in greater detail. In the examplein Figure 4G, the parameter Ec was tuned by trial and error; the exactparameter values for this network are given in appendix A.7.3 Positive andNegativeMonotonic TuningCurves II: Coupling LineAttractor Networks. The definition ofm is purely conventional. By simplyredefining it, that is, by setting m → −m, or, equivalently, setting x → −x,the networks in Figures 4A to 4E will have tuning curves with negativeslopes. In principle, one could therefore obtain opposite tuning curves byconstructing two independent networks. However, two independent net-works could lead to conflicting representations of the analog memory m,which is, after all, one-dimensional. To solve this problem, we need to askhow to couple these networks so that both represent the samem. These con-siderations will provide a different solution to the problem of constructingnetworks with both positive and negative monotonic tuning curves. Sincethe question of coupling line attractor networks is interesting in its ownright, we discuss it next.Imagine that we have two networks with Toeplitz weight matricesw11(x, x′) = k11(x − x′) and w21(x, x′) = k22(x − x′). To couple these net-works, we choose Toeplitz coupling matrices, so that the connections fromthe first to the second network are given by w12(x, x′) = k12(x − x′) and thereverse connections by w21(x, x′) = k21(x − x′). The steady-state equationsfor the two layers are simply rewritten from equation 3.2,si (x) = f 2∑j=1∫ 1−1dx′ki j (x − x′)s j (x′) + Ei (x) ,where si (x) is the activity at position x in the ith layer with i ∈ {1, 2}. Aderivation similar to the one in sections 4 and 6 shows that the externalinputs are again determined by the weight matrix and the boundaryconditions,Ei (x)= Ei,c +2∑j=1[q j (1 −m)∫ x−1dx′ki j (x′ − 1)− q j (−1 −m)∫ x−1dx′ki j (x′ + 1)].As discussed before, the boundary conditions need to be self-consistentwith the Toeplitz weight matrices. Due to the increase in possibilities, thereare far more free parameters in this problem than before, which makes aself-consistent design more complicated.Design of Continuous Attractor Networks 469Figure 5: Coupling of two line attractor networks. (A) Connectivity matrix ofthe coupled network. Within each layer, connections are excitatory with equalweights; between layers, they are inhibitory with equal weights. (B, C) Neuronsin the two layers receive external inputs that increase (layer 1) or decrease(layer 2) as a function of their position. (D) Activity profiles s1(x,m) and s2(x,m)in the two layers. Note the opposite tuning with respect to both x and m. Thethick black curve shows the synaptic output of neurons at position x = 0 as afunction of the memory m. (E) Tuning curves for neurons at different positionsx. The thick black curves correspond to the neurons at position x = 0.A simple case is given by building on the example of section 5. Letus take two of these networks, so that k11(x − x′) = k22(x − x′) = wE , andcouple them with constant, mutually identical connections, k12(x − x′) =k21(x − x′) = wI . While these connections could be either excitatory or in-hibitory, the index I insinuates that wewill eventually assume that they areinhibitory (see also Figure 5A).470 C. Machens and C. BrodyWewill assume that the first layer has negativemonotonic tuning curves(achieved by setting s1(−1) = q1(−1 −m) = 0 and s1(1) = q1(1 −m) = 1)and the second layer positive monotonic tuning curves (achieved by set-ting s1(−1) = q1(−1 −m) = 1 and s1(1) = q1(1 −m) = 0). The external in-puts therefore become (compare equation 5.1)E1(x)= (wE − wI )x + EcE2(x)=−(wE − wI )x + Ec,leading to the steady-state equationss1(x)= f (wEu1 + wI u2 + (wE − wI )x + Ec) (7.3)s2(x)= f (wEu2 + wI u1 − (wE − wI )x + Ec)with u1 =∫dx′s1(x′) and u2 =∫dx′s2(x′). With similar methods as in theone-layer case, we can derive the parameter regime for which this networkyields a line attractor. An outline of the derivations is given in appendix D.For the parameters of the weight matrix, we find thatwE − wI ≥ 12 (ssat − sth). (7.4)Note that ifwI is inhibitory, its value is negative and can therefore be chosenarbitrarily. Furthermore, we find that Ec can take a whole range of values:Ec ≥ − 2wIwE − wI ssat + wI − wE −wE + wIwE − wI[∫ ssatsthds ′ f (s ′) − ssat](7.5)Ec ≤ − 2wIwE − wI sth − 3wI − wE −wE + wIwE − wI[∫ ssatsthds ′ f (s ′) − ssat]. (7.6)The distribution of tuning curves is shown in Figure 5E. Note that theoverall network activity U = u1 + u2 is constant. The memorized variableis determined by m = −u1 = U − u2.We note two additional aspects about the coupled network. First, sincethe overall network activity is constant, the external, x-dependent inputscould also be supplied through the network itself, that is, made internalby appropriate changes of the weight matrix. If w(z, z′) denotes the weightmatrix of the complete network (see Figure 5A), thenwe canwritew′(z, z′) =w(z, z′) + c(z) and E ′(z) = E(z) −Uc(z) for an arbitrary function c(z) and thenetwork with w′(x, x′) and E ′(x) will have a line attractor as well.11Quite generally, if there exists a function h(x, x′) with∫dx′h(x, x′)s(x′,m) = c(x) forall indicesm on the line attractor, then this indicates an invariance of theweightmatrix andDesign of Continuous Attractor Networks 471Second, we note that by concatenating the two layers such that positionx = 1 in layer one corresponds to position x = −1 in layer two, we obtain asingle layerednetworkwith a single shiftable activityprofile. This illustratesthat the existence of (single) shiftable activityprofiles is not restricted topureToeplitz-type weight matrices. The restriction to these weight matrices wasa simplification introduced for the derivation of equation 6.1.8 From Continuous to Discrete NetworksAll of our derivations so far have been done for a continuous layer of neu-rons. While this has the benefit of simplifying the analysis, in both realityand simulations, only a finite number of neurons will be available. In thesediscrete networks, we can still fine-tune the external inputs, and the exact,discrete analog to equation 6.1 is provided in appendix A. The finite net-works then exhibit a discrete series of fixed points rather than a continuousline, since the activity profile can be shifted only one neuron at a time. Theconstruction of line attractor networks based on translational symmetry istherefore not restricted to large-scale networkswhose dynamics arewell de-scribed by the neural field equations (see equation 2.1) but can be employedin smaller networks as well. For instance, all the networks in Figure 4 weresimulated with N = 51 units.9 DiscussionThe investigation of neural networks that give rise to persistent activity hasa long history (Wang, 2001; Major & Tank, 2004). Here, we have focused ongraded persistent activity, which is thought to be the neural substrate of theshort-term memory of analog values. The design of networks that exhibitgraded persistent activity is relatively straightforward if the neuron’s input-output functions are linear or threshold linear (Morishita & Yajima, 1972;Cannon, Robinson, & Shamma, 1983; Seung, 1996); however, the construc-tion of such networks has proven more complicated for nonlinear, realisticinput-output functions and has often relied on numerical tuning (Droulez& Berthoz, 1991; Zipser, Brandt, Littlewort, & Fuster, 1993; Seung, 1996;Seung et al., 2000; Eliasmith, 2005).The bump attractor has been a prominent exception and has shown thatthe emergence of a line attractor can be tied back to anunderlying symmetryin the network connectivity (Amari, 1977; Ben-Yishai et al., 1995; Skaggset al., 1995). In this work, we have shown that this symmetry principle canbe used to design a plethora of line attractor networks if one allows externalinputs that are inhomogeneous.While symmetrymay not be a necessity forexternal inputs, and we can set w′(x, x′) = w(x, x′) + αh(x, x′) and E ′(x) = E(x) − αc(x)for any number α without changing the stationary states of the network.472 C. Machens and C. Brodythe existence of continuous attractors—after all, numerical approximationsof continuous attractor networks usually do not make that assumption—itprovides additional insights and allows some analytical treatment of therespective networks.Perhaps most important, the geometric understanding of symmetry-based line attractors put forth here may provide a useful conceptual basisfor solving amajor outstanding problem:What plasticity rules does biologyuse in constructing line attractors? To date, models of line attractors havenot addressedhow theymight be constructed in biology. Line attractors pro-duced throughnumerical tuning (Seunget al., 2000; Eliasmith, 2005; Singh&Eliasmith, 2006, for example) typically have a complex network connectiv-ity that does not easily lend itself to devising biologically plausible plasticityrules with which they could be constructed. We speculate and hope thatthe far simpler underlying structure of the translational symmetry-basedline attractors may facilitate such a development and generalize from theartificial models constructed here to biological line attractors.In our framework, we assume a Toeplitz-symmetric weight matrix andthen tune the external inputs so that (possible) stationary states of the net-work can be translated along the network’s spatial axis. The continuousattractor therefore emerges due to the shift symmetry that the tuned ex-ternal inputs create.2 A crucial ingredient in creating this symmetry for(monotonic) line attractors is the saturation of synaptic outputs (see alsoSeung et al., 2000).Aprominent feature of all of our networks is that the tuning curves of dif-ferent neurons are shifted along the axis representing the memorized vari-able, the m-axis. These shifts are often only partially visible (see Figure 4),as should be expected given the finite and bounded nature of both thememorym and the network itself. Many tuning curves found in neural sys-tems, including the ones that are monotonic, show this kind of shifting andare therefore at least consistent with the idea of continuous attractors builton translational invariance. It should be mentioned, however, that tuningcurves in real systems are usually a lot more heterogeneous than the oneswe find in our simplified models. For instance, the slopes of monotonictuning curves vary in both the oculomotor system of the goldfish (Seunget al., 2000) and the prefrontal cortex (Brody et al., 2003). While such hetero-geneity could in principle be built into our line attractor networks (see alsoappendix E), it would simply be an add-on without any explanatory value.For simplicity, we have therefore neglected this issue. (For models that areexplicitly based on such heterogeneity, see Eliasmith, 2005.)For analytical simplicity, we have derived our results within the meanfield framework and assumed that all synapses are identical except for their2Indeed, this principle applies to both stable andunstable fixedpoints of the dynamics.When inverting the spatial axis in one of the layers of the coupled network (see Figure 5),one can obtain a continuous line of unstable fixed points as well (simulations not shown).Design of Continuous Attractor Networks 473(positive or negative) weight. When we distinguish explicitly between dif-ferent synapse types, such as excitatory and inhibitory synapses,we can stilldesign a continuumof stationary states by fine-tuning the external inputs tothese networks along the lines of equation 6.1. However, there exist circum-stances in which the different time constants of excitation and inhibitioncan lead to dynamical instabilities (Wang, 1999; Pinto & Ermentrout, 2001),rendering the stationary points unstable. While assuming slow excitatorydynamics, for example, through NMDA channels, alleviates this problem(Wang, 1999), future work will have to determine the precise conditionsunder which the continuous attractor networks presented here are stable.Last, but not least, all of our networks require that synaptic weights andexternal inputs are precisely tuned to the right parameters. This is a genericproblem that all line attractor networks face, whether they are producedthrough symmetry principles or throughnumerical tuning, and it is an issuethat thiswork is notmeant to address.Wewant to add, though, that all of thenetworks here can bemade robust against small changes in their parametersif we assume that individual neurons are hysteretic as originally proposedby Koulakov et al. (2002). Indeed, a spiking neuron implementation of arobust version of the coupled line attractor network (see Figure 5) has beendescribed in Machens et al. (2005).Appendix A: SimulationsIn this section, we briefly describe the details necessary to replicate thesimulations. In particular, we provide all the parameters for the networksshown in Figure 4.A.1 Input-Output Function. The construction of the synaptic input-output function s = f (stot) from the mapping of firing rates into synap-tic outputs, s = g(r ), and the mapping of synaptic inputs to firing rates,r = h(stot), is shown in Figure 6. For the construction of line attractor net-works along the lines presented here, the detailed shape of the input-outputfunction s = f (stot) does not matter, as long as it is monotonic and as longas f (stot) = 0 if stot ≤ sth and f (stot) = 1 if stot ≥ ssat (compare Figure 6C).In our network models, we assumed that synapses will saturate forsufficiently high presynaptic firing rates. One prominent form of synapticsaturation is given through synaptic depression. If the probability of vesiclerelease recovers with τ , then the postsynaptic conductance gwill follow theequation (Dayan & Abbott, 2001),g = gmax r1 + (1 − fD)τrwhere gmax is themaximum conductance input and fD controls the strengthof depression. Note that the precise biophysical mechanism for saturation474 C. Machens and C. BrodyFigure 6: Aneuron’s input-output function. (A) The synaptic output of a neuronas a function of its firing rate. The synaptic output is proportional to the con-ductance input to the postsynaptic neuron. In this example, it saturates throughsynaptic depression. (B) Firing rate of a neuron as a function of its total synapticinput. (C) Combining A and B allows plotting a neuron’s synaptic output as afunction of its input; we call this function the neuron’s synaptic input-output(i/o) function.does not matter in our framework. Previous work has shown that NMDAsynapses yield a more robust implementation of synaptic saturation inbiophysically realistic networks (Wang, 1999).Based on the above equation, we formulate the relation between thepresynaptic firing rate r and the synaptic output s. Choosing fD = 0.5 andτ = 1 sec, we assume that the synaptic output of a neuron obeys the relations = g(r ) =0 if r ≤ 0 HzA r1+r/2 if 0 < r < 50 Hz1 if r > 50 Hz,where A= 13/25 is chosen to make this function continuous. The piece-wise definition of the function serves to ensure that the above-saturationregime is exactly, and not just approximately, constant.For simplicity, we assume a threshold-linear dependency of the firingrate on the synaptic input,r = h(stot) ={0 if stot ≤ 050stot if stot > 0.For the simulations in Figure 4, stot varies approximately between 0 and 2,leading to firing rates r that range from 0 to 100 Hz.The input-output function is then given by s = f (stot) = g(h(stot)), asshown in Figure 6.Design of Continuous Attractor Networks 475A.2 Discrete Networks. While our derivations have been done forcontinuous mean field networks, the simulations in all of the figureswere performed with relatively small discrete networks (network sizeN = 50, . . . , 500 neurons). When rewriting the main equations for finitenetworks, a few subtleties need to be taken into account.For a network with N neurons, we replace the continuous position x bya discrete index i = 1, . . . , N and rewrite equation 3.1:si = f N∑j=1wi j s j + Ei .The derivation in section 4 can be performed for a discrete network as well,resulting in the following analog to equation 6.1,Ei = Ec + s1i∑j=2w j−1,N − sNi∑j=2w j,1,where it is implicitly understood that the weight matrix wi j has a Toeplitzstructure such that wi j = ki− j . Note the index shift in j between the secondand third sums. Also note that the definition of Ec here is different from theone in equation 6.1 since the indices i are not symmetric around zero. (Theposition x is symmetric around zero.)To calculate the value of Ec for the networks in Figures 4A to 4F, wesimulate the following set of (differential) equations:s˙i =−si + f N∑j=1wi j s j + Ei0=N∑i=1 f N∑j=1wi j sl + E j (Ec)− si ,with the initial conditions si = 0 for i < N/2 and si = 1 otherwise. Thesecond equation is an implicit equation for Ec that can be solved at ev-ery time step using standard numerical root-finding methods (Press et al.,1992). While the first equation will evolve onto a continuous attractor,the solution of the second equation will converge onto the correct valueof Ec .The values of Ec in the networks in Figures 4G to 4I were estimated witha simple trial-and-error procedure and are given below.476 C. Machens and C. BrodyTable 1: Design of Continuous Attractor Networks.4 wi j s1 sN EcFigure 4A 1/25 1 0 −1.924Figure 4B 3/25 exp(−|i − j |/12) 1 0 −1.308Figure 4C 3/50(i − j) exp(−|i − j |/30)+ 3/50(−(i − j)) exp(−|i − j |/8) 1 0 −0.4Figure 4G 1/25 + 2/25 cos(2π (i − j)/N) 0 0 −1 ∈ [−1.13,−0.7]Figure 4H −7/50 + 1/5 exp(−(i − j)2/160) 0 0 0.3 ∈ [−0.19, 4.5]Figure 4I −7/50 + 1/5 exp(−(i − j)2/160) 1 1 3 ∈ [1.5, 5.13]A.3 Parameters for Line Attractors in Figure 4. For the line attractorsin Figure 4, we used networks with N = 51 neurons. The parameters of theweight matriceswi j and the constant external inputs Ec are given in Table 1.The intervals in the last column indicate that the respective continuousattractor systems exist for several values of Ec .Appendix B: Basic Monotonic Line Attractor: Derivation of EcTo determine the correct value of Ec for the basic monotonic line attractor,we note that the dynamics of the network will always reduce to a singledimension, since the weight matrix has rank one. This statement is provedin appendix E. The relevant variable for the basic monotonic line attractoris the integrated network activity u. Using equation E.2, we obtain:u=∫ 1−1dx f (wEu+ wE x + Ec)= 1wE[F (wEu+ wE + Ec) − F (wEu− wE + Ec)], (B.1)where F (·) is the antiderivative of f (·). Without loss of generality, we canchoose F (·) such that F (sth) = 0. This function is then piece-wise defined asF (stot) =0 if stot ≤ sth∫ stotsthds ′ f (s ′) if sth < stot ≤ ssatF (ssat) + stot − ssat if stot > ssat.(B.2)Wenote that the right termon the right-hand side of equationB.1will vanishbecause of equation 5.2. Furthermore, the input to F (·) in the left term onthe right-hand side of equation B.1 will always be larger than ssat, whichDesign of Continuous Attractor Networks 477follows from equation 5.3. Using the definition of F (·) in equation B.2, wetherefore obtain for equation B.1,u = 1wE[F (ssat) + wEu+ wE + Ec − ssat],which will have a continuum of solutions for u iffEc = −wE + ssat − F (ssat),which proves equation 5.5.Appendix C: Line Attractors with One Saturated Boundary:Existence Proof and ConstraintsC.1 Constraints on Connectivity Kernel. Given the boundary condi-tions s(−1) = 0 and s(1) = 1 and the appropriately tuned external inputsE(x), equation 6.1, we can formulate inequalities, similar to equations 5.2and 5.3, that need to be fulfilled for the left-most and right-most borderneurons:∫ 1−1dx k(−1 − x)s(x) + Ec ≥ sth (C.1)∫ 1−1dx k(1 − x)s(x) +∫ 1−1dx k(x − 1) + Ec ≤ ssat. (C.2)Solving these inequalities for Ec and combining them leads to a singleinequality:∫ 1−1dx [k(1 − x) − k(−1 − x)]s(x) +∫ 1−1dx k(x − 1) ≥ ssat − sth. (C.3)Sincewedonot know the stationary network states s(x) for arbitrary kernelsk(y), we can only approximate the left-hand side. If we assume that s(x)takes a form that leads to the smallest possible value of the left integral,then this approximation will be conservative. Let us define the set of allx for which the term in the square brackets is negative:  = {x|k(1 − x) <k(−1 − x), x ∈ [−1, 1]}. Tominimize the left integral with respect to s(x), weassume that s(x) = 1, whenever x ∈  and s(x) = 0 otherwise. Accordingly,∫dx [k(1 − x) − k(−1 − x)] +∫ 1−1dx k(x − 1) ≥ ssat − sth.478 C. Machens and C. BrodySimplifying, we obtain∫dx k(1 − x) +∫[−1,1]\dx k(−1 − x) ≥ ssat − sth,which is identical to equation 7.1. Note that this is a sufficient but notnecessary condition on the kernels k(y). In otherwords, theremaybekernelsthat do not fulfill this inequality yet still allow the construction of a lineattractor with the required boundary conditions.C.2 Existence of Stationary States. To show that a line attractor existsfor kernels that fulfill equation 7.1, we need to prove that—for at least onevalue Ec—the network has a stable stationary state that obeys the boundaryconditions.Due to the tuning of the external inputs in equation 6.1,we knowthat if one such stationary state exists, it canbe shiftedwithin a certain range,and a continuum of stationary states must exist as well.Given that a line attractor exists, we observe that the equality sign inequation C.1 will hold for the left-most state on the line attractor, which wedenote as sL (x). This suggests that the correct choice for Ec isEc = sth −∫ 1−1dx k(−1 − x)sL (x). (C.4)While this choice of Ec is motivated by the left-most state on the putativeline attractor, we will next assume that sL (x) could be any activity profile.In turn, we can investigate the properties of states sL (x) that are solutions tothe steady-state equation, equation 3.1. Given equations 6.1 and C.4, thesemust fulfillsL (x)= f(∫ 1−1dx′[k(x− x′)− k(−1− x′)]sL (x′)+ sth +∫ x−1dx′ k(x′ − 1)).Certainly this equation will have at least one solution for sL (x). While wecannot specify this solution exactly, we can investigate its properties at theboundaries x = −1 and x = 1. Plugging in the value x = −1, we notice thatsL (−1) = 0, so that the solution fulfills the left boundary condition. Plug-ging in x = 1, we notice from equation C.3 that the synaptic inputs are insaturation, so that sL (1) = 1. Accordingly, the choice of Ec in equation C.4guarantees the existence of a stationary state that fulfills the requiredbound-ary conditions.To determine the stability of these stationary states is difficult as long astheir precise form is not known. Practical experience suggests, though, thatthey are always stable, thus giving rise to a line attractor.Design of Continuous Attractor Networks 479Appendix D: Derivation of Parameter Regime for Mutual InhibitionNetworkFor the mutual inhibition network, the sum of the integrated activities ofthe first and second layer is a constant (compare Figure 5),U = u1 + u2 = const,so that the equation for u1 suffices to describe the system. Similar to theone-layer case, discussed in appendix B, we obtain an equation for u1 byintegrating the stationary network, equation 7.3, to obtainu1 = 1wE − wI [F ((wE − wI )(u1 + 1) + wIU + Ec)− F ((wE − wI )(u1 − 1) + wIU + Ec)], (D.1)where F (·) is the antiderivative of f (·) as given in equation B.2. Looking atthe boundaries in the first layer, we find that for any state u1, the followingconditions must hold:stot,1(−1) = (wE − wI )(u1 − 1) + wIU + Ec ≤ sth (D.2)stot,1(1)= (wE − wI )(u1 + 1) + wIU + Ec ≥ ssat, (D.3)and similar conditions hold for the opposite layer. Subtracting equation D.2from equation D.3 yields the condition on the weights wE − wI givenin equation 7.4. The two inequalities also show that the first term inequation D.1 will always be in saturation, and the second term will alwaysbe below threshold. Using equation B.2, we obtainu1 = u1 + 1wE − wI [F (ssat) − ssat + wIU + (wE − wI ) + Ec],whichwill be valid if the second term vanishes. This results in the followingequation for the summed activityU:U = − 1wI(F (ssat) − ssat + wE − wI + Ec). (D.4)Following the arguments for the one-layered case (see section 5), thereare two possibilities for a left-most state. Either the activity profiles run intothe threshold condition in the first layer, in which case,stot,1(−1) = (wE − wI )(uL − 1) + wIU + Ec = sthstot,2(−1)= (wE − wI )(uR + 1) + wIU + Ec ≥ ssat, (D.5)480 C. Machens and C. Brodyor the left-most state runs into saturation condition in the second layer, inwhich case,stot,1(−1)= (wE − wI )(uL − 1) + wIU + Ec ≤ sthstot,2(−1)= (wE − wI )(uR + 1) + wIU + Ec = ssat. (D.6)Similar conditions hold for the right-most state.We can use these conditions to obtain a lower bound on Ec . If we takeequation D.2 for u = uR and add it to equation D.5, then(wE + wI )U − 2(wE − wI ) + 2Ec ≤ 2sth,which can be solved for Ec , using equation D.4, and results in equation 7.5.An upper bound on Ec can be obtained in a similar way by takingequation D.3 for u = uL and adding it to equation D.6 to yield(wE + wI )U + 2(wE − wI ) + 2Ec ≥ 2ssat,from which follows equation 7.6.Appendix E: Monotonic Line AttractorsE.1Numerical Tuning of theWeightMatrix. The construction ofmono-tonic line attractors in the literature has usually been based on numericaltuning of the weight matrix (Seung, 1996; Eliasmith, 2005). Here we reviewthe construction of these line attractors and show their relation to the basicmonotonic line attractor from section 5.Let us assume that the weight matrix of the network has rank one, thatis, that it can be written as the outer product of two vectors,w(x, x′) = a (x)b(x′).In this case, all trajectories of the networkwill flow onto a single dimension.To see that, let us define the variableu(t) =∫ 1−1dx b(x)s(x, t). (E.1)With this definition, the dynamics of the network follow from equation 2.1:∂∂ts(x, t) = −s(x, t) + f (a (x)u(t) + E(x)).Design of Continuous Attractor Networks 481In other words, the firing rates s(x, t) of all the neurons are driven by asingle time-varying variable u(t), and the dynamics of this network can beinvestigated by looking at the dynamics of this single variable. The timederivative of equation E.1 yieldsdu(t)dt= −u(t) +∫ 1−1dx b(x) f (a (x)u(t) + E(x))with steady stateu =∫ 1−1dx b(x) f (ua (x) + E(x)). (E.2)The choice of a rank-one weight matrix therefore reduces the task of de-signing a line attractor to finding a set of parameters a (x), b(x), and E(x)for which this equation holds. Note that for given (smooth) a (x) and E(x),this equation is known as an inhomogeneous Fredholm integral equation ofthe first kind with the kernel K (m, x) = f (ma (x) + E(x)) and the unknownfunction b(x) (Byron & Fuller, 1969; Press et al., 1992).The equation can be solved numerically for b(x) if the kernel K (m, x) isinvertible (Press et al., 1992). In previous studies, a (x) and E(x) have usuallybeen chosen randomly, ensuring invertibility of the above equation, so thatb(x) could be determined numerically. Note that this approach does notrequire saturating synapses; in fact, the function f (·) could in principle takealmost any form.E.2 Relation to Basic Monotonic Line Attractor and Heterogeneity.The basic monotonic line attractor from section 5 is an analytical solution toequation E.2 and can therefore be regarded as a special case of the rank-oneline attractors.However, the analogy to the numerically tunednetworks canbe taken one step further when considering large networks with heteroge-neous neurons. In essence, if the network is large enough, any heterogeneitywill average out, so that all rank-one line attractors become equivalent tothe basic monotonic line attractor.Let us assume that every neuron has a different input-output functions = fx(stot), marked by its position x. We still assume that the synapticoutput eventually saturates; however, we now allow saturation at valuesdifferent from one. In particular, we are interested in the parameterizations = fx(stot) = η(x) g(ξ (x)stot). (E.3)We assume that the parameters η(x) and ξ (x) of this input-output func-tion are randomly drawn at each position x from appropriate probabilitydistributions. Without loss of generality, we assume that both distributions482 C. Machens and C. Brodyare centered around one. We denote the average input-output functionas f¯ (·).We can now construct a basic monotonic line attractor network withweight matrix w(x, x′) = wE and input-output function f¯ (·) as described insection 5. For this network, the synaptic inputs in the continuous attractorstate are given bystot(x)=∫dx′wEs(x′) + E(x)=∫dx′wE f¯ (stot(x′)) + E(x).Since the synaptic input stot(x′) is statistically independent from the ran-domly drawn input-output functions fx′ (·), the integral remains the same ifwe replace the average input-output function by the heterogeneous input-output function,stot(x) =∫dx′wE fx′ (stot(x′)) + E(x).Accordingly, the total synaptic input stot(x) to each neuron in a heteroge-neous network with input-output functions s = fx(stot) is the same as inthe basic monotonic line attractor network. Since the above equations aresteady-state equations for the synaptic inputs, both the heterogeneous andhomogeneous networksmust be in a fixedpoint.Hence, a networkwith het-erogeneous neurons designed along the lines above also has a line attractor.The resulting heterogeneous network is formally equivalent to a net-work of homogeneous neuronswith aweightmatrixw(x, x′) = wEη(x)ξ (x′),which can be seen by plugging equation E.3 into equation 3.1.3 The reason,of course, is that in large-scale networks, this random heterogeneity is sim-ply smoothed out, creating a dynamics that is equivalent to the dynamicsof the basic monotonic line attractor network. Hence, any network with arank-one matrix built along these lines can be regarded as equivalent to thebasic monotonic line attractor.For discrete networks, the heterogeneity will not be averaged out exactlysince we sum only over a finite number of neurons. In this case, there is noguarantee to obtain N fixed points, and the dynamics of the homogeneousnetwork are only an approximation to those of the heterogeneous network.The resulting mismatches will then lead to drifts of the memory states3A similar argument can be made for arbitrary Toeplitz-type networks. Accordingly,line attractors can potentially be built for any weight matrix that can be written asw(x, x′) = η(x)k(x − x′)ξ (x′) if η(x) and ξ (x′) are random numbers and k(x − x′) is suf-ficiently smooth.Design of Continuous Attractor Networks 483between stable fixed points, as has been noted before (Seung, 1996). Insummary, the numerical tuning ofmonotonic line attractor networks allowsa large variety of networks, some of which are mathematically identical tothe basic monotonic line attractor network in section 5.AcknowledgmentsWe thank Ranulfo Romo for a fruitful and inspiring collaboration. Thiswork was supported by a Swartz Foundation Fellowship to C.K.M. andNIH grant R01MH067991 to C.D.B.ReferencesAksay, E., Baker, R., Seung, H. S., & Tank, D. W. (2003). Correlated discharge amongcell pairs within the oculomotor horizontal velocity-to-position integrator. J. Neu-rosci., 23, 10852–10858.Aksay, E., Gemkrelidze, G., Seung, H. S., Baker, R., & Tank, D. W. (2001). In vivo in-tracellular recording and perturbation of persistent activity in a neural integrator.Nat. Neurosci., 4, 184–193.Amari, S. (1977). Dynamics of pattern formation in lateral-inhibition type neuralfields. Biol. Cybern., 27(2), 77–87.Ben-Yishai, R., Bar-Or, R. L., & Sompolinsky, H. (1995). Theory of orientation tuningin visual cortex. Proc. Natl. Acad. Sci. USA, 92, 3844–3848.Brody, C. D., Hernandez, A., Zainos, A., & Romo, R. (2003). Timing and neuralencoding of somatosensory parametric working memory in macaque prefrontalcortex. Cereb. Cortex, 13, 1196–1207.Byron, F. W., & Fuller, R. W. (1969). Mathematics of classical and quantum physics.Reading, MA: Addison-Wesley.Cannon, S. C., Robinson, D. A., & Shamma, S. (1983). A proposed neural networkfor the integrator of the oculomotor system. Biol. Cybern., 49, 127–136.Compte, A., Brunel, N., Goldman-Rakic, P. S., & Wang, X.-J. (2000). Synaptic mech-anisms and network dynamics underlying spatial working memory in a corticalnetwork model. Cereb. Cortex, 10, 910–923.Dayan, P., & Abbott, L. F. (2001). Theoretical neuroscience. Cambridge, MA:MIT Press.Droulez, J., & Berthoz, A. (1991). A neural network model of senoritopic maps withpredictive short-term memory properties. Proc. Natl. Acad. Sci. USA, 88, 9653–9657.Durstewitz, D. (2003). Self-organizing neural integrator predicts interval timesthrough climbing activity. J. Neurosci., 23, 5342–5353.Eliasmith, C. (2005). A unified approach to building and controlling spiking attractornetworks. Neural Comput., 17(6), 1276–1314.Ermentrout, B. (1994). Reduction of conductance based models with slow synapsesto neural nets. Neural Comp., 6, 679–695.Fransen, E., Tahvildari, B., Egorov, A. V., Hasselmo, M. E., & Alonso, A. A. (2006).Mechanism of graded persistent cellular activity of entorhinal cortex layer Vneurons. Neuron, 49, 735–746.484 C. Machens and C. BrodyHorn, R. A., & Johnson, C. R. (1985). Matrix analysis. Cambridge: CambridgeUniversity Press.Koulakov, A. A., Raghavachari, S., Kepecs, A., & Lisman, J. E. (2002). Model for arobust neural integrator. Nat. Neurosci., 5(8), 775–782.Loewenstein, Y., & Sompolinsky, H. (2003). Temporal integration by calciumdynam-ics in a model neuron. Nat. Neurosci., 6, 961–967.Machens, C. K., Romo, R., & Brody, C. D. (2005). Flexible control ofmutual inhibition:A neural model of two-interval discrimination. Science, 307, 1121–1124.Major, G., & Tank, D. (2004). Persistent neural activity: Prevalence and mechanisms.Curr. Opin. Neurobiol., 14, 675–684.Miller, P., Brody, C. D., Romo, R., & Wang, X.-J. (2003). A recurrent network modelof somatosensory parametric working memory in the prefrontal cortex. Cereb.Cortex, 13(11), 1208–1218.Morishita, I., & Yajima, A. (1972). Analysis and simulation of networks of mutuallyinhibiting neurons. Kybernetik, 11, 154–165.Nakamura, K. (1999). Auditory spatial discriminatory andmnemonic neurons in ratposterior parietal cortex. J. Neurophysiol., 82(5), 2503–2517.O’Keefe, J. (1979). A review of the hippocampal place cells. Prog. Neurobiol., 13,419–439.Pinto, D. J., & Ermentrout, G. B. (2001). Spatially structured activity in synapticallycoupled neuronal networks: II. Lateral inhibition and standing pulses. SIAM J.Appl. Math., 62, 226–243.Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (1992). Numericalrecipes in C. Cambridge: Cambridge University Press.Quirk, G. J., Muller, R. U., & Kubie, J. L. (1990). The firing of hippocampal place cellsin the dark depends on the rat’s recent experience. J. Neurosci., 10, 2008–2017.Rainer, G., & Miller, E. K. (2002). Timecourse of object-related neural activity in theprimate prefrontal cortex during a short-term memory task. Eur. J. Neurosci., 15,1244–1254.Renart, A., Brunel, N., & Wang, X.-J. (2003). Mean-field theory of recurrent corticalnetworks: From irregularly spiking neurons to workingmemory. In J. Feng (Ed.),Computational neuroscience: A comprehensive approach. Boca Raton, FL: CRC Press.Romo, R., Brody, C. D., Hernandez, A., & Lemus, L. (1999). Neuronal correlates ofparametric working memory in the prefrontal cortex. Nature, 399, 470–473.Samsonovich, A., & McNaughton, B. L. (1997). Path integration and cognitive map-ping in a continuous attractor neural network model. J. Neurosci., 17, 5900–5920.Seung, H. S. (1996). How the brain keeps the eyes still. Proc. Natl. Acad. Sci. USA, 93,13339–13344.Seung, H. S., Lee, D. D., Reis, B. Y., & Tank, D. W. (2000). Stability of the memory ofeye position in a recurrent network of conductance-basedmodel neurons.Neuron,26, 259–271.Shriki, O., Hansel, D., & Sompolinsky, H. (2003). Rate models for conductance-basedcortical neural networks. Neural Comp., 15, 1809–1841.Singh, R., & Eliasmith, C. (2006). Higher-dimensional neurons explain the tuningand dynamics of working memory cells. J. Neurosci., 26, 3667–3678.Skaggs, W., Knierim, J. J., Kudrimoti, H. S., & McNaughton, B. L. (1995). Amodel of the neural basis of the rat’s sense of direction. In G. Tesauro,Design of Continuous Attractor Networks 485D. S. Touretzky & T. K. Leen (Eds.), Advances in neural information processingsystems, 7 (pp. 173–180). Cambridge, MA: MIT Press.Smyrnis, N., Taira, M., Ashe, J., & Georgopoulos, A. P. (1992). Motor cortical activityin a memorized delay task. Exp. Brain Res., 92, 139–151.Taube, J. S., & Bassett, J. P. (2003). Persistent neural activity in head direction cells.Cereb. Cortex, 13, 1162–1172.Tsodyks, M. (1999). Attractor neural network models of spatial maps in hippocam-pus. Hippocampus, 9, 481–489.Wang, X.-J. (1999). Synaptic basis of cortical persistent activity: The importance ofNMDA receptors to working memory. J. Neurosci., 19, 9587–9603.Wang, X.-J. (2001). Synaptic reverberation underlying mnemonic persistent activity.Trends in Neurosciences, 24, 455–463.Wilson, H. R., & Cowan, J. D. (1973). A mathematical theory of the functional dy-namics of cortical and thalamic nervous tissue. Kybernetik, 13, 55–80.Zhang, K. (1996). Representation of spatial orientation by the intrinsic dynamics ofthe head-direction cell ensemble. J. Neurosci., 16(6), 2112–1226.Zipser, D., Brandt, K., Littlewort, G., & Fuster, J. (1993). A spiking network model ofshort-term active memory. J. Neurosci., 13, 3406–3420.Received July 31, 2006; accepted January 28, 2007.