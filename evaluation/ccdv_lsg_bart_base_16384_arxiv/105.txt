ABSTRACT: 
Australia has reflected an international shift toward public participation in governance and science. Researchers have critiqued this shift as insufficient. Meanwhile, studies of how research funds are allocated also found room for improvement. This experiment tested a way to add value to the effort researchers put into research proposals by using them for deliberative public engagement. Three Australian events tested a model of deliberative participation in decision-making about science funding. These events were shorter than most deliberative processes, based on a model tested in the United Kingdom. Although recruitment was aimed at broad representation, participants had more formal education than Australia’s average. Voting decisions were most influenced by potential benefits to society of the planned research, as well as participants’ understanding of plans presented. Some reported that their decisions were influenced by whether benefits would happen locally. Results suggested that participants’ voting decisions were more influenced by the research plans than who presented them. However, unconscious biases cannot be ruled out as factors in decision-making. Participants reported they would be keen to participate in such a process again; however, this enthusiasm was linked to a meal incentive. The impact of brevity on deliberative decision-making is discussed, along with potential modifications for future experiments. 
 
PREDICTION: 
 this article describes a pilot study of deliberative public engagement with science . participants were asked to vote for a research proposal in a pre-event survey . 
 participants were asked to vote for the proposal they would vote for . 
 participants rated the presentations on 11 criteria , as well as their ratings of the presentations on 11 criteria , and feedback about the event . 
 ratings of the presentations were used to deduce rankings in analysis . 
 participants were asked to rate research proposals on 11 criteria , as well as to vote for the proposal they would vote for .  
 
FULL TEXT: 
  Keywords public participation, deliberative, engagement, decision-making, science, policy, funding, allocations, agricultural science, Australia by guest on March 5, 2014Downloaded from 2 SAGE Open chances for public involvement to genuinely influence fund- ing decisions. Some public engagement is critiqued as con- sultation that aims to rubber-stamp policymakers’ foregone conclusions (Powell & Colin, 2009; Wynne, 2006). So rather than focusing on the top 10% that certainly gets funding, pub- lic engagement could prioritize among those facing random- ness or uncertainty in the current system. The pilot described in this article could enhance public participation in science where it may otherwise be left to chance. If randomness is arguably preferable to the current norm, then public participa- tion could be better still, if only for enhancing participants’ knowledge and experience. It would also allow scientists leading these uncertain proposals to understand public per- ceptions and preferences about their research. If unfunded following deliberations, future iterations of the proposal shaped by public feedback might be more successful. The same researchers (Graves et al., 2011, p. 4) said anec- dotal evidence suggested researchers skilled at winning funding complete most of the research before applying for funding. This goes against ideals of upstream public partici- pation in science policy decisions (Pidgeon & Rogers- Hayden, 2007; Tait, 2009; Wilsdon & Willis, 2004) and further indicates little opportunity for publicly funded sci- ence to be shaped by public deliberations in the current sys- tem. The experiment described in this article served as a “proof of concept” for one possibility of how public partici- pation could be used to prioritize research proposals for funding in Australia. There are many possibilities; context explaining why this method was chosen to pilot follows. Enhancing participants’ civic and scientific knowledge is typically an aim of deliberative public processes in science. In addition to academic policy reasons outlined above, there should be practical benefits for individuals participating in deliberations, whether as scientists or laypeople. These prac- tical benefits were drivers for this particular experiment. Frustration of scientists at the Australian Centre for Plant Functional Genomics about the state of public dialogue about biotechnology meant they were open to participating in experiments with public engagement. So 2008 saw a shift from media-focused communications and lecture-based pub- lic interactions toward experiments with deliberative pro- cesses. These experiments aimed to demonstrate that deliberative public engagement could be useful for scientists as well as participating members of the public. The public could gain knowledge and experience with deliberative deci- sion-making about science, whereas scientists could better understand other citizens’ perspectives and see how their research is perceived when presented firsthand, rather than through media filters. The U.K. model modified for use in Australia came from the Institute for Food Research, linked to the John Innes Centre, another center doing plant genom- ics research. This connection helped to legitimize this method of deliberative public engagement as a starting point for par- ticipating scientists and management in Australia. Method The method used was brief in comparison with many types of deliberative public engagement that run for more than 1 day, which may require participants to consider informa- tion before and between events. In contrast, this method took no more than 3 hours of participants’ time. This has benefits for both participants and organizers, as well as shortcomings, which will be discussed later. The event format, which was repeated 3 times across 2 cities, is as follows: 0:00 Participants arrive and have time to choose a seat, read the research project information sheet, fill in the pre-event survey, and interact with other participants. 0:20 The facilitator welcomes the participants and intro- duces the project, emphasizing the importance of par- ticipation and introducing the presenting researchers. 0:25 The first scientist presents his or her proposal. 0:40 The facilitator asks participants to discuss potential questions with each other. 0:45 Questions relevant to the specific project are answered by the scientist; those the facilitator consid- ers relevant to all of the projects are recorded for later. 0:55 Repeat Steps 3 to 5 with second scientist. 1:25 Repeat Steps 3 to 5 with third scientist. 1:55 Questions that were recorded earlier are addressed, with each scientist given right of reply to each ques- tion, and other participants given the chance to ask follow-up questions. 2:20 The formal deliberative component is finished. Public participants record their private vote on slips of paper, which are collected by volunteers. They are asked to fill in the second survey while the votes are being counted, and to give these surveys to circulating volunteers once completed. 2:30 After the votes are counted twice, the outcome of vot- ing is announced. Participants are invited to stay and chat with the scientists and each other over a drink, so discussions can continue informally and unrecorded. Differences in Event Formats There were some differences in the format of the Australian and U.K. events. The U.K. version involved recorded endorsements from local celebrities for the different proj- ects; these were not used in the Australian events as they were of limited value (Rowe, Rawsthorne, Scarpello, and Dainty, 2009, p. 236). In Australia, three scientists presented their research plans rather than four as only three suitable scientists were available for all three events. In addition, all three presented their genuine research plans; there was no deliberately dubious research project as there was in the U.K. event. No one voted for the fake project in the U.K. event by guest on March 5, 2014Downloaded from Smith 3 (Rowe et al., 2009, p. 234), so this aspect was excluded in the Australian model. In the U.K. event, it was implied that the public vote would lead to funding for the winning project. The author of this article was a public participant at the U.K. event, where there were discussions among participants about the novelty of genuinely making a decision about what science is funded. After voting, it was revealed that the event was an experi- ment to learn what the public thought about research fund- ing, rather than an event that would actually allocate funding based on public preferences. In the Australian events, partici- pants were informed from the beginning that they were par- ticipating in a research project testing a model of deliberation, although the facilitator expressed hope that such pilot events could lead to genuine allocations in future. Australian par- ticipants were asked to imagine they would be awarding AUD500,000 to the winning project. There was no evidence about whether the deception affected the outcomes of the U.K. study, so it was omitted from the Australian events. Repetition and Location Another difference between the U.K. and Australian events was repetition: There were three in Australia across two states, compared with one event in the United Kingdom. Adelaide, the capital of South Australia, hosted the largest event, whereas Canberra, the nation’s capital, hosted two smaller events. To capitalize on repetition, the order of the Australian presenters was changed on each occasion. Each presenter had the opportunity to present first, second, and last, to counter possible effects of presentation order. However, it is worth noting that although the same scientists presented the same research project on each occasion, there were inevitably subtle differences in each presentation and the manner in which it was presented as the presenters gained experience. Furthermore, it is likely that confidence levels among the presenters changed after the first event’s vote, which may have influenced presentations and voting. The events were held at public venues, away from where the research projects under deliberation would happen, to help foster open-ended discussion as suggested by Powell and Colin (2008). In Adelaide, the event happened at the National Wine Centre in the city center, which is owned by the University of Adelaide but used as a public space. In Canberra, the two smaller events happened at The Front Gallery and Cafe in Lyneham, an informal venue with no research or university links, apart from hosting National Science Week events. Canberra is renowned for its lack of a dominant city center, so the events happened in a typical sub- urban hub. Research Proposals Presented As in the U.K. event, proposals all related to agricultural and food science. In Australia, all three presented projects related to research happening at the Australian Centre for Plant Functional Genomics. Each presenting scientist reworked an existing research proposal into a 10-min pre- sentation (with 5 min leeway in reality). Each had experi- ence in public science communication and consulted with professional science communicators beforehand to ensure that information presented would be publicly accessible. The presentations and format were piloted with a local high school science class before National Science Week, after which presenters had the opportunity to tweak their presen- tations further. All proposals had two things in common: They involved some research using genetic modification (GM) and each discussed at least one potential environmental benefit. The type of GM and its role in final outcomes of the research varied, as did research applications. One involved moving genes from one type of cereal into another to enhance salt tolerance. Another involved manipulating existing genes in barley known to be involved in producing beta-glucan, with applications for human health and biofuel production. Another involved investigating the genomes of different corn varieties with the aim of finding genes relevant to nitro- gen use efficiency. The presenters were selected for diversity as well as sci- ence communication talent. There was a female senior researcher, Dr Rachel Burton, who had recently published research in the journal Science. A male senior researcher, Dr Trevor Garnett, was leading a project with funding from a private company as well as government. A male graduating PhD student, Darren Plett, was working on a collaborative project with the University of Cambridge. Each scientist was encouraged to share information such as the above about col- laborations and past research achievements, as this type of information is used to inform decision-making in current funding models. Public Participants There were 85 formal participants in total across the three events. The majority, 57, participated in Adelaide, whereas the two events in Canberra had 20 and 8 formal participants, respectively. Participants were considered formal if they reg- istered prior to the event and participated in the voting and survey processes. There were several informal participants who did not complete surveys, mostly at the smaller venue in Canberra. At the larger Adelaide event, participants were seated at round tables of eight, which facilitated discussion among those on the table about presentations and ques- tions. At the more intimate Canberra venue, participants were spread on sofas and chairs around a room; conversa- tion among them was limited to those nearby. In addition to the public participants, presenting scientists and facili- tator, event volunteers, and hospitality staff participated informally. by guest on March 5, 2014Downloaded from 4 SAGE Open Survey Method Participants completed surveys before and after the events, which were designed to take 10 min to complete. The design and text of the surveys are openly available online (Smith, 2013a, 2013b). The pre-event survey included 15 questions covering demographics, political participation, participants’ own areas of expertise, and science funding issues. The post- event survey included 13 questions asking for the partici- pant’s voting decision, their ratings of the presentations on 11 criteria, and feedback about the event. The survey ques- tions were designed to facilitate comparison with data from the U.K. event, detailed analysis of which is beyond the scope of this article. In questions rating the presentations, survey respondents could rate all presenters the same. These ratings were used to deduce rankings in analysis. These quantitative questions were complemented by open-ended questions, notably, for example, asking why people voted for the proposal they did. Participants also filled in a separate voting slip, the results from which were announced at the end of the event. The sur- veys before and after were collected for later analysis. Selecting Participants Rowe, Rawsthorne, Scarpello, and Dainty (2009) noted that a shortcoming of their event was the lack of participant rep- resentativeness. Longstaff and Burgess (2009) discussed in detail challenges in recruiting participants. A common prob- lem in public engagement with science is that the same audi- ences are repeatedly attracted, whereas other types of people are rarely engaged. Left to self-selection, participants are more likely to be middle class and well educated, to be mem- bers of political parties or lobby groups, and to have previ- ously interacted with their local government than the average citizen, thus already having greater chance for input into policy than other members of the public (Adams, 1989). Thus, fair recruitment should be an important consideration for organizers of public engagement with science, as Nagel (1992) argued: If policymaking were always just a matter of finding neutral, technical solutions to common problems, then disproportionate involvement of educated people would be desirable because of their superior competence. More often, however, policy choices depend on interests and values not universally shared. Education is statistically associated with higher income and occupational status, as well as with distinctive cultural tastes. Thus, participation based on intensive, deliberative forms of citizen participation will usually neglect the needs and desires of more plebeian members of the population, unless the process is carefully structured to counteract the normal bias in favour of the well-educated. (p. 1969) Given such problems, a method of counteracting this bias was tested by actively seeking participants without a background in science. Events were publicized in community services such as event listings in media, as well as through university services. Social networks were used to share infor- mation, where people were encouraged to invite friends with- out a background in science, emphasizing it was designed for them. On registering interest, prospective participants were sent an information sheet explaining what would happen at the event and why; the preference for people without science degrees was reiterated. People with science degrees who inquired about participating were put on a waiting list. Participants were able to specify seating preferences and register to participate in a group. Allowing participants to register in groups, in line with snowball sampling methods (Atkinson & Flint, 2001), drew individuals unlikely to attend such an event normally. It is probable that “seed” participants who drew others to participate were more actively or confi- dently involved in deliberation at the event; so although such recruitment draws a wider range of people, power imbal- ances in participation may result. Given that snowball sam- pling recruits participants through social networks, isolated individuals were unlikely to be engaged using this method. Results Raw responses from before and after surveys were collected along with voting slips. There were 85 participants who par- ticipated in the surveys and voting processes across the three events. Response rates to the whole surveys were analyzed to assess participants’ engagement. Individual responses to open questions were analyzed and processed to determine word frequency. Quantitative data were inputted to Microsoft Excel then analyzed using SPSS. The relatively small sample size in this pilot study limits significance of quantitative results, without supporting evidence from qualitative results. Researchers interested in meta-analysis or similar use of the quantitative anonymized data are encouraged to contact the author. There is limited discussion in this article of quantita- tive data in isolation, given the sample size. With What Aspects of the Surveys and Format Did People Engage the Most? A few participants responded to open-ended and Likert-type questions but not rating questions. Of the 85 formal partici- pants, 80 consistently responded to questions quantitatively rating the presentations, demonstrating the majority’s inter- est and capacity to rate research proposals in the requested manner. Participants were asked to vote for only one of the three presenters, reflected in the question wording, which of the three projects did you vote for? Although in theory partici- pants could have written “all” or “none” or could have abstained from marking the vote paper, all 85 participants responded in the manner requested with a single, valid vote. by guest on March 5, 2014Downloaded from Smith 5 This 100% response rate to the voting question suggests that participants valued the voting process. Who Participated? Participants were asked about their attitudes to science and politics in the pre-event survey. When asked about how they would rate their knowledge of science, 37% said they had average knowledge and the rest were split between rating themselves either above (30%) or below (28%) average. Only 5% reported below average interest in science. Regarding political involvement in Australia, 34% of people considered themselves average, 35% considered themselves more, and 25% less than average. Participants were asked in the pre-event survey to list three areas of their own expertise, in an open-ended response. This was done instead of asking directly about profession for two reasons. First, people with science qualifications can move into other professions. People without qualifications in science may also develop expertise through their experi- ences, as patients, for example. Second, asking participants to consider their own areas of expertise ahead of interacting with scientists was designed to promote feelings of compe- tence and the concept of lay expertise. A higher than average level of education (Australian Bureau of Statistics, 2010a) is inferred from the number of responses in areas including law, policy, education, and information technology (IT). Such professional areas of expertise were more commonly listed first, with topics such as travel or sport listed second or third. This inference about above-average education levels was supported by quantitative questions. People were asked to tick their educational experiences, more than one if applica- ble. A third (33%) had postgraduate experience, while only one person reported not completing high school. Some par- ticipants chose not to answer the education question. Of 85 participants across three events, 48 were women and 32 were men; 5 people didn’t specify their gender. People born between 1975 and 1984 were overrepresented compared with Australia’s general population (Australian Bureau of Statistics, 2010b), comprising more than 40% of the partici- pants, while 18% were born between 1949 and 1957. Adults born in the 1960s or before 1940 were the least represented. In Canberra, events did not reach capacity, so some people with science degrees were invited from the waiting list to par- ticipate. The Adelaide event reached capacity with people who had not indicated they had science degrees. Calling for partici- pants without science qualifications seemed effective, with some exceptions. Assumed exceptions included people who listed an area of expertise as epidemiology or microbiology. When Did People Participate? The smallest event was held in Canberra over lunchtime; this event had the highest number of drop-ins. At least 6 people were noted coming and going during this event, typically sitting to watch parts of the presentations, without participating in the surveys, voting, or asking questions. Evening events were better attended than lunchtime events, as 77 people participated in the evening as opposed to 8 dur- ing the day—that is 90.5% participating in the evening. However, people were more likely to stop by during the day, so less formal events that do not require committed participa- tion may suit lunchtimes better. Which Research Proposal Was Preferred? Voting slips counted at the end of the events were compared with reported votes on the second surveys later. Votes on the slips and surveys were consistent. Across all three events, regardless of presentation order, one project consistently received the most votes. At each of the three events and over- all, the research proposal about increasing salinity tolerance received the most votes (52 in total). The nitrogen use effi- ciency proposal received one more vote (17) than the beta- glucan proposal (16). How Did Participants Rate the Research Proposals? Similar to the U.K. study, participants were asked to rate each of the three presentations on 11 criteria. Rating was on a scale between 1 and 5, in which 1 represented excellent and 5 bad. The criteria are listed in Table 1. These criteria reflect those used in the U.K. study (Rowe et al., 2009), covering benefits to society or environment; likeability, trustworthiness or persuasiveness of the researcher; whether participants found the talk understandable, interest- ing, or personally relevant; and whether the research would have timely outcomes, be innovative, or profitable. Participants’ ratings of the research proposals on the 11 criteria listed in Table 1 were aggregated. Averages (means) of the ratings for each research proposal were calculated. These were sorted into a list with the highest rating for a research proposal on a given criteria at the top, shown in Table 2. Table 1. Criteria on Which Participants Were Asked to Rate the Research Proposals. 1. My understanding; 2. Benefit to society; 3. Benefit to environment; 4. Personal relevance; 5. Speaker persuasiveness; 6. Research innovation; 7. Likeability of researcher; 8. Interesting talk; 9. Speaker trustworthiness; 10. Timely outcomes; and 11. Potential for profit. by guest on March 5, 2014Downloaded from 6 SAGE Open Table 2 shows the most popular proposal about salinity was rated equally best for “my understanding” and “benefit to society.” The nitrogen proposal rated the best on “benefit to environment,” which ranked third highest overall, fol- lowed by “interesting talk” for the salinity project. The nitro- gen proposal rated the best on “speaker trustworthiness” and “speaker persuasiveness,” despite this proposal not receiving as many votes as the salinity project. The ratings were accompanied by qualitative questions, one of which directly asked participants why they voted the way they did. Analyzing the frequency of words in this quali- tative data revealed that the 10 words most used in describ- ing decision-making were benefit (12), research (8), salinity (8), relevance (7), issues (7), Australia (6), important (6), global (5), interesting (5), and problem (5). As these indicate, words with the highest frequency related to the proposal content rather than the presenter. However, the presence of “interesting” in the top 10 could relate to presentation style rather than substance. Qualitative responses suggested influence of a factor in participants’ decision-making that was unaddressed in the rating criteria. Location of benefits was something people reported as influ- encing their voting decisions, reflected in the frequency of respondents using the words “Australia” and “global” in context with the word “relevance.” Did the Presenter Influence Decision-making? Voting outcomes from both the U.K. and Australian studies revealed the most junior male researchers garnered the most public support and the most senior female researchers the least. The poor result for the most experienced female in the U.K. study was confounded by the fact that she presented a deliberately dubious project; a factor that was removed from the Australian experiment. Small sample sizes limit the sig- nificance of this finding. However, it is worth flagging given evidence about barriers facing women in science (Clark Blickenstaff, 2005; Murray & Graham, 2007) and how eth- nicity biases research funding decisions (Ginther et al., 2011; Viner, Powell & Green, 2004). Interestingly in the Australian experiment, the most successful proposal was pitched by a scientist with a foreign (North American) accent, whereas in the U.K. study (Rowe et al., 2009), the most successful pro- posal was pitched by a Black scientist (not discussed in the cited article, but known from author participation in that study). Although voters consciously justified decisions by discussing the content of proposals, unconscious bias based on characteristics of presenting scientists cannot be ruled out. How Did Participants Evaluate the Event? Participants were asked to rate the event they attended with three Likert-type questions and one open-ended question (Smith, 2013b). They were asked about enjoyment and like- lihood of attending again. They were also asked their likeli- hood of attending again without food and drink as incentives. The open-ended question requested suggestions for improve- ment. Feedback was positive; 87% of the participants rated their enjoyment as above average and no one reported below average. The same percentage said they were likely to attend a similar event again, although two people were unlikely to attend again. However, when asked whether they would attend without the meal incentive, less than half of the par- ticipants (46%) said they would be likely to attend; 12% would be unlikely to; 11% would not. Participants were asked how useful they thought such a process would be for making real funding decisions. More than half thought so (54%), although 29% were unsure, 12% thought it would not be of much use and 5% thought it would be useless. Of those who did not think it would be useful, concerns were that the event was too brief and information Table 2. Average Criteria Ratings for Each Project, From Highest Vote to Lowest. Criteria for each research project Score (1 = excellent, 5 = poor) My understanding: Salinity project 1.71 Benefit to society: Salinity project 1.71 Benefit to environment: Nitrogen project 1.85 Interesting talk: Salinity project 1.89 My understanding: Nitrogen project 1.98 Benefit to society: Nitrogen project 2.01 Benefit to environment: Salinity project 2.03 Interesting talk: Nitrogen project 2.03 Likeability of researcher: Salinity project 2.04 Speaker trustworthiness: Nitrogen project 2.05 Potential for profit: Salinity project 2.06 Likeability of researcher: Nitrogen project 2.06 Speaker trustworthiness: Salinity project 2.08 Benefit to society: Beta-glucan project 2.1 Speaker persuasiveness: Nitrogen project 2.13 Speaker trustworthiness: Beta-glucan project 2.21 Potential for profit: Nitrogen project 2.23 Benefit to environment: Beta-glucan project 2.23 Speaker persuasiveness: Salinity project 2.24 Likeability of researcher: Beta-glucan project 2.25 Research innovation: Salinity project 2.31 Potential for profit: Beta-glucan project 2.34 Research innovation: Nitrogen project 2.36 Timely outcomes: Salinity project 2.4 Research innovation: Beta-glucan project 2.41 My understanding: Beta-glucan project 2.46 Interesting talk: Beta-glucan project 2.48 Personal relevance: Salinity project 2.55 Timely outcomes: Nitrogen project 2.58 Personal relevance: Nitrogen project 2.69 Timely outcomes: Beta-glucan project 2.7 Speaker persuasiveness: Beta-glucan project 2.74 Personal relevance: Beta-glucan project 2.8 by guest on March 5, 2014Downloaded from Smith 7 too shallow for people to make an informed decision. Some were concerned that presentation style might influence deci- sions. Feedback from those who thought it would be useful reported what they learnt from the process. They also dis- cussed the importance of democracy and giving everyday people voices in decision-making, as well as how the process promoted a sense of community. Discussion and Recommendations This pilot “proof of concept” experiment highlights several areas of improvement and further research. The criteria par- ticipants rate proposals on could be modified. Different recruitment methods could improve representation. The impact of presenter diversity on decision-making could be further explored. Venues could be compared, with informal participation in mind. Deliberations among publics and experts or policymakers could be compared. The impact of deliberation length and depth could be explored, drawing on research in psychology, political science, and behavioral economics. Benefits Based on Location Participants were asked to quantitatively rate proposals for personal relevance but not about relevance for people in dif- ferent places or societies. Qualitative survey feedback as well as discussions during the events suggested that deci- sions were influenced by where benefits would happen. So a novel criterion about which participants could be asked to rate proposals would be location of benefits. Doing so could link this type of research with that regarding public prefer- ences for aid budgets (Brautigam, 1992; Rye Olsen, 2001). Recruiting People Without Science Degrees to Improve Representation Although successful in attracting people without a formal background in science, participants were more educated than average, and particular age groups were underrepresented. Future experiments to refine the method could recruit par- ticipants with no tertiary qualifications to better overcome the dominance of professionals. This experiment lacks repre- sentativeness in contrast to stratified random sampling, a method used for example in prioritizing health spending (Mitton, Smith, Peacock, Evoy, & Abelson, 2009). Presenter Diversity This experiment sought diverse presenters to showcase the diversity of people who work in science and seek funding for research. Participants’ reasons for their decisions appeared to be based on the content of proposals, consistent with reasoning used to make real research funding deci- sions. However, research shows that biases affect decisions, often unconsciously (Burgess, Ryn, Dovidio, & Saha, 2007; Fiske, 2002; Green, Pallin, Raymond, Iezzoni, & Banaji, 2007; Jost, Banaji, & Nosek, 2004; Krieger, 1995). This experiment did not explore the affective nature of decision- making (De Martino, Kumaran, Seymour, & Dolan, 2006). Emotional factors are likely stronger when proposals are communicated by a person rather than in writing. Future experiments could group demographically similar research- ers together to present their proposals, reducing variables of gender, age, and ethnicity. To test whether such variables do affect decision-making in this model, three labs with inter- nal diversity could work together to present a research pro- posal from each lab. The same proposals could be presented by different researchers within the respective labs at differ- ent events. Regardless of how this affects voting outcomes, participating scientists may enhance their science communi- cation skills through seeing the same proposal presented and received via diverse perspectives. Actively acknowledging diversity and its role in decision-making may also improve participants’ satisfaction (Abdel-Monem, Bingham, Marincic, & Tomkins, 2010). Including Informal Participants Volunteers and venue staff can also benefit from participa- tion. At one event, a waitress became demonstrably engaged in the event, actively seeking out presenting scientists after formal deliberations to discuss a scientific question. Powell and Colin (2008) said that public engagement events should happen away from research centers so they are less intimi- dating to those generally disengaged from science. This also potentially benefits venue staff who may have little exposure to science, as opposed to those working in science centers who are exposed daily. Comparing Deliberations of Different Groups This experiment was a proof of concept for the idea that members of the public can make decisions about funding using similar values to those of experts or policymakers. A variation could involve using the same method to elicit deci- sions from groups of policymakers and experts, as well as groups of public participants. The acceptability and credibil- ity of such a public method for making decisions could be influenced by understanding similarities or differences between voting outcomes depending on who decides. The Impact of Brevity Participation for a few hours during a meal is more realistic for time-poor participants than multi-day jury processes. Even busy people need to eat. Researchers have discussed the role of incentives in participatory processes (Powell & Colin, 2008); providing a meal during the event is a form of this. Mansbridge (1973) observed that the time spent by guest on March 5, 2014Downloaded from 8 SAGE Open in participatory decision-making alienates many people, particularly when there is little social incentive. Kleinman, Delborne, and Anderson (2009) assessed events using a free meal or paid child care as incentives and found they can strongly influence participation. Event organizers are also likely to benefit from shorter events. The costs of hiring a venue, arranging catering, and coordinating speakers, participants, and staff are minimized. Dietrich and Schibeci (2003) questioned the value of consen- sus conferences, arguing the cost and effort involved can be prohibitive, particularly for reiterative processes. Elster (1998) said in his book Deliberative Democracy, Whereas scientists can wait for decades and science can wait for centuries, politicians are typically subject to strong time constraints, in two different senses. On the one hand, important decisions tend to be so urgent that one cannot afford to discuss them indefinitely. On the other hand, less important decisions do not justify lengthy deliberations. As I observed earlier, the importance of time in political life implies that, in addition to deliberation, voting as well as bargaining inevitably has some part to play. (p. 9) Decision-making about science funding, at the interface of science and politics, does not wait for decades or centuries. So exploring the practicalities of how to make such decisions in cost- and energy-efficient timeframes is worthwhile. However, voting events during a meal allow minimal time for deliberation and limited consideration of new facts and viewpoints. There is a trade-off between accessibility and the benefits of a more in-depth deliberation process. An empiri- cal study of making decisions using the Delphi technique found that four rounds of questions and feedback were gen- erally the best; two iterations rarely achieve a stable outcome (Erffmeyer, Erffmeyer, & Lane, 1986). The optimal amount of time and available information for decision-making is debatable and depends on context. Satisfaction with information and decisions does not always correlate with good decisions (O’Reilly, 1980; Stumpf & Zand, 1981). Participants have differing percep- tions of time’s value (Elster, 1998). The amount of time peo- ple have to make decisions affects how much information they can handle before feeling overloaded (Buchanan & Kock, 2001; Eppler & Mengis, 2004). Some psychological research suggests having too much information can nega- tively affect decision-making (Lyengar & Lepper, 2000; Wilson & Schooler, 1991). People may make better deci- sions subconsciously than they do with conscious delibera- tion (Dijksterhuis, 2004). There may be inconsistencies between which methods participants prefer and which result in best decisions (Erffmeyer & Lane, 1984; Tjosvold & Field, 1983). The amount of information or time preferred for decision-making may vary with age (Cassileth, 1980) or culture (Gambetta, 1998), suggesting different deliberative processes may favor different demographics. There are diverse academic and practical perspectives on ideal lengths, depths, and types of deliberations, as well as how these vari- ables shape decision quality. Given this, concise deliberative processes have a place in typologies of public engagement methods (Rowe & Frewer, 2005). Conclusion Three events experimented with a model of deliberative par- ticipation in decision-making about science funding. These events were much shorter than most deliberative processes, which had benefits for participants and organizers in costs in money and time, but penalties regarding the depth of delib- erations. Recruitment was targeted at those without science degrees, but participants were still more educated than Australia’s average. Results were consistent across three rep- etitions of the event format with different publics. Participants’ reports of their decision-making suggested their decisions were influenced by potential benefits of the planned research, as well as their understanding. Research proposals that were better understood were rated higher. Participants indicated concern for where the benefits of research flow, for example, whether the benefits have an impact on their own community or internationally. Although participants were not unanimous in thinking the method would be useful for real funding decisions, most enjoyed their experience and would participate again. However, without the same meal incentive, many would not participate again. This experi- ment demonstrated participants’ capacity to rate research proposals on the type of criteria used in deciding what research proposals governments fund. Quality of delibera- tions and unconscious factors may have affected decision- making in unobserved ways. However, these biases have been shown in existing decision-making processes. Other researchers have shown inefficiencies in research funding processes, even suggesting some randomization of funding allocations as an improvement. Given that participants val- ued this deliberative process and participation in decision- making, it is arguably preferable to randomization. Civic benefits from public involvement in prioritizing research proposals may improve the value of grant allocation pro- cesses in democracies. Acknowledgments The support of the Australian Centre for Plant Functional Genomics is gratefully acknowledged. Thanks to Gene Rowe and Dee Rawsthorne from the Institute for Food Research for support and advice on the U.K. model. Thanks to Australian National University’s Statistical Consulting Unit for advice on survey design. Thanks to the Australian National Centre for Public Awareness of Science community for encouragement and advice, particularly my supervisor Dr Rod Lamberts. Many thanks to the presenting scientists and event volunteers. Thanks also to reviewers who provided feedback on the draft of this article, both through SAGE Open and also Public Understanding of Science, from which this article was withdrawn due to the journal’s open access policy in 2012. by guest on March 5, 2014Downloaded from Smith 9 Declaration of Conflicting Interests The author(s) declared the following potential conflicts of interest with respect to the research, authorship, and/or publication of this article: The author was a freelance science journalist paid through the University of Cambridge to report on the U.K. deliberative event referred to in this article for a [now defunct] magazine, before commencing this research. During writeup of this research the author was employed by the Royal Institution of Australia funded through the Australian Department of Industry, Innovation, Science, Research and Tertiary Education. Funding The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: The author was employed by the University of Adelaide as Communications Manager for the Australian Centre for Plant Functional Genomics during the start of this research. References Abdel-Monem, T., Bingham, S., Marincic, J., & Tomkins, A. (2010). Deliberation and diversity: Perceptions of small group discussions by race and ethnicity. Small Group Research, 41, 746-776. Adams, L. (1989). Healthy cities, healthy participation. Health Education Journal, 48, 179-182. Atkinson, R., & Flint, J. (2001, Summer). Accessing hidden and hard-to-reach populations: Snowball research strategies (Social Research Update, Issue 33). Retrieved from http://sru .soc.surrey.ac.uk/SRU33.pdf Australian Bureau of Statistics. (2010a). Education and work (Cat. No. 6227.0). Canberra, Australia: Author. Australian Bureau of Statistics. (2010b). Population by age and sex, regions of Australia (Cat. No. 3235.0). Canberra, Australia: Author. Bowman, D. M., & Hodge, G. A. (2007). Nanotechnology and pub- lic interest dialogue: Some international observations. Bulletin of Science, Technology & Society, 27, 118-132. Brautigam, D. (1992). Governance, economy, and foreign aid. Studies in Comparative International Development, 27(3), 3-25. Buchanan, J., & Kock, N. (2001). Information overload: A deci- sion-making perspective. In M. Köksalan & S. Zionts (Eds.), Multiple criteria decision-making in the new millennium (pp. 49-58). Berlin, Germany: Springer Berlin Heidelberg. Burgess, D., Ryn, M., Dovidio, J., & Saha, S. (2007). Reducing racial bias among health care providers: Lessons from social- cognitive psychology. Journal of General Internal Medicine, 22, 882-887. Cassileth, B. R. (1980). Information and participation prefer- ences among cancer patients. Annals of Internal Medicine, 92, 832-836. Clark Blickenstaff, J. (2005). Women and science careers: Leaky pipeline or gender filter? Gender and Education, 17, 369-386. Cormick, C. (2009). Piecing together the elephant: Public engage- ment on nanotechnology challenges. Science and Engineering Ethics, 15, 439-442. De Martino, B., Kumaran, D., Seymour, B., & Dolan, R. J. (2006). Frames, biases, and rational decision-making in the human brain. Science, 313, 684-687. Dietrich, H., & Schibeci, R. (2003). Beyond public perceptions of gene technology: Community participation in public policy in Australia. Public Understanding of Science, 12, 381-401. Dijksterhuis, A. (2004). Think different: The merits of unconscious thought in preference development and decision-making. Journal of Personality and Social Psychology, 87, 586-598. Elster, J. (1998). Introduction. In J. Elster (Ed.), Deliberative democracy (pp. 1-18). Cambridge, UK: Cambridge University Press. Eppler, M. J., & Mengis, J. (2004). The concept of information overload: A review of literature from organization science, accounting, marketing, MIS, and related disciplines. The Information Society, 20, 325-344. Erffmeyer, R. C., Erffmeyer, E. S., & Lane, I. M. (1986). The Delphi technique: An empirical evaluation of the optimal number of rounds. Group & Organization Management, 11, 120-128. Erffmeyer, R. C., & Lane, I. M. (1984). Quality and acceptance of an evaluative task: The effects of four group decision-making formats. Group & Organization Management, 9, 509-529. Fiske, S. T. (2002). What we know now about bias and intergroup conflict, the problem of the century. Current Directions in Psychological Science, 11, 123-128. Gambetta, D. (1998). Claro! An essay on discursive machismo. In J. Elster (Ed.), Deliberative democracy (pp. 19-43). Cambridge, UK: Cambridge University Press. Ginther, D. K., Schaffer, W. T., Schnell, J., Masimore, B., Liu, F., Haak, L. L., & Kington, R. (2011). Race, ethnicity, and NIH research awards. Science, 333, 1015-1019. Gordon, R., & Poulin, B. J. (2009). Cost of the NSERC science grant peer review system exceeds the cost of giving every qual- ified researcher a baseline grant. Accountability in Research, 16, 13-40. Graves, N., Barnett, A. G., & Clarke, P. (2011). Funding grant pro- posals for scientific research: Retrospective analysis of scores by members of grant review panel. British Medical Journal, 343. Retrieved from http://www.bmj.com/content/343/bmj .d4797 Green, A. R., Pallin, D. J., Raymond, K. L., Iezzoni, L. I., & Banaji, M.R. (2007). Implicit bias among physicians and its prediction of thrombolysis decisions for black and white patients. Journal of General Internal Medicine, 22, 1231-1238. Herbert, D. L., Barnett, A. G., & Graves, N. (2013). Funding: Australia’s grant system wastes time. Nature, 495, 314. Jost, J. T., Banaji, M. R., & Nosek, B. A. (2004). A decade of system justification theory: Accumulated evidence of con- scious and unconscious bolstering of the status quo. Political Psychology, 25, 881-919. Kleinman, D. L., Delborne, J. A., & Anderson, A. A. (2009). Engaging citizens: The high cost of citizen participation in high technology. Public Understanding of Science, 20, 221-240. Krieger, L. H. (1995). The content of our categories: A cognitive bias approach to discrimination and equal employment oppor- tunity. Stanford Law Review, 47, 1161-1248. Longstaff, H., & Burgess, M. (2009). Recruiting for representa- tion in public deliberation on the ethics of biobanks. Public Understanding of Science, 19, 212-224. Lyengar, S. S., & Lepper, M. R. (2000). When choice is demoti- vating: Can one desire too much of a good thing? Journal of Personality and Social Psychology, 79, 995-1006. by guest on March 5, 2014Downloaded from 10 SAGE Open Lyons, K., & Whelan, J. (2010). Community engagement to facili- tate, legitimize and accelerate the advancement of nanotech- nologies in Australia. NanoEthics, 4, 53-66. Mansbridge, J. J. (1973). Time, emotion, and inequality: Three problems of participatory groups. The Journal of Applied Behavioral Science, 9, 351-368. Mitton, C., Smith, N., Peacock, S., Evoy, B., & Abelson, J. (2009). Public participation in health care priority setting: A scoping review. Health Policy, 91, 219-228. Murray, F., & Graham, L. (2007). Buying science and selling sci- ence: Gender differences in the market for commercial science. Industrial and Corporate Change, 16, 657-689. Nagel, J. (1992). Combining deliberation and fair representation in community health decisions. University of Pennsylvania Law Review, 140, 1965-1985. O’Reilly, C. A. (1980). Individuals and information overload in organizations: Is more necessarily better? Academy of Management Journal, 23, 684-696. Pidgeon, N., & Rogers-Hayden, T. (2007). Opening up nanotech- nology dialogue with the publics: Risk communication or “upstream engagement”? Health, Risk & Society, 9, 191-210. Powell, M. C., & Colin, M. (2008). Meaningful citizen engagement in science and technology: What would it really take? Science Communication, 30, 126-136. Powell, M. C., & Colin, M. (2009). Participatory paradoxes. Bulletin of Science, Technology & Society, 29, 325-342. Rowe, G., & Frewer, L. J. (2005). A typology of public engage- ment mechanisms. Science, Technology & Human Values, 30, 251-290. Rowe, G., Rawsthorne, D., Scarpello, T., & Dainty, J. R. (2009). Public engagement in research funding: A study of public capa- bilities and engagement methodology. Public Understanding of Science, 19, 225-239. Russell, A., Vanclay, F., Salisbury, J., & Aslin, H. (2011). Technology assessment in Australia: The case for a formal agency to improve advice to policy makers. Policy Sciences, 44, 157-177. Rye Olsen, G. (2001). European public opinion and aid to Africa: Is there a link? The Journal of Modern African Studies, 39, 645-674. Schibeci, R., & Harwood, J. (2007). Stimulating authentic commu- nity involvement in biotechnology policy in Australia. Public Understanding of Science, 16, 245-255. Smith, C. (2013a). Public engagement in prioritising research pro- posals: Survey 1. Retrieved from http://dx.doi.org/10.6084/ m9.figshare.844619 Smith, C. (2013b). Public engagement in prioritising research pro- posals: Survey 2. Retrieved from http://dx.doi.org/10.6084/ m9.figshare.844620 Stumpf, S. A., & Zand, D. E. (1981). Participant estimates of the effectiveness of judgmental decisions. Journal of Management, 7, 77-87. Tait, J. (2009). Upstream engagement and the governance of sci- ence. EMBO Reports, 10(S1), S18-S22. Tjosvold, D., & Field, R. H. G. (1983). Effects of social context on consensus and majority vote decision-making. Academy of Management Journal, 26, 500-506. Viner, N., Powel, P., & Green, R. (2004). Institutionalized biases in the award of research grants: A preliminary analysis revisiting the principle of accumulative advantage. Research Policy, 33, 443-454. Wilsdon, J., & Willis, R. (2004). See-through science: Why pub- lic engagement needs to move upstream (Technical report). London, England: Demos. Wilson, T. D., & Schooler, J. W. (1991). Thinking too much: Introspection can reduce the quality of preferences and deci- sions. Journal of Personality and Social Psychology, 60, 181-192. Wynne, B. (2006). Public engagement as a means of restoring pub- lic trust in science: Hitting the notes, but missing the music? Community Genetics, 9, 211-220. Author Biography Cobi Smith is a PhD student in science communication at Australian National University. Her research interests include deliberative democracy, collaborative learning, participatory evaluation and decision-making. by guest on March 5, 2014Downloaded from 