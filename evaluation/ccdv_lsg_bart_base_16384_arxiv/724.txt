ABSTRACT: 
The increasing prominence given to student satisfaction at UK Universities as a response to the introduction of fees and the growing stature of league tables has led to a desire to understand the factors that affect the quality of the student experience. Therefore, this paper examines whether students who study at universities in the UK where research is highly rated or where a high proportion of faculty are professionally qualified are more satisfied, measuring satisfaction through data from the National Student Survey. Our key results are first, that students are happiest at pre-1992 universities outside the Russell group and where the amount of top-rated research is lower. Second, we uncover no link between student contentment and the percentage of faculty holding formal teaching qualifications. Our findings have important implications for university policies regarding the link between research and teaching and for the current drive to ‘professionalise’ teaching in higher education. 
 
PREDICTION: 
 ' ' '' ''    ' '' ''    ' '' ''    ' '' ''  
 
FULL TEXT: 
  JEL classifications: C52, I21, I23 Keywords: National Student Survey, student satisfaction, professional teaching qualifications, teaching quality. Acknowledgements We would like to thank seminar participants at the Universities of Reading and Bath for insightful comments. We are grateful for helpful discussions with Cherry Bennett, Maxine Davies, Nathan Helsby, Eileen Hyder and Claire McCullogh. We also thank Amanda Chetwynd, Nicola Dandridge, Tony Moore and James Walker for detailed comments on earlier drafts. 1 1. Introduction Universities are naturally keen to maximise student satisfaction, not only to improve their positions in the rankings, but also to lock into a virtuous circle where their reputations are enhanced and with them the number and quality of future student applications. Results from the National Student Survey (NSS), a broad-based measure of student satisfaction used across all UK universities and discussed in detail below, are given increasing importance as key ingredients in many national league tables of universities (including the Guardian University Guide and the Complete University Guide), and indeed Gibbons et al (2015) find that the overall ranking of institutions is one of the most important drivers for university choice by students. Moreover, the NSS results are just one aspect of the Key Information Sets (KIS) that all universities are required to present to prospective applicants These KIS also include the actual cost (for the course and living expenses) and post-graduation employment rates (Taylor and McCaig, 2014). Further, student satisfaction is now a major driver of the Teaching Excellence Framework (TEF) -which has recently been introduced by the British government in order to measure the effectiveness of universities and concentrating on the student experience. The new TEF draws heavily on distinct elements of the NSS for perceived insights into the student experience and adds to the number of indicators now being used to ensure accountability within UK higher education. The assessment of the TEF (Gold, Silver or Bronze) will also impact on the fee level chargeable to students and thus university income – with qualifying institutions allowed to increase the home/EU fee by the rate of inflation. With students increasingly viewed as customers, it means that feedback from students and knowing what drives their satisfaction are core concerns for all universities in the UK. The body of academic work in this area of student satisfaction is surprisingly not substantial, especially in the UK context, with notable exceptions being Bell and Brooks (2017), Borden (1995), Hewson (2011) and Fielding et al. (2010). Work on the individual level dataset from the NSS does reveal that the results are eliable year-on-year and therefore stable over time (Cheng and Marsh, 2010; Surridge, 2009) From the students’ own perspective, according to the Student Academic Experience Survey (Buckley et al, 2015), 39% of those responding rated ‘formal training to teach’ as the key characteristic that they sought, against only 17% rating their teachers being currently involved in research as most important. A considerable number of students believed that additional training for university teachers should be prioritised as an area for expenditure rather than, for example, improvements in the physical infrastructure. According to Neves and Hillman (2016), students’ preferred method for universities to save money would be to spend on less on buildings or on sports and social facilities. The requirement for formally trained lecturers was felt most strongly, with 49% rating this highest, at Russell Group universities (a mission group of 24 UK universities representing what many would consider to be the elite institutions), whereas students at Million+ universities (the mission group representing post 1992 universities) prioritised teachers with relevant industry or professional expertise (54% rated this attribute as their top priority). The Browne Report proposed that all new academics who teach should be required to register for a teaching qualification, proposing that: ‘institutions require all new academics with teaching responsibilities to undertake a teacher training qualification accredited by the HEA [Higher Education Academy] and that the option to gain such a 2 qualification is made available to all staff - including researchers and postgraduate students - with teaching responsibilities’ (Browne 2010, p. 50). Thornton (2014) outlines how others have made similar calls. A former Chief Executive of the HEA, Craig Mahoney, told the Times Higher Education (THE) in 2010 that he ‘wanted to see every member of staff teaching in UK higher education to have formal qualifications in teaching’ (Atwood, 2010). Boffey (2012) reports that the National Union of Students (NUS) has also called for those teaching in higher education to be formally qualified. A report by the NUS and Quality Assurance Agency (QAA) in 2012 claimed, based on the results from a survey, that students wanted lecturers to improve their teaching skills: ‘Students want academic staff to develop their teaching styles to be more engaging, interactive and use technology and props to make the subject more accessible and interesting. Developing an active learning style is a teaching skill which needs to be taught and developed over time, and 34% of students in this research articulated that they wanted their lecturers to have better teaching skills’ (QAA and NUS, 2012). However, not all academics share these views. A well-cited paper by Layton and Brown (2011, p. 163-4) argues that the assumptions behind these conclusions and recommendations are simplistic and ‘mask a neoliberal agenda and culture of managerialism’. According to the Student Academic Experience Survey (Buckley et al, 2015), students claim that their preference is to be taught by qualified faculty who have strong links with the industry, with current research activity being rated as the least important attribute of the three by more than half of respondents. It is, perhaps, rather surprising given that entry requirements and demand for places are typically highest among the ‘elite research universities’ and are considerably lower among those institutions where a high proportion of faculty are professionally qualified and where faculty work closely with industry.1 This evidence also flies in the face of efforts by universities to strengthen the ‘teaching-research nexus’ and to ensure that teaching is research-led or research-informed. Indeed, in a commentary on the HEPI 2015 survey results, David Palfreyman, Director of the Oxford Centre for Higher Education Policy Studies, is quoted as saying that, ‘Universities should admit that there is no evidence to suggest a ‘magical link’ between research and good teaching’.2 In this study, we directly examine the drivers of the NSS scores with a focus on the effect of the extent to which those teaching students are professionally qualified. Since this is a survey of undergraduate student satisfaction, our focus is only on them rather than postgraduates. Thornton (2014, p.1) claims that the possession of formal teaching qualifications is ‘seen by many as key to enhancing the student experience’, in this paper, we directly test this purported linkage. It is important to note, however, that possession of a teaching qualification is increasingly a requirement of new staff but among more experienced academics, obtaining such a qualification will represent individuals' independent commitment to professional development of their teaching. Given this, the sample of those with teaching qualifications is partially self-selected, and thus it is not possible from our quantitative approach using secondary data to disentangle the effect of enthusiasm for teaching-related professional development among a sub-set of academics who care deeply about 2Times Higher Education, No. 2206, 4-10 June 2015, p.6. 3 this from the effect of the knowledge imparted during the qualification process.3 Thus it is hard to evidence the impact of pedagogic qualifications on teaching practice and quality. An alternative approach might be to adopt a qualitative research design involving a focused examination of specific individuals who have gone through the qualification process and noting any changes in their style or the way that their teaching is perceived by students; however, this is well beyond the scope of the present study and we leave this as an area for future research. Whilst we have not found evidence from studies measuring the direct impact of teaching qualifications on student satisfaction, studies have investigated the impact of teacher development and considered outcomes and the evidence for the impacts on the practice of University teachers and the learning outcomes for students (Thornton, 2012; Trigwell, 2013; Parsons et al 2012; Gibbs and Coffey, 2004). These studies have shown that academics personally benefit from such programmes and that students have learning gains as a result of the resultant learning experience. Also, Locke (2014) reports summary statistics for the demographics and employment conditions of academics in the UK but there is no attempt to link these with student satisfaction. We use a multi-layered dataset that formally tests the link between student satisfaction, the proportion of faculty with professional teaching qualifications, and the research quality of the university as well as range of other potentially relevant factors. So far, there has been no evidence on whether there exist any ‘magic links’ between research quality or intensity and student satisfaction derived from the student experience and as such, ours is the first quantitative study of its kind. Our study is necessarily very much rooted in the UK context since this provides not only a unique source of data for analysis but also this is the home of the debates described above. However, we believe that the discussion and analysis will be of relevance to the higher education sector beyond the UK since many of the tensions that we discuss are present in other countries and many are likely to follow the UK’s direction of travel. 2. Literature review As we have discussed, existing evidence on the link between research, teaching qualifications and student satisfaction is limited in its scope and where it exists, is mainly anecdotal or concentrated at the level of the individual student. The ‘magic link’ between research and teaching could manifest itself either directly through the benefits of being an active researcher on the nature and quality of the teaching, or indirectly through its effect on the brand strength of the university. Hattie and Marsh (1996) conducted a meta-analysis of 58 previous studies and concluded ‘that the common belief that research and teaching are inextricably entwined is an enduring myth. At best, research and teaching and very loosely coupled’. The closest study to our own is that of Lenton (2015), who examines the factors affecting student satisfaction using a panel of data from 2007-2010. However, her research incorporates 11 subject areas whereas we include all those captured in the NSS; more importantly, we examine a much wider range of possible drivers including variables that capture the composition of the faculty members, whether they have professional teaching qualifications (or equivalent) and the research ratings of the universities concerned. Cheng and Marsh (2010) and Surridge (2009) show that the NSS is meaningful as a measurement tool over time. Fielding et al. (2010) and Langan et al (2013) used the NSS to analyse the reported satisfaction of undergraduates 3 We thank an anonymous referee for suggesting this point. 4 studying science and engineering - finding excellent teaching was one of the key drivers, whilst quality of feedback was not significant. Earlier research that has been conducted on the purported research-teaching nexus offers mixed conclusions. Zamorski (2002) found that students had a real interest in academic research but sometimes felt ‘excluded’, while Kinchen and Hay (2007) further suggested that there was a benefit from being both a researcher and a teacher, and that academics should embrace both aspects as part of their professional role. Schapper and Mayson (2010) find strong support for research-led teaching and suggest ways in which they can be bettered ‘married’ together. Considering the indirect effect of research quality on reputation, Alves and Raposo (2010) document the importance of a university’s brand image as a driver of student satisfaction at Portuguese institutions. The study found that image is the most important influence on student satisfaction and student loyalty. Palacio et al. (2002), also examine image as a factor in student satisfaction and find that it does have an influence. In this context, they define image in terms of the student cognitive and affective impression of the university which they equate with brand image rather than using league tables. Brown and Mazzarol (2009) look at Australian universities and find that the most important driver of student satisfaction and loyalty is institutional image (p. 81). 3: The Institutional Framework for Student Evaluation and Professional University Teaching Qualifications in the UK This section discusses the three key aspects of our study which we then employ in the quantitative study in the following section: student satisfaction, teaching qualifications and research quality. 3.1 The National Students Survey The National Student Survey (NSS) is a questionnaire-based measure of student satisfaction established in 2005 at the behest of the UK government. It comprises a total of 23 questions split into seven categories. A critical discussion of the negative aspects of the NSS is given by Sabri (2013), while further detail on the nature of the NSS data, its key features and a possible new methodology for producing an overall score is provided in Bell and Brooks (2017). Our study focuses on data aggregated to the HESA subject code level within each institution so that any differences in standards of assessment between students should wash out as each score in our database comprises the average rating of many students for each course at each university. It may be the case that students at specific institutions or students in certain subject areas may have different expectations of their own student experience, but we allow for this in the analysis using fixed effects for institutions and for subject areas. In 2014, the year for which we use data, a total of 321,449 students completed the questionnaire, representing a response rate of 71%. 3.2 The HEA and its Fellowships In the UK, a professional teaching framework for academics is organised mainly via the HEA, which was established in 2004 as the successor to the Institute for Learning and Teaching and as a professional organisation to champion standards of teaching.4 The UK Professional Standards 4 J. Gill ‘Behind the scenes at the academy’, 7 August 2008, Times Higher Education. 5 Framework (UKPSF) is ‘a set of … guidelines for everyone involved in teaching and supporting learning in HE’.5 Teachers and support staff whose experiences adequately map onto these dimensions can apply for recognition as Associate Fellow, Fellow, Senior Fellow or Principal Fellow of the HEA. Note that in common with other studies, throughout this paper we refer to membership of the HEA as a teaching qualification; however, strictly this is not the case although HESA recognises it to be equivalent. It had been expected within the university sector that a key metric in the Teaching Excellence Framework (TEF) would be the proportion of academic staff holding teaching qualifications. But there was no mention of teaching qualifications or HEA membership becoming a metric within the TEF framework. One possible explanation is a serious lack of data upon which to base any policy or assessment. Currently, the Higher Education Statistical Agency (HESA), and indeed the higher education institutions themselves, simply do not know how many of their teaching staff are professionally qualified. As a leading exponent and early adopter of a target for the percentage of teaching-qualified academics, Thornton (Pro Vice Chancellor of The University of Huddersfield), describes the move by his university to use the UKPSF as a route to accreditation by the HEA to be their strategic aim for all of their academic staff (Thornton, 2014, p. 6). His paper also includes the intriguing finding that 65% of staff did not make any changes to their teaching practice as a result of the institutional process, although a wider observation was that staff involved in teaching but not directly in lecturing roles found the whole process beneficial. One could argue that this is not surprising since those obtaining HEA Fellowship at any level are already demonstrating the required skills and attributes. But he nevertheless argues that the institution’s NSS scores improved significantly over the period as did the proportion of students gaining an upper second or first class degree. So, although his paper does not attempt to find any direct link, it does sketch out the rationale for believing that there might potentially be a relationship between teaching qualifications and student satisfaction. 3.3 Research Evaluation In the UK, research quality is formally evaluated approximately once every six years through the Research Excellence Framework (REF, previously known as the Research Assessment Exercise, RAE). The evaluation is highly comprehensive and involved 154 universities which submitted their work for assessment to one of 36 subject area sub-panels. The exercise was also formerly conducted in 1986, 1989, 1992, 1996, 2001 and 2008. As well as supporting judgements about the quality of research conducted, the results of the RAE and REF exercises were used to distribute ‘QR’ research funding between institutions (see Broadbent, 2010). In the most recent exercise, REF2014, ‘quality profiles’ were produced for each submitting unit along three dimensions (outputs, research environment, and impact), which involved determining the percentage of activity under each heading that was assessed to be at each of five-star quality levels from 0 to 4*. For presentational ease and comparison, institutions and the media then translated the overall quality profiles into grade point averages by multiplying each quality score (0-4) with the proportion of work judged to be at that category. Similarly, a ‘research power’ statistic is 5 https://www.heacademy.ac.uk/recognition-accreditation/uk-professional-standards-framework-ukpsf#sthash.A71xybQ5.dpuf. 6 often calculated as the number of full-time equivalent researchers submitted to the REF multiplied by the grade point average. 4. Method 4.1 Key Variables Table 1 presents the definitions and units of measurement of the key variables we employ in the study as well as their mean values. The first group of variables is drawn from the NSS results themselves, completed by students in July 2014. We focus mainly on the overall satisfaction responses (i.e. the response to survey question 22, “Overall I am satisfied with the quality of the course”). The information from this question is typically presented in one of two ways – either the scores from the Likert-scale question are averaged to form a numerical value between one (strongly disagree, i.e. least satisfied) and five (strongly agree, i.e. most satisfied), or it is presented as the percentage of respondents who are satisfied with their course (i.e. they select either strongly agree or agree in response to this question). We primarily focus on this question since it is this which forms the basis of most of the league tables6 and is the most reported by institutions of all of the questionnaire responses. However, since the factors affecting student satisfaction we measure are focused on various attributes of the teaching faculty, we also investigate the 1-5 scale and percentage of students who are satisfied in response to “the teaching on my course” section, where the responses questions 1 to 4 are averaged. We include all groups of courses from all UK universities which possess degree awarding powers and which offer at least one undergraduate course, since the NSS is an undergraduate only student survey. Our database comprises a total of 4465 such courses or groups of courses within cognate sub-fields. Two other relevant variables from the NSS questionnaire database which we employ in our study are on whether students are registered as full- or part-time students and on the overall response rate. We include the mode of study as we believe from first-hand experience that this can have a considerable influence on student happiness since part-time students tend to be older and also concurrently in employment. In addition, the response rate is a factor of interest, as it has been reported that this variable is correlated with satisfaction (Williams and Cappuccini-Ansfield, 2007). The second group of variables described in Table 1 comprises a set of factors that capture the demographic, ethnic and contractual status of the faculty that teach the students, obtained from Higher Education Statistical Agency (HESA) and based on returns made in early 2014 by institutions for the 2013-14 academic year. A particular difficulty is that the units of measurement in the NSS and from HESA do not match. The NSS publishes survey responses organised by ‘course collections’ in cognate disciplinary areas,7 whereas in HESA the data are organised by cost codes. In order to combine the two databases, we manually map each NSS subject area within each institution separately with its closest matching HESA cost code, resulting in a total of 3895 course groups which we are able to match. During the mapping process, it became clear that each institution has its own method of categorising programmes and of linking to schools or departments. Therefore, it was necessary to make links using plausible assumptions informed by our own knowledge of internal 6 Specifically, the Times, the Complete University Guide and The Guardian, although not the TEF. 7 We employ the NSS data organised by JACS Codes Level 3, of which there are a total of 108 subject areas. There is a total of 45 cost code areas, although given the nature of the sub-field we are not able to map any NSS course data to the ‘Continuing Education’ HESA code 136. 7 university structures. These imperfect mappings made by intuition only accounted for a small proportion of the overall sample. From the HESA data, a key variable that we include is the percentage of total staff holding a formal teaching qualification (e.g. PGCE, HEA Fellowship). This is the variable that has perhaps had the most prominence in discussion on what drives student satisfaction (e.g. Thornton, 2014). It has led a number of universities to include statements on engagement with this agenda as part of their institutional key performance indicators. Despite the interest, this link has never been empirically tested but Thornton and others have hypothesised a positive link between the percentage of qualified teaching staff and satisfaction. We should note at the outset, and we discuss in more detail below, that the coverage of this variable is only partial: most universities have only broad information on whom among their faculty holds a teaching qualification, and this limitation should be borne in mind when interpreting our findings. Nonetheless, we believe that conducting an investigation of this variable is worthwhile. One of the other motivations of this paper is the claim (discussed earlier) that ‘there is no evidence to suggest a ‘magical link’ between research and good teaching’ and power measures the quality-adjusted volume of research output, which is likely to be strongly correlated with research reputation. To capture the research rating of the institutions we employ the two most commonly quoted measures. First, we use the research power of the university, calculated as the grade-point average of the total quality profile x total FTE staff submitted. Second, we also employ the percentage of the university’s overall research quality profile that was rated 4* (‘world-leading in terms of originality, significance and rigour) in the REF2014. The percentage of so-called world leading research is likely to be a sharp discriminator between the research standings of the institutions. 4.2 Control Variables We incorporate a range of other relevant factors related to the characteristics of the teaching staff (aggregated to the subject area within each institution) that are included in HESA’s database, each of which is available by cost centre (subject) and by institution. These comprise the following variables: Academic staff factors: total numbers; teaching qualifications or equivalent held; length of service; average age; ethnicity; gender; nationality; employment history; are they a fresh graduate; employed in a specifically teaching-only role; average salary; full Professors; holding a doctorate; full time or part time; number of students per staff member (FTE). University wide factors comprise: average UCAS score of students on entry; employment on graduation; students achieving a first or 2i; student completion rates; Russell group membership; QS Top 400 membership; University established post-92; University green score; ratio of expenditure on staff to general expenditure. We describe the following variables in more detail to demonstrate that each will have differing impacts. The percentage of academic staff whose employment function involves only teaching. The formal separation of those who engage in research from those who only teach has been an increasing feature of the UK academy over the past three decades (Locke, 2004; 2012). Such widespread shifts to teaching only contracts are undesirable if they imply a reduction in the standing of those involved, disenfranchising them, cutting-off their future career options and damaging 8 prospects for promotion (Locke, 2014, p.28). Such individuals are likely to be disincentivised from engaging in research-led teaching. We also employ as a variable the percentage of academic staff holding a doctorate, since this qualification is now a minimum requirement for entering the academy at most universities. We would expect that a doctorate would provide the holder with at least a minimal level of exposure to the latest research in their field, and may provide a certain level of intellectual credibility. We include a dummy variable representing whether the university is a member of the Russell group. We use this to examine the brand and image of a university as it has been shown to be a factor in student satisfaction (Palacio et al, 2002; Brown and Mazzarol, 2009; Alves and Raposo, 2010). We also include a dummy variable representing whether the university is a member of the QS Top 400 universities ranking. This is a further variable to measure how brand and image can drive student satisfaction. Finally, we also include a dummy variable representing whether the university is ‘new’ (established post-1992). 5. Results 5.1. Key findings The multiple regression results are in Tables 2 and 3. Focusing first on the results in Table 2, where the dependent variable is the 1-5 scale measure of overall student satisfaction based on question 22 of the NSS, reveals several significant influences on the scores. The mode of study variable has a consistently positive sign, and is marginally significant in some specifications, indicating that part-time students are slightly more contented than their full-time counterparts, which is perhaps surprising as it is the other way around in other countries. But the other variable from the NSS database, the response rate, is highly significantly and positively related to satisfaction, indicating that popular courses can encourage a higher percentage of their participants to complete the questionnaire. Now considering the effects of the faculty characteristics drawn from the HESA database, importantly for this study, we do not uncover any link in any of the specifications examined between the percentage of teaching staff who are professionally qualified and student satisfaction. This could be because there genuinely is not any relationship between the two variables at the nationwide, aggregate level, or it could simply reflect the noise embedded in this variable due to the high percentage of unknowns. Within the dataset that we obtained from HESA, universities are only aware of whether 55% of their faculty have a teaching qualification or not, with the remaining 45% unknown. We simply calculate the percentage of faculty having a teaching qualification among the known staff for each course or course collection. So, suppose for example that there are 100 teaching staff in a department and the university knows that 40 of them are professionally qualified teachers and 10 are not with no information on the other 50. For this data point, the percentage qualified would then be 40/(10+40) = 80%. It is more likely that those who are professionally qualified would report their status to the university and thus more of the unknowns will be ‘no’. So nationally the figures held by HESA are probably over-estimates of the percentages who are qualified. Examining the institution-wide influences on satisfaction presented in the penultimate panel of Table 2 reveals several important findings: graduate prospects and degree completion are both 9 positively linked with satisfaction, although neither the qualifications of students on entry nor whether the university has strong environmental commitments (the ‘green score’) have any significant influences in any of the models we consider. Fascinatingly, we find that the research quality of the institution, which may have drawn students there in the first place, is negatively viewed at the point when they complete the NSS.8 Table 3 uses an identical set of specifications and explanatory variables as in Table 2, except that now the dependent variable is the percentage of students who are satisfied. In terms of overall model fit and the signs and statistical significances of the coefficients, the findings are identical to those of Table 2 but we now proceed to interpret their magnitudes since they are more intuitive when the explained variable is itself a percentage. Broadly, the estimates are of plausible sizes – for example, part-time courses have on average around 1-1.7% more satisfied students; increasing the response rate by ten percentage points would lead average measured satisfaction to be 0.8% higher, while the same percentage point increase in the number of fixed term staff would lead to a 0.2-0.3% increase in contentment9. Students at Russell Group universities or at new institutions have levels of satisfaction that are 1-2.2% and 1.2-1.9% lower respectively than those at otherwise identical old universities outside the Group. 5.2. Discussion We found that the biggest driver of student satisfaction was the proportion of students who complete the NSS questionnaire, the more the better. This ties in with the widespread view that when response rates are low, the survey findings will be biased downwards by the disproportionate effect of those with the most strongly jaundiced views being more likely to want to complete it; as the response rate grows, students with more moderate and positive views of their experience will be added to the results pool thus raising the average measured satisfaction level (Williams and Cappuccini-Ansfield, 2007). We found that our results pertaining to staff characteristics can be intuitively interpreted – a higher number of staff may be indicative that teaching is able to be undertaken by subject specialists in each sub-field, whereas low numbers may signal a lack of critical mass and that teachers must deliver material that is out of their field to cover all the syllabi. Faculty with long service not only bring additional experience, and will have had the opportunity to ‘iron out any bugs’ in their module structure and delivery, but this may also be demonstrative of a department or school which is stable and a pleasant environment in which to work; on the other hand, a low percentage of long serving staff could either indicate that the department is fairly new or growing, or more worryingly that it is unable to retain staff which would be detrimental to the student experience. The percentage of research which was rated as 4* in the REF2014, and being a member of the Russell group of universities, both have consistently strong negative influences on satisfaction, and the research power (number of faculty submitted to the REF multiplied by the REF grade point 8 It is possible that a variable such as entry qualifications would mediate this link between student satisfaction and the research quality of the institution, but we leave further investigation of this as a possible avenue for future research. 9 We should note that a significant proportion of part-time students ae registered at the Open University, although our use of institution fixed effects should mitigate the effect of this somewhat in the regression results presented. 10 average) has a mildly significant negative effect on satisfaction. Being at a post-1992 institution also leads to lower average levels of contentment, all else being equal, than being at an older university, although students appear unconcerned by the percentage of expenditure allocated to staff, or whether the institution is ranked in the QS Top 400 universities. According to the HEPI Student Academic Experience Survey (2015) discussed above, the most common reason for a course not meeting students’ expectations is that they felt that they themselves had not put in enough effort; 36% selected this reason which was higher at old than new universities (Buckley et al., 2015, p.12). Generally, we found that the best performing universities were those who had no affiliation with the Russell Group and were established before 1992. The finding here indicates that when all other variables are controlled for, it is the ‘squeezed middle’ that demonstrate the highest student satisfaction. It does therefore seem that these universities, which are research intensive while also striving to provide an excellent student experience, are somehow pulling off this difficult balancing act. Our results also demonstrate that the Russell Group contains a large range of prestigious institutions with a variety of different student experiences yet on average their undergraduate courses are less well received than the other characteristics of their provision and of their faculty would predict. This finding once again reinforces the dichotomy between students’ reaction to the brand and prestige of an institution as opposed to their actual experience, although we find no evidence that ‘better’ students with higher entry qualifications are more satisfied (as Letcher and Neaves, 2010 might have suggested). Finally, we consider that the negative link between satisfaction and staff salaries is, on the surface, counter-intuitive but may relate regional effects whereby salaries are highest in locations where satisfaction is lowest and vice versa (viz. London versus Northern Ireland). Since specifications (1) to (3) in all of the tables include subject fixed effects, it is unlikely that the salary effect is due to differences in average pay (particularly at the professorial level) across sub-fields. We also suggest, somewhat speculatively, that it may also be the case that the very large salaries which skew the distribution are paid principally to ‘research superstars’ who focus on research output generation and dissemination thus are less engaged with students and teaching. Interestingly, the negative sign on the new university dummy variable loses much of its influence (as does, to a lesser extent, being a member of the Russell Group), suggesting that it is this wider experience that students find wanting at such institutions rather than their concern being with the quality of the teaching. 6. Reflection and Conclusions This paper is the first systematic, quantitative study of the drivers of student satisfaction in the UK that examines the effect of faculty characteristics and research quality. We base our research on the 2014 results of the National Student Survey aggregated to the course level within each institution, linking each of these course collections with variables drawn from the HESA that capture the characteristics of the teaching faculty. Core to our study, we are unable to uncover any link between student satisfaction and the proportion of teaching staff who are professionally qualified, although we must note the paucity of currently available data which may have masked any patterns that do exist. We also find that satisfaction is positively linked to the percentages of full professors, of white faculty and of those holding doctorates. We also incorporate in the analysis several university-wide variables that capture the reputation of the university and characteristics of the student body, including entry standards, career prospects 11 and completion rates. An important finding from our research is the negative link between the percentage of 4* research, an indicator of the extent to which the institution is elite in research terms, and student satisfaction. There are several possible explanations of this apparent paradox. The first is that students are initially seduced by the value of the ‘research brand’ of the elite universities at the application stage but later change their minds and express disappointment if they are subsequently faced with absent professors who assign student-related activities a low priority. A second possibility is that, at the time they complete the NSS questionnaire, students fail to see the link between an academic’s research and the positive effect it may have on their own learning experience. Finally, a reasonable proportion of students at the most prestigious universities will achieve a lower classification despite the quality of the entry tariff and these students are therefore more likely to be aggrieved. There appears to be a tension between the primary desire that students expressed to be taught by professionally qualified teachers and their revealed preference for elite universities where the percentage of faculty holding such qualifications is lower. Perhaps this apparent paradox can be reconciled by re-interpreting their view as implying that given the standing of the institution at which they had already made the decision to study, they would prefer to be taught by professionally qualified teachers. It also appears likely that students misperceive what becoming a professionally qualified teacher entails, believing it must involve extensive formal training on how to teach, which will not be the case for those engaging through the experienced route. It is quite natural for students to want it all, and for them to fail to appreciate the trade-offs that exist between the times spent on teaching-related activities versus research. Many universities are already drawing an increasingly marked distinction between teaching-intensive faculty and those who are engaged in research submittable for the REF who have much lower teaching loads. It appears as if students are saying that they are happy to be taught by colleagues other than those generating the research reputations of the institutions at which they study. Put another way, one could argue that academics have not sufficiently marketed the research that they conduct as being relevant for the students they teach, and students care more for high quality teaching than research-informed teaching, potentially jeopardising the relevance of the concept of a research-teaching nexus. This was recognised by Hattie and Marsh (1996) who made the recommendation, ‘Thus, institutions need to reward creativity, commitment, investigativeness, and critical analysis in teaching and research and particularly value these [at]tributes when they occur in both teaching and research’. Our findings have several potentially important policy implications. First, at the aggregate level, this perception among students that there is not a strong link between good research and good teaching ties in with government policies aimed at concentrating research in a smaller number of institutions. The direction of travel is towards a bifurcation where one subset of institutions is focused almost entirely on teaching so that research activity would be the exclusive preserve of a much smaller number of universities than is the case today. One could therefore argue that the interesting developments will be among the mid-ranked universities considering themselves research intensive but which are not research elite, and which are currently attempting to be all things to all people. As our results show, these mid-rankers are currently managing to deliver on the student experience but as the dual demands of the TEF and the REF become more onerous, such institutions may have to 12 choose their priority in order to excel at either teaching or research as to do both may not be possible. Second, and relatedly, students should be careful what they wish for, since these views are likely to accelerate the trend within universities towards a more formal separation of teaching and research duties and with it the portfolio of teaching styles and subject matter that students receive will be narrowed, more textbook driven, less research focused and possibly less timely. Between universities, those which rank lower in research measures and where teaching is unambiguously the number one priority may decide that if research activity is not valued by students, they may as well withdraw from it entirely. Additionally, where courses are professionally accredited, academics may be presented with a list of prescribed syllabi or topics and thus the scope for lecturers to link the content of the modules with their own research, or indeed any research, is very much reduced. Consequently, those who believe in research-led or research-informed teaching need to extol its virtues much more loudly than they have in the past. References Alves, H. and Raposo, M. 2010. “The influence of university image on student behaviour.” International Journal of Education Management 24 (1), 73-85. Atwood, R. 2010. “A teaching qualification for all as new academy broom sweeps in.” Times Higher Education, 1 July. Bell, A.R. and Brooks, C. 2017. “What Makes Students Satisfied? A Discussion and Analysis of the UK’s National Student Survey.” Journal of Further and Higher Education (forthcoming). Boffey, D. 2012. “Lecturers should need a teaching qualification, says NUS president.” The Guardian, 22 April. Retrieved from http://www.guardian.co.uk/education/2012/apr/22/liam-burn- nus-academics-lecturers. Borden, V.M. 1995. “Segmenting student markets with student satisfaction and priorities survey.” Research in Higher Education 36 (1), 73-88. Broadbent, J. 2010. “The UK Research Assessment Exercise: Performance measurement and resource allocation.” Australian Accounting Review 20, 14-23. Brown, R.M. and Mazzarol, T.W. 2009. “The importance of institutional image to student satisfaction and loyalty within higher education.” Higher Education 58, 81-95. Buckley, A., Soilemetzidis, I., and Hillman, N. 2015. “The 2015 Student Academic Experience Survey.” Working Paper, Higher Education Policy Institute and Higher Education Academy. Cheng, J.H.S., and Marsh, H.W. 2010. 'National Student Survey: are differences between universities and courses reliable and meaningful?' Oxford Review of Education 36(6): 693-712 Fielding, A., Dunleavy, P.J., and Langan, M. 2010. “Interpreting context to the UK’s national student (satisfaction) survey data for science subjects.” Journal of Further and Higher Education 34 (3), 347-368. 13 Gibbons, S., Neumayer, E. and Perkins, R. 2015. “Student satisfaction, league tables and university applications: evidence from Britain.” Economics of Education Review 48, 148-164. Gibbs and Coffey. 2004. 'The impact of training of university teachers on their teaching skills, their approach to teaching and the approach to learning of their students', Active Learning in Higher Education 5(1): 87-100. Hattie, J and Marsh, H.W 1996. “The relationship between research and teaching: A meta-analysis.” Review of Educational Research 66 (4), 507-542. Hewson, P. 2011. “Preliminary analysis of the national student survey.” MSOR Communications 11 (1), 25-28. Kinchin, I.M., & Hay, D.B. 2007. “The myth of the research-led teacher.” Teachers and Teaching 13 (1), 43-61 Langan, A.M., Dunleavy, P., and Fielding, A. 2013. “Applying models to national surveys of undergraduate science students: what affects ratings of satisfaction?” Education Sciences 3, 193-207. Layton, C., and Brown, C. 2011. “Striking a balance: Supporting teaching excellence award applications.” International Journal for Academic Development, 16, 163–174. Lenton, P. 2015. “Determining student satisfaction: An economic analysis of the National Student.” Survey Economics of Education Review 47, 118–127. Letcher, D.W. and Neves, J. S. 2010. “Determinants of undergraduate business student satisfaction.” Research in Higher Education Journal 6, 1-26. Locke, W. 2004. “Integrating research and teaching strategies: implications for institutional management and leadership in the United Kingdom.” Higher Education Management and Policy 16 (4), 101-120. Locke, W. 2012. “The dislocation of teaching and research and the reconfiguring of academic work.” London Review of Education 10(3), 261-274. Locke, W. 2014. “Shifting Academic Careers: Implications for Enhancing Professionalism in Teaching and Supporting Learning” Higher Education Academy, York. Neves, J., and Hillman, N. 2016. “The 2016 Student Academic Experience Survey.” Higher Education Policy Institute and Higher Education Academy, York. Palacio, A., Meneses, G. and Pérez, P. 2002. “The configuration of the university image and its relationship with the satisfaction of students.” Journal of Educational Administration 40 (5), 486-505. Parsons, D., Hill, I., Holland, J., and Willis, D. 2012. “Impact of teaching development programmes in higher education”. Higher Education Academy, York. Sabri, D. 2013. “Student Evaluations of Teaching as ‘Fact-Totems’: The Case of the UK National Student Survey.” Sociological Research Online 18 (4), 1-15. 14 Schapper, J. and Mayson, S.E. 2010. “Research-led teaching: moving from a fractured engagement to a marriage of convenience.” Higher Education Research & Development 29 (6), 641-651. Surridge, P. 2009. “The NSS Three Years On: What have we learned?” Higher Education Academy, York. Taylor, C. and McCaig, C. 2014. “Evaluating the Impact of Number Controls, Choice and Competition: An Analysis of the Student Profile and the Student Learning Environment in the New Higher Education Landscape.” Higher Education Academy, York. Thornton, T. 2014. “Professional recognition: promoting recognition through the Higher Education Academy in a UK higher education institution.” Tertiary Education and Management 20 (3), 225-238. Trigwell, K. 2013. Evaluating the Impact of University Teaching Development Programmes: Methodologies that Ask Why There is an Impact. In E. Simon and G. Pleschova (Eds.), Teaching Development in Higher Education: Existing programs, Program Impact, and Future Trends, (pp. 257-273). Williams, J. and Cappuccini-Ansfield, G. 2007. “Fitness for purpose? National and institutional approaches to publicising the student voice.” Quality in Higher Education 13 (2), 159-72. Zamorski, B. 2002. “Research-led Teaching and Learning in Higher Education: A case.” Teaching in Higher Education 7 (4), 411-427 15 Table 1: Variable Definitions, Sources and Summary Statistics Variable type Name Description Units Mean Source Subject-specific Total satisfaction score The average of the responses to survey question 22, “Overall I am satisfied with the quality of the course” imposed on a Likert scale so that strongly agree =5, agree = 4, etc. 1-5 scale 4.22 NSS Percent satisfied overall The percentage of respondents who strongly agree or agree with question 22, “Overall I am satisfied with the quality of the course” Percentage 86.13 NSS Teaching satisfaction score The average of the responses to the section “the teaching on my course” section, where the responses questions 1 to 4 are themselves averaged 1-5 scale 4.22 NSS Percent satisfied with teaching The percentage of respondents who are satisfied in “the teaching on my course” section, where the responses questions 1 to 4 are themselves averaged Percentage 87.59 NSS Mode Part-time =1; full-time = 0 0 or 1 0.045 NSS Response rate Percentage of the eligible population who complete the questionnaire Percentage 75.02 NSS Cost centre and institution specific Total staff Total number of academic staff Integer 87.90 HESA teaching qualification Percentage of total staff who hold a formal teaching qualification (e.g. PGCE, HEA Fellowship) Percentage 69.95 HESA Length of service Average length of service of academic staff Years 7.70 HESA Age of academic staff Average age of academic staff Years 45.05 HESA White Percentage of academic staff who define their ethnicity as white Percentage 81.33 HESA Female Percentage of female academic staff Percentage 43.20 HESA Doctorate Percentage of academic staff holding a doctorate Percentage 50.15 HESA UK nationality Percentage of academic staff defining their nationality as UK Percentage 74.28 HESA Other EU nationality Percentage of academic staff defining their nationality as non-UK European Percentage 13.39 HESA Previously at another HEI Percentage of academic staff whose previous post was at Percentage 34.92 HESA 16 Notes: NSS is the National Students Survey; HESA is the Higher Education Statistical Agency; CUG is the Complete University Guide; THE is the Times Higher Education. Variable type Name Description Units Mean Source Previously a student Percentage of academic staff who were immediately previously students Percentage 10.39 HESA Teaching-only Percentage of academic staff whose employment function involves only teaching Percentage 11.40 HESA Staff salary Average salary of full-time academic staff Pounds 47872 HESA Professors Percentage of academic staff whose contract level is full professor Percentage 9.43 HESA Part-time staff Percentage of academic staff employed part-time Percentage 32.31 HESA Fixed-term staff Percentage of academic staff employed on fixed-term contracts Percentage 27.59 HESA Student-staff ratio Number of registered students per academic staff member FTE Number 17.80 HESA Institution wide Entry standard The average UCAS tariff score of new undergraduates Integer 356.68 CUG Graduate prospects The number of graduates who take up employment or further study as a percentage of the total number of graduates with a known destination Percentage 66.65 CUG Good honours The percentage of graduates achieving a first or upper second class honours degree Percentage 71.20 CUG Degree completion The percentage of students completing their course Percentage 86.76 CUG Green score A rating of universities based on questions about their environmental and ethical commitments and actions Percentage 47.77 CUG Staff-to-total expenditure The ratio of expenditure on staff salaries to total expenditure Percentage 54.65 CUG Research power Calculated as the grade-point average of the total quality profile x total FTE staff submitted Number 14.26 THE 4* research Percentage of the university’s overall research quality profile that was rated 4* in REF2014 Percentage 20.60 THE Russell group 1 = the University is a member of the Russell group 0 or 1 0.23 CUG In QS Top 400 1 = the University is within the QS Top 400 Universities ranking 0 or 1 0.37 CUG 17 Table 2: Regression to Explain Total Satisfaction Scores Variable type Explanatory variable Expected sign (1) (2) (3) (4) Constant ? 3.3120 (0.1712)*** 3.6101 (0.1308)*** 3.0855 (0.1318)*** 3.072 (0.1905)*** Mode - 0.0561 (0.0315)* 0.0648 (0.0262)* 0.0435 (0.0309) 0.0326 (0.034) Response rate + 0.0046 (0.0006)*** 0.0038 (0.0006)*** 0.0032 (0.0005)*** 0.0034 (0.0006)*** Cost centre and institution specific Total staff ? 0.8130 (0.4079)** 0.1370 (0.3988)*** - 0.5494 (0.4241) teaching qualification + 0.0003 (0.0003) 0.0001 (0.0002) - 0.00003 (0.0002) Length of service + 0.0085 (0.0032)*** 0.0060 (0.0027)** - 0.0071 (0.0030)** Age of academic staff ? -0.0038 (0.0022)* -0.0030 (0.0021) - -0.0013 (0.0023) White ? 0.0020 (0.0007)*** 0.0012 (0.0006)** - 0.0009 (0.0006) Female ? 0.0002 (0.0005) 0.0003 (0.0005) - 0.0003 (0.0006) Doctorate + 0.0008 (0.0004)* 0.0015 (0.0004)*** - 0.0011 (0.0004)*** UK nationality ? -0.1920 (8.7066) 0.0017 (0.0008)** - 0.0012 (0.0008) Other EU nationality ? -0.0002 (0.0011) 0.0019 (0.0010)* - 0.0016 (0.0010) Previously at another HEI + 0.0712 (2.5516) -0.1095 (2.1319) - -0.8685 (2.4131) Previously a student - 0.0005 (0.0004) 0.0005 (0.0004) - 0.0001 (0.0004) Teaching-only - 0.0006 (0.0007) 0.0004 (0.0005) - 0.0004 (0.0006) Staff salary ? 0.0010 (0.0157) -0.0260 (0.0136)* - -0.0299 (0.0150)** Professors + 0.0013 (0.0010) 0.0017 (0.0009)* - 0.0010 (0.0009) Part-time staff - 0.0007 (0.0005) 0.0002 (0.0004) - -0.0002 (0.0004) Fixed-term staff - 0.0007 (0.0005) 0.0012 (0.0003)*** - 0.0008 (0.0004)** Student-staff ratio - -0.0004 (0.0009) -0.0019 (0.0009)** - -0.0014 (0.0009) Institution wide Entry standard ? - - -0.0002 (0.0002) -0.0001 (0.0002) Graduate prospects + - - 0.0050 (0.0009) 0.0045 (0.0010)*** Good honours + - - 0.0007 (0.0014) 0.0005 (0.0015) Degree completion + - - 0.0057 (0.0016)*** 0.0044 (0.0018)** Green score + - - -0.0006 (0.0003) -0.0005 (0.0004) 18 Variable type Explanatory variable Expected sign (1) (2) (3) (4) Staff-to-total expenditure + - - 0.0007 (0.0007) 0.0008 (0.0007) Research power ? - - -0.0010 (0.0006)* -0.0008 (0.0006) 4* research ? - - -0.0033 (0.0010)*** -0.0035 (0.0011)*** Russell group + - - -0.0370 (0.0186)** -0.0751 (0.0209)*** In QS Top 400 + - - 0.0005 (0.0196) 0.0185 (0.0217) New university - - - -0.0700 (0.0183)*** -0.0509 (0.0201)** N - 3895 3895 4247 3730 R2 - 0.22 0.11 0.12 0.14 Subject F.E. - YES YES YES YES Institution F.E. - YES NO NO NO Notes: This table reports the results from a set of regressions where the dependent variable is the overall satisfaction score (Likert scale). Specifications (1) to (4) all use the same dependent variable but vary according to which groups of explanatory variables they contain and whether both subject- and institution-fixed effects or just the former are employed. The observations are courses or course collections at each higher education institution. Standard errors are given in parentheses. *, ** and *** denote significance at the 10%, 5% and 1% levels respectively. The total staff, UK nationality, previously at another HEI, and staff salary parameters and their standard errors are multiplied by 10000 for ease of presentation. 19 Table 3: Regression to Explain Percent Satisfied Overall Variable type Explanatory variable Expected sign (1) (2) (3) (4) Constant ? 58.2861 (7.2337)*** 68.2507 (4.3682)*** 49.8597 (4.4784)*** 48.6494 (6.3265)*** Mode - 1.7922 (1.1085) 1.7680 (0.9014)** 1.1803 (1.0531) 1.0762 (1.1731) Response rate + 0.1165 (0.0194)*** 0.0944 (0.0181)*** 0.0746 (0.0174)*** 0.0812 (0.0188)*** Cost centre and institution specific Total staff ? 0.0031 (0.0014)** 0.0007 (0.0013) - 0.0020 (0.0014) teaching qualification + 0.0024 (0.0109) -0.0026 (0.0065) - -0.0028 (0.0071) Length of service + 0.1752 (0.1060) 0.0804 (0.0901) - 0.1264 (0.0977) Age of academic staff ? -0.0354 (0.0729) -0.569 (0.0695) - 0.0071 (0.0757) White ? 0.0632 (0.0245)*** 0.0372 (0.0192)* - 0.0333 (0.0198)* Female ? 0.0163 (0.0174) 0.0168 (0.0173) - 0.0179 (0.0175) Doctorate + 0.0243 (0.0146)* 0.0530 (0.0126)*** - 0.0369 (0.0136)*** UK nationality ? -0.0201 (0.0285) 0.0410 (0.0246)* - 0.0238 (0.0251) Other EU nationality ? -0.0287 (0.0363) 0.0345 (0.032) - 0.0282 (0.0334) Previously at another HEI + -0.0030 (0.0085) -0.0047 (0.0076) - -0.0070 (0.0080) Previously a student - 0.0074 (0.0139) 0.0082 (0.0116) - -0.0038 (0.0120) Teaching-only - 0.0213 (0.0216) 0.0096 (0.0173) - 0.0119 (0.0202) Staff salary ? -0.2147 (0.5261) -0.7032 (0.4276) - -0.9205 (0.4703)* Professors + 0.03727 (0.0319) 0.0530 (0.0284)* - 0.0340 (0.0302) Part-time staff - 0.0197 (0.0163) -0.00002 (0.0132) - -0.0119 (0.0141) Fixed-term staff - 0.0179 (0.0170) 0.0344 (0.0111)*** - 0.0206 (0.0118)* Student-staff ratio - 0.0162 (0.0299) -0.0440 (0.0296) - -0.0256 (0.0304) Institution wide Entry standard ? - - -0.0091 (0.0060) -0.0058 (0.0070) Graduate prospects + - - 0.1477 (0.0303)*** 0.1298 (0.0323)*** Good honours + - - 0.0188 (0.0439) 0.0209 (0.0481) Degree completion + - - 0.2267 (0.0546)*** 0.1757 (0.0613)*** Green score + - - -0.010 (0.0114) -0.0107 (0.0120) 20 Variable type Explanatory variable Expected sign (1) (2) (3) (4) Staff-to-total expenditure + - - 0.0220 (0.0216) 0.0312 (0.0231) Research power ? - - -0.0341 (0.0176)* -0.0325 (0.0188)* 4* research ? - - -0.0997 (0.0339)*** -0.0931 (0.0378)** Russell group + - - -0.9652 (0.5906)* -2.1940 (0.6600)*** In QS Top 400 + - - 0.4600 (0.6391) 1.2161 (0.7015)* New university - - - -1.9490 (0.6025)*** -1.2195 (0.6550)* N - 3895 3895 4247 3730 R2 - 0.20 0.10 0.11 0.12 Subject F.E. - YES YES YES YES Institution F.E. - YES NO NO NO Notes: This table reports the results from a set of regressions where the dependent variable is the overall satisfaction score (percentage of students satisfied). Specifications (1) to (4) all use the same dependent variable but vary according to which groups of explanatory variables they contain and whether both subject- and institution-fixed effects or just the former are employed. The observations are courses or course collections at each higher education institution. Standard errors are given in parentheses. *, ** and *** denote significance at the 10%, 5% and 1% levels respectively. The total staff salary parameters and their standard errors are multiplied by 10000 for ease of presentation. 