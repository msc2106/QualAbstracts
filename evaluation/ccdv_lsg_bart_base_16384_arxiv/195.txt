ABSTRACT: 
peer-reviewedMany live performance practitioners have explored the development of live gestural controllers in the hope of achieving natural and intimate interaction with their instrument. While the relevant literature offers a great deal of techniques for the development of mapping strategies that link physical gestures to sounds, resources and discussions on how these techniques have been used for the practical development of a live performance are, in comparison, less dense. The advantage of providing a detailed documentation that offers a clear picture as to how and why these strategies have been implemented in the context of a live performance would ultimately provide an additional and necessary tool for the proper analysis of the performance itself. This article presents, analyses and discusses the journey to the development of Agorá, a live sound theatre performance written for a custom-built data glove named 'Pointing-at' . The analysis of both the theoretical and practical elements of performance formed the basis for the development of an approach to the design of nuanced gestural performative actions that are both visually and sonically understandable by audience members. In that regard, the use of metaphors that are coherent to the theme of the performance have been found to be a useful tool that can enhance both the performer and audience experience of the live performance.ACCEPTEDpeer-reviewe. 
 
PREDICTION: 
 this article presents and discusses the development of Agorá , a solo sound theatre performance which makes use of a data glove musical device named ‘Pointing-at’ . Agorá , having been performed in several international contexts , is the last of a series of performances that I developed between 2009 and 2013 . in that regard 
 , the use of metaphors that are coherent to the theme of the performance have been found to be a useful tool that can enhance both the performer and audience experience of the live performance . In doing so , I address the relationship between glove-based controllers ( although relevant to DMIs in 
 
FULL TEXT: 
 ​sound theatre performance 1written for a custom-built data glove named Pointing-at. The analysis of both the theoretical and practical elements of performance formed the basis for the development of an approach to the design of nuanced gestural performative actions that are both visually and sonically understandable by audience members. In that regard, the use of metaphors that are coherent to the theme of the performance have been found to be a useful tool that can enhance both the performer and audience experience of the live performance. Keywords: digital musical instrument; sound theatre; live performance, data glove. Subject classification codes: 1 In this article, I use the term “sound theatre” in the same manner as used by Caroline Wilkins (2013), which describes a genre that lies in between electroacoustic music and music theatre. The way a performer interprets and links the gestural input data of their own controller to a generator and sound output device (i.e. computer), a process known as mapping, is crucial for the performer in order to control, explore and ultimately master their device through a series of nuanced gestural actions. The relevant literature offers a wide range of strategies regarding mapping issues (Fels et al. 2002; Goudeseune 2002; Malloch et al. 2011; Wanderley 2002; Verfaille et al. 2006; Marier 2012). Conversely, a detailed literature, showing how these techniques are deployed beyond demonstration purposes and within the context of a live performance, is less dense. Nevertheless, it is important to take into account how, in what circumstances and for what reasons, these strategies are put in place; that is, a live performance. Broadly speaking, live performances (also including the ones that make use of novel technologies) are highly complex systems in which anthropological, sociological, psychological and technical issues are involved. In its basic form, however, it is sufficient to state that a live performance is established in the dualism between performer and audience. By considering the audience, it is possible to approach mapping problems not only from a Human Computer Interaction (HCI) perspective, which specifically deals with the interaction between the performer and their instrument, but also from a wider scenario which involves elements of psychoacoustics, cognitive psychology, anthropology, sociology and aesthetics. In traditional instrumental practice, the relationship between performer and instrument is straightforward in that the audience will inevitably associate the receiving sound as coming from a given performer/instrument. This knowledge, ‘acquired through massive experience of sound-sources in general and music performances in particular ’, is described by Godøy as ​ecological knowledge ​(Godøy 2010, p.106). The evaluation of the performer’s ability to control their instrument can be done on the basis of multiple factors such as touch, speed, intonation and errors (Fyans and Gurevich 2011) which relate to the ability of controlling, sometimes failing to control, and investigating the almost infinite nuances that belong to the instrument. In live electronic performances, where the performer uses devices such as a laptop and/or digital musical instruments (DMIs), this relationship becomes confused. The audience has little information with respect to the degree of control the performer has in relationship to the generated musical output or even what gestures can be considered causing the given sound. Arguably, this could be traced to the fact that, contrary to a traditional musical instrument, the computer has no intrinsic sound. However, through the use of a DMI, almost every possible sound or cluster of sounds can be achieved. In addition, what is considered a DMI is often composed of two autonomous devices: a controller and a computer. In contrast, a traditional instrument is both a controller and a sound-outputting device. It is fundamental to acknowledge that mapping, being mainly a programming problem, is always hidden to an audience and it may reveal itself only through those gestures that enable the performer to control their instrument. Once the cause/effect mechanism between gesture and sound becomes clear, the audience is more able to speculate on the device’s properties and limitations. Thus it becomes central to develop nuanced gestural performative actions that are both visually and sonically understandable by audience members. This could then offer the audience an additional element to judge the performer’s skill or more generally the overall performance. From this perspective, the mapping problem is twofold: on the one hand, we are concerned with how to design DMIs that have high levels of controllability as required by the performer in order to control fine nuances of the sound producing mechanism, described by Moore as the ‘control intimacy’, that is, ‘the match between the variety of musically desirable sounds produced and the psycho-physiological capabilities of a practiced performer ’ (Moore 1988, 21). On the other hand, we need to provide the audience with a clear understanding of the relationship between the performer’s gestures and the sounds generated (cause/effect mechanism) in a manner that is nuanced and non-trivial. To this end, I am presenting and discussing here the development of Agorá, a solo sound theatre performance which makes use of a data glove musical device named ‘Pointing-at’. Agorá, having been performed in several international contexts, is the last of a series of performances that I developed between 2009 and 2013. An iterative process of development, composition, performance and reflection was undertaken which has been discussed in an audiovisual documentary (Torre 2013a). In doing so, I address the relationship between glove-based controllers (although relevant to DMIs in general), musical output and audience understanding/engagement in a live electronics performance scenario. The performance of Agora aims to strive for communicative efficiency of both the DMI and the mapping strategies adopted by the performer/composer to facilitate performance readability from an audience, adding to the existing literature concerned with live electroacoustic performances for data gloves. Data gloves in Live Performances Data glove technology has a long history, probably the earliest example being the “Communication Device” data glove developed by Siebel and Rochester at IBM in 1962. This data glove worked as a wearable typewriter device used by computer operators in airborne high acceleration aircraft. The Sayre Glove (1977) was the first example of a data glove that enabled user manipulation of real-time computer graphics. In the 1980s, the aerospace industry demonstrated an interest in data glove technology for applications in Virtual Reality systems: VIEWS, developed by NASA in 1985, and the Virtual Cockpit, developed by the British Aerospace in 1987, are fully immersive virtual reality systems that make use of data gloves. Nintendo released the Power Glove in 1989 making glove technology available to a wider audience.. From the late 1980’s, many private companies started developing and selling data gloves. (For a tentative list of this effort see Torre 2013b, 31-40). A large number of researchers and artists have also designed and utilised data gloves for live performance purposes. The relevant literature is extensive, and detailing it is beyond the scope of this article . However, notable performances, that go beyond 2demonstration purposes, are Michael Waisvisz’ ​Hands ​, Letitia Sonami’s ​Why Dreams Like a Loose Engine,​ Rajmil Fischman’ ​Ruraq Maki​, Joseph Butch Rovan’s ​Collide​, and Mark Bokowiec’s ​V’Oct (Ritual),​ as well as more recent performances by Imogen Heap. Each artist, with the exception of Fischman who uses the P5 Glove from Cyberworld and Heap who uses a 5DT 14 Ultra data glove from Fifth Dimension with a fitted lavaliere microphone, developed a custom-built data glove. Waisvisz’s ​Hands​ uses a pair of gloves each of which are provided with switches, accelerometers, pressure sensors, microphone and ultrasound distance sensor to calculate the distance between the two hands. Sonami’s ​Lady’s Glove​ consists of a single Hall Effect sensor on the thumb and magnets on the other four fingers, five switches on the tips of the fingers, a mercury switch on the top of the hand, flex sensors on each finger and an ultrasonic transmitter located on the palm of the hand with receivers strapped on the foot and the other hand. Rovan’s glove for ​Collide ​uses 2 Please refer to (Torre 2013b, 40-54). force-sensitive resistors (FSRs), flex sensors and an accelerometer in one hand and a second glove with reflective material attached to it on the second hand to interact with an infrared sensor. Bokowiec’s ‘Bodycoder’ system, although a sensor suit, makes use of key switches on the tip of the fingers and flex sensors on the wrist and elbow (Wilson-Bokowiec 2010). The different sensor technology employed in each data glove described above, as well as the software used and the equipment chosen, can limit the range of possibilities are available to the performer and composer. In line with the literature that draws from Magnusson (2010) and Pierce and Wiggins (2002), I refer to these as technological constraints​ (Torre 2013, p.17). Similarly, the chosen genre, the composer’s ability and audience’s cultural backgrounds are referred to as ​cultural constraints ​(ibid.). The combination of both technological and cultural constraints creates dramatically different aesthetic results ; however, a common feature of is the various performers’ desire to exploit the innate ability of individuals to perform hand gestures for undertaking complex musical tasks. The causal links between bodily movements and sonic manipulation in live performance practices have been described as ‘kineasonic’ (Wilson and Bokowiec 2006). Kineasonic gestures, however, reflect an approach to the mappling problem that focuses exclusively on the relation between the sensual and the sonic from a performer’s perspective. In doing so, the problem of the transparency of gestures for an audience is delegated to the performer’s ancillary gestures, that is, gestures not acting upon the sound producing mechanism. I argue that considering the audience’s perspective is instead instrumental to the development of both mapping strategies and DMI performances in general. The lack of ecological knowledge from an audience perspective, and the ontological basis on which a live performance exists (i.e. an action judged by its observer: the audience), are a testament to this. For this reason I use the term ​metaphor​ in relation to the development of Agorá. The term metaphor expands on the interpretation given by Fels et al. (2002) and Fischman (2013) that relates to the dynamics between user interactions, data and sound producing mechanisms. Metaphor is intended here in its broader meaning of ‘in place of’ and becomes central to every aspect concerning the creation of ​ Agorá ​. The performance and its theme are metaphors; A story narrated through gestures and music. The use of a gestural vocabulary that is familiar to an audience (e.g. grabbing, pointing etc.) is developed, used, presented and organised in a manner that exhibits coherency with the theme, narrative and metaphor of the performance. In doing so, the gestures sonify a story and, at the same time, help the audience to read the story and to understand the kineasonic expressivity of the data glove (or DMI). Thus, the DMI, rather than being the sole focus of the performance, becomes an element of equal importance between the many that play a role in the making of a live performance. In other words, we do not create a performance for the DMI but we create a performance in which the use of a DMI is justified by the aesthetic goals of the performance. Agorá Agorá ​represents a voyage inside a performer’s musical and acoustic memories. The title, derived from the ancient greek ​a​`​γoρa​´, refers to the public square, the heart of old Greek towns, in which business, political debates and many other social activities took place. In this regard, Agorá is a metaphor that sees the performer placed at the centre of the agorá while observing and interacting with its surroundings (i.e. his own memories). In ​Agorá ​the theme of memory is inspired by Wordsworth’s poem ‘The Prelude: There are in our existence spots of time, That with distinct pre-eminence retain A renovating virtue, whence-depressed By false opinion and contentious thought, Or aught of heavier or more deadly weight, In trivial occupations, and the round Of ordinary intercourse-our minds Are nourished and invisibly repaired; A virtue, by which pleasure is enhanced, That penetrates, enables us to mount, When high, more high, and lifts us up when fallen. (Wordsworth 1850, verses 208-218) Figure 4. Screenshot from Agorá (DVD version). The theme deals with the ability we possess to recollect past experiences in our lives and how these memories can help us in dark moments of our lives. In the performance of ​Agorá ​ , the theme of memories is considered in terms of our inability to fully re-live all the sensations experienced in the original instance of the memory, since these are intrinsically connected to the present temporal dimension (the here and now) of the real experience lived. In this regard, the performance enacts the effort in trying to recall such memories with all the associated sensory cues when initially experienced.. These memories are in the form of music, sounds and voices. Within the process of discovery is the corresponding desire of re-appropriation of something that is, in its essence, lost forever. The performance of ​Agorá​ is in three main sections (Part A, B and C). The mapping strategies devised in each section develop with the agorá metaphor in mind. The first section (Part A) sees the performer unveiling his musical memories, which slowly come to focus; the performer stands at the centre of the agorá in the act of slowly refocusing the scene around them after a long period of self-imposed isolation. In the second section (Part B), the performer interacts with his musical memories with the intent of modelling them. Thus, the aim is to re-appropriate the memories and reflect on them in order to ‘understand’ the lived experience. This action is a personal development process in which the performer is involved in the first person. In the third section (Part C) voices from the subconscious emerge to the conscious mind revealing the ‘real’ underlining reasons for the actions made. That is, the memories were only pathways to the most precious memory of all (the final recording to be heard at the end of the performance), something that is, however, lost forever. Lost forever in its essence perhaps, but for which the mind, by means of interlocked pathways, has safely preserved via defense mechanisms in the form of this maze of related sound memories. A brief Description of the Pointing-at Data glove Agorá is a solo performance for the custom-built data glove – Pointing-at – and developed by the Interaction Design Centre and the Digital Media & Arts Research Centre at the University of Limerick and the Tyndall National Institute of Research in Cork under the National Access Program (NAP) 60 research grant. The Pointing-at glove is designed around the Tyndall’s 25mm Wireless Inertial Measurement Unit (WIMU), which is an array of accelerometers, gyroscopes, magnetometers and a single flex sensor on the index finger communicating wirelessly to a base station connected, via USB, to the host computer. The data received at the serial port is then imported into Max/MSP software. A custom Max/MSP object named ​mote ​reads from the serial port of the computer at a constant rate and displays the raw ADC data from the sensors into the Max/MSP environment (see Torre et al. 2007). Data Retrieval and Numerical Evaluation The digital data coming from the sensors, with values between 0 and 4096 (12-bit resolution), have little meaning in their raw form. Therefore, after a preliminary calibration and resolution procedure for the cluster of sensors, a set of mathematical operations is required which represents a preliminary step to the interpretation and mapping development discussed later in this article, and facilitates the development of meaningful cause/effect gestures. The flex sensor has been devised to work as a simple on/off switch. The cluster of sensor constituting the accelerometer, gyroscopes and magnetometers is used to retrieve the attitude of the hand wearing the glove. In aeronautics, the attitude refers to the orientation of an object relative to another. In our case, the attitude of the hand is calculated with respect to the Earth. Both glove and Earth reference system axes are orientated according to the convention depicted in Figure 1. The algorithm chosen to retrieve the attitude is known as TRIAD, an efficient and elegant method widely used by aeronautical engineers to calculate the attitude of aero-vehicles and satellites (Bar-Itzhack and Harman 1996). The main advantage of the TRIAD algorithm is that it does not require any mathematical integration to calculate the attitude and thus it avoids estimation errors that propagate and increase over time. The attitude estimation errors that the TRIAD algorithm introduces are, instead, mainly due to sensor reading errors caused by flicker noise, calibration error, thermo-mechanical white noise and/or external agents such as temperature. However, the results obtained, while not suitable for critical systems, are satisfactory for the purposes of a live performance. A more detailed description of the sensor technology used and numerical strategies implemented is offered in Torre 2013b.. Figure 1. The figure shows the convention adopted for the numerical evaluation and discussion of object and reference system axes For the purpose of this article, it is sufficient to state that the available data for mapping is limited to: the hand’s attitude , azimuth and elevation for X, Y and Z axes, 3and a copy of the raw incoming sensor data. With the retrieved attitude data, a further numerical manipulation was undertaken. By placing the performer at the centre of a virtual sphere, whose surface has been divided into thirty-two slices, sixteen for the Northern hemisphere and sixteen for the Southern hemisphere, the software retrieves the slice of the sphere the hand (glove) is pointing towards. It is important to note that the glove enables calculation of the orientation of the hand and not the position. Thus, we can assume that the sphere is always around the performer no matter where the performer is located in the physical space. Each slice is then assignable to the desired behaviour according to the performance requirements. Table 1 summarises the available data for the mapping and development of the Agorá performance described and discussed hereafter. Data Numerical Evaluation Gyroscopes raw ADC values none Accelerometers raw ADC values hand’s attitude estimation and azimuth and elevation for X, Y and Z axis Magnetometers raw ADC values Flex Sensor raw ADC values on/off switch Virtual Sphere hand’s attitude sphere’ slice number (1 to 32) Table 1. Summary of data available for mapping. 3 The way the hand on which the sensor is placed is oriented with respect to the physical space. Performance Development Agorá requires a quadraphonic audio system; the performer is located at the centre of the area delineated by the four speaker array and the audience sits within the same space and around the performer. This space defines the agora, enabling the audience not only to be located nearer the ‘sweet spot’ of the surround system but also to more closely experience the performer's memories that are about to be recreated and explored. Thus, the audience sits within the agorá as well. The performance is written using a combination of Max/MSP and Ableton Live patches with the former controlling the latter via MIDI and OSC messages. Each of the three sections presents a different performer’s paradigm of interaction with my musical and sonic memories. Their development is described hereafter and a video of the performance is publicly available at (Torre 2013c). Part A – The making of the Agorá Forty songs selected from my personal digital library, with each song representing a musical memory associated to a personal experience, represent the sonic material as the basis of the performance. I refer to these as ​memory songs​. The unveiling process that characterises this section is enacted through a series of gestural mapping techniques and automated systems. In that respect, the gestural vocabulary is designed in order to present the unveiling action. The visual reference to this gesture can be observed in physical emulator libraries such as the MSAFluid developed for the openFrameworks C++ toolkit​.​ The physical modelling library used in Agorá was initially developed for Pure Data and then made available for Max/MSP (the main software in use) and is named ​pmpd​. In line with the visual projects mentioned above, the aim here was to find a metaphor to rearrange the perceived visual movements in acoustic terms. The metaphor created sees the tail of the visual particle matching the volume of the sound (a sort of fade-out). The gestural mapping generated follows this metaphor too, based on the sweeping-like movement of the hand in the air as if to remove dust from a piece of furniture without physically touching it (i.e. the dust is removed by the wind - here volume - created by the hand’s movement near the furniture). This hand gesture controls a series of elements in the software. For the purpose of clarity, let us assume for now that the user is able to control the volume of a sound loop found in a slice of the virtual sphere surrounding the performer. The volume parameter is then controlled by a combination of user input gestures interfacing with a custom-built abstraction made with the ​pmpd ​library, which ultimately controls the output volume for the loop. More specifically, each time the performer points toward a slice of the virtual sphere surrounding them (only the Northern hemisphere of the virtual sphere is considered for this section of the performance) the physical modelling patch emulates the motion of a spring of given rigidity, length, thickness, damping of link deformation and damping of mass speed values. The output values of the spring emulator module are then scaled and sent as MIDI control change messages to Ableton Live (the audio engine) to control the volume of the loop. However, it is important to note that the spring, when triggered, will oscillate only once as to recreate a sort of spring-like motion in which the loops will fade-in fast and fade-out slower (i.e. the wind power exerted on the dust at the passage of the hand). The aforementioned assumption that the user could control a single loop can now be disregarded. In fact, the patch allows the user to control, or at least to interact with, a library of forty songs. These songs are hosted in the Ableton Live patch and distributed randomly across eight Ableton audio tracks each of which host five songs. The selection of the loop is controlled via Max/MSP which also triggers the spring emulator, which in turn triggers a random generator unit that controls which song will be played and its starting point within the song-file via MIDI note and MIDI control change messages respectively. There are eight of these modules in the Max/MSP patch, each of which controls one of the eight Ableton tracks. Lastly, the spatialisation engine is implemented in Ableton via the Ambipanner 1.0 Max/MSP for Live audio device, which is controlled by the y-axis azimuth values scaled in the range between 0 and 127 and sent from Max/MSP to Ableton via MIDI control change messages. Part A - automated computer events There are several elements in Part A that have been preprogrammed and for which the performer does not have any real-time control. At the beginning of Part A, the performer controls the volume of a ‘ ​whoosh​’ sound loop. This sound could be thought of as the ‘dust’ in our metaphor. The way the volume of this loop is controlled is identical to that described earlier in relation to the ​memory songs ​ (spring emulator module). The way this ​whoosh ​sound is slowly faded out, leaving space for the emerging ​memory songs​, is, however, automated by dynamically changing the rigidity, length, thickness, damping of link deformation and damping of mass speed values of the spring emulator modules of both the ​whoosh ​and ​memory songs​ over time. In other words, the stiffness of the spring for the ​whoosh ​loop will increase over time while decreasing for the ​memory songs’​ modules. This action, combined with the performers’ gestural input previously described, is the second element that helps to enact the metaphor of removing the dust to unveil the musical memories. A further element is given by the automated dry parameter value of a reverb unit that moves from a high to a low value. A high amount of reverb slowly fading out over time emulates the sound memory moving from a distant and out-of-focus place to a closer and in-focus location. Part A ends abruptly with the entrance of a breaking glass sound triggered at the end of the automated sequence of preprogrammed events and evenly spatialised across the array of speakers. Part B – Reworking the lost memories Part B is the section in which the performer manipulates and aims to gain control over his musical memories. Thus, the source material is once again made of the same forty selected ​memory songs.​ The section presents also a mixture of real-time and automated elements. Three modes of real-time interaction are devised in this section and are enabled sequentially via an automated chain of events. These events are triggered over time and their active mode is signalled via specific automatically triggered sounds appearing throughout the section. I will refer to these modes by the general names of Freeze​, ​Hey ​and ​Resonator​. In ​Freeze ​mode, the user is able to freeze and loop a few millisecond-long excerpts taken from the forty ​memory songs ​. The signal flow enabling this mode sees a combination of the main Agorá Max/MSP patch, the Ircam’s Cata-RT Max/MSP application and Ableton Live. Cata-RT is an application enabling corpus-based concatenative sound synthesis (Schwarz 2004) , and is based on the idea of storing a 44 Corpus-based concatenative synthesis makes use of several short sound snippets to generate sounds. The snippets are retrieved from the analysis of a large sound library. large data set of short sound-snippets retrieved from the analysis of a selected sound library (i.e. the forty songs) in order to reassemble them according to the selected descriptors extracted from the analysis. Thus, the forty songs are imported, analysed and reorganised in a Cartesian graph according to the selected descriptor in the CataRT application. The software has been set to repeatedly select random snippets from the loaded audio library. The audio output of the CataRT application is then routed into four dedicated Ableton audio tracks, each of which is connected to a single audio output channel (no dynamic spatialisation is allowed here) and faded in automatically. In each Ableton audio track, an instance of the Freeze 1.0 plug-in has been loaded. The user interaction is limited here to the deactivation and activation of the freeze function (literally freezing in time a few milliseconds of sounds) in the Freeze 1.0 plug-in by crooking and stretching the index finger (bending sensor data). The user can decide which instance of the plug-in (audio track or speaker) to interact with by pointing the hand in the direction of the speaker they choose. Thus, having divided the circumference around the performer in four segments, the y-axis azimuth coordinate works as a splitter that routes the bending sensor data to the desired channel (speaker or Ableton audio track). Each audio track also has an instance of the native Ableton reverb plug-in; this was done in order to achieve a smoother drone-like sound from the sound output by the freeze effect. The automatic fade-out of all the ongoing sounds signals the enabling of the ​Hey mode. In this mode the performer is able to control the in-points of four vocal samples screaming ‘Hey!’ The sound selection is achieved by using the y-axis elevation and azimuth coordinates retrieved by the pointing direction of the hand. If the elevation value is lower than a certain threshold the performer is able to send a ‘start playback’ command (via MIDI message) to one of the four loops by crooking and stretching the index finger. In order to select one of the four loops, the azimuth value needs to be within one of the four segments of the circumference around the body of the performer. The automatic introduction of sounds resembling distant voices signals the enabling of the ​Resonator ​mode. In this mode the performer can control the dry/wet parameter of the Ableton’s Resonator native plug-in. The parameter is controlled by the x-axis elevation values (tilting movement of the hand) scaled and sent via MIDI to Ableton. Part B - automated computer events The automated elements in this section are used to signal to the performer the enabling of a new mode of interaction with the software. The appearance of these elements is precisely timed. Another automated event is used to lower the computational power required by the simultaneous use of several pieces of software. In fact, the CataRT software’s random selection function requires a great deal of CPU resources. Thus, the function is enabled only at the start of Part B and disabled during section A. The start and stop commands for the CataRT patch are sent by the main Agorá ​patch via OSC messages. Part C - The revelation Part C is the short Coda section of the performance, and it includes computer-automated events only. All functions in all patches are closed, preventing any kind of performer ’s control of the software and its parameters. This is signalled to the performer by the automatic fading out of all ongoing sounds while new sounds are introduced. The new sounds are an ‘unfrozen’ version of the ongoing random snippets selection mechanism from the CataRT application that is now played to all the channels in the array of speakers via four additional Ableton audio tracks. An automated function pitch-shifts this output down by controlling the pitch shift parameter within the Cata-RT application. The dialogue between three voices, extracted from some of my family footage, marks the end of the performance. Score Macro Structure - Length and Structure of the Performance The several automated elements of the performance are timed from a macro to a microstructural level. The macro level defines the length of the performance. Each section of the performance is using up a precise percentage of the overall length. While the length of the entire performance can be changed, the length of each section cannot because they are bound to a precise percentage calculated with respect to the overall performance length value. The same proportional system is applied to the single automated event within a section; this system was devised for two main reasons. Firstly, it offers a way of setting a precise length for the performance according to the needs of the venue hosting it and/or the ‘feel’ for the occasion. Secondly, it allows for a fast method to rehearse the automated events included in each section without the need for waiting a long time. In other words, if the performance had a fixed length of twenty minutes and we wanted to test an automated event at the eighteenth minute, we would have had to wait eighteen minutes. With the method devised, instead, we could set the length of the performance to be two minutes and wait one-hundred-eight seconds only to test that automated event. Assuming the overall performance’s length to be one hundred units, each section takes up the following percentage: Part A: 40%, Part B + Part C: 60% of which​ ​97% (Part B) and 3% (Part C). Within each section all the macro events are further divided with reference to the duration of the section they belong to: Part A = Duration Act (99%) + Pause after the ‘breaking glass’ sound file is played (1%); Part B = ​Freeze Gesture (35%) + ​Hey ​Gesture (20%) + ​Resonator ​Gesture (45%); Part C = Automated events (3% of the Part B + Part C duration); Within each section each event is then timed using the same percentage system or through a series of delay functions whose time-values are scaled with proportion to the section length. Figure 2 tries to depict the sequence of events and can be considered as a form of graphical score. Figure 2. Agorá - Macro Structure - Percentage allocated to each section and subsection. Discussion The entire gestural vocabulary presented in Agorá is built upon the metaphor and theme it represents. The development of mappings attempts to be coherent with the metaphor in place and, at the same time, to increase the perceived control intimacy. The ‘pointing’ gesture in Part A became a ‘dust removing gesture’ defining a personal space metaphorically representing the memories of the performer, physically defined by the quadraphonic array of speakers. The perceived control intimacy from the performer ’s perspective is increased by the underlying algorithm placed between the data retrieved and the sound-generating mechanism. This mapping is designed according to a precise idea of movement which coherently relates to the metaphor in place (i.e. the wind generated by the hand removing the dust). This process was also enabled by the numerical evaluation of the data retrieved, giving it a precise meaning , which, in our case, was the attitude of the hand and its pointing direction. Furthermore, the numerical evaluation and the consequent mapping strategies enabled us to state that the data glove affords ‘pointing’ while the bending sensor on the index finger, used as a on/off switch, affords ‘grabbing’ (hand-closing) and ‘throwing’ (hand-opening). In this way the gestures, rich both in communicative attributes and in coherence with the metaphors of the theme of the performance, facilitate the description of the device’s affordances and thus facilitate the comprehension of the cause/effect mechanisms from an audience perspective. The technological constraints imposed by the available data and its numerical evaluation allowed for a better exploitation of the data itself. In other words, it encouraged a better understanding of what more could be done with the attitude and switch mechanism data. On the other hand, the lack of ecological knowledge from the audience with regard to the Pointing-at glove, a new instrument of unknown properties, was the cultural constraint that led me to the idea of introducing a theme (Agora)​.​offering other elements for the audience to latch on to. The performance presents a mix of live and computer automated sounds in which Emmerson’s terminology (2007) of function and frames helps to better describe their use in the performance. Emmerson defines two functions: ​local​ and ​field​. The “​local​ controls and functions seek to extend (but not to break) the perceived relation of human performer action to sounding result” (92). The “field functions create a context, a landscape or an environment within which local activity may be found” (92). Both local and field functions are described in terms of the real and the imaginary. In that regard, a local real function describes the performer’s direct manipulation of a sound-producing mechanism. The local imaginary concerns instead the performer’s triggering of loops or pre-recorded material. Furthermore, real and imaginary in relation to the field function describe the semantic nature of sounds not directly manipulated by the performer (automated events). The areas occupied by these functions are referred to in terms of frames. These are: event, stage, arena and landscape. From Figure 3 it emerges clearly how these frames can be thought not only in relation to the sound material presented but also as demarking lines between the performer’s and audience’s physical space. Figure 3. Local and field function displacement as in Emmerson (2007). In Agorá, functions and frames were also developed with the metaphoric content of the performance in mind. Their use, static or dynamic, can therefore represent a further tool for defining and highlighting the metaphors, the theme of the performance, and validate the gestural interactions devised. The spatialisation gestures in Part A enable the definition of the performer’s meta-kinesphere​’s boundaries and collapse the ​field ​function into the ​local ​one. The function’s attributes of ​real ​and ​imaginary ​also seem to move from the former to the latter. Indeed, what at first seems to be directly controlled by the performer becomes, as the time passes, an act in which the performer clearly controls sound-loops. The movement from the ​real ​to the ​imaginary ​is enabled here by the automated functions in place which change the way the performer interacts with the design algorithm by an automated changing of the ​force ​values for both the ​whoosh​ sound and the memories. This combination of automated systems and live interactions does not seem, however, to preclude the understanding of the affordances of the data glove. The combination of spatialisation gestures and the ‘dust removal’ gesture informs and helps speculate about the possible mapping strategies in place. In particular, the pointing gestures reinforce the live act of delimiting the boundaries of the meta-kinesphere while the ‘dust removal’ gesture helps unveil the ‘objects’ (memories) located within the space created (i.e. the agorá). Part B presents similar characteristics. The meta-kinesphere is now delimited by the fixed-point sound sources (no real-time spatialisation is involved) given by the quadraphonic arrangement of the speakers. This again collapses the ​field ​function into the ​local ​one. The boundaries between ​imaginary ​and ​real ​are, however, less clear. The first section, ‘Freeze’, sees a random generator algorithm at the core of the mapping strategies, making it difficult to judge the control the performer is allowed over the sound-producing mechanism. The sounds are prepared in advance, analysed and imported in real time as metadata into the CataRT software, and this would be a characteristic of an ​imaginary ​function. Furthermore, these sounds are initially introduced by an automated event that progressively fades them in. However, the real sources are somewhat hidden and heavily processed behind a mechanism that, to a certain extent, is controlled directly by the performer and thus belongs to the ​real function. In addition, the random generator unit is also seen as a metaphor for the memory trying to fool the performer, a memory over which they are constantly trying to gain control. Thus, the random generator unit is an inclusive rather than separate element of the sound producing mechanism and becomes an opposing character in the context of the performance. Since the performer is interacting with pre-recorded files, the local function in the ‘Hey’ section is ​imaginary​. In the ‘Resonator’ section, instead, we have a direct manipulation of the sound producing mechanism and, although limited to the control of the wet/dry effect’s parameter, we can still speak of ​real ​function. Figure 4. Local and field function displacement across Part A, B and C. The variety of mapping strategies experimented with in Part B are, however, in stark contrast with the complexity of the single one presented in Part A. This is also reflected in the level of control intimacy perceived by the performer that has decreased in Part B. This, however, can be counterbalanced by the clarity of the cause/effect mechanisms presented in this section from an audience perspective. To a certain degree it can be stated the quality (Part A) of the devised mapping strategies were contrasted with the quantity (Part B) of the mapping strategies. In that regard, another observation concerns the way each new gesture is introduced in Part B. Indeed, all gestures (‘Freeze’, ‘Hey’ and ‘Resonator’) are introduced and enabled progressively throughout the section. This was done for two reasons. Firstly, it was thought that this method would have enabled the audience to engage in a sort of learning session in which the affordances of the devices could be investigated. Each gesture has its own space in time to reveal its properties before the new gesture is introduced. In this way a gestural vocabulary is built dynamically over time. Secondly, the progressive addition of new gestures to the vocabulary aimed at the creation of a climax for the performance, thus allowing the performance to develop in complexity. This complexity is also interpreted through the increasing number of automated score events added starting from the second half of Part B. I am here referring to the pre-recorded voices extracted from one of my family tapes. The audio extracted from the tape is introduced repeatedly at precise intervals of time and contains confused and loud voices of both adults and children. The complexity here is provided in two ways. On the one hand, the voices add to the soundscape that the performer is creating. On the other hand, these voices begin a process of re-evaluation of the performance’s ​frames​. They belong to the Field function. Furthermore, since human voices are from the real world, they belong to the real field function. Thus, if in Part A the ​field ​and ​local functions were a ​continuum ​confined in the Local territory, now they begin to separate. This ​frames’ ​movement continues in Part C where no performer interaction is allowed and thus now sees the Local function vanishing to leave space for the Field function only. In the metaphoric context of the performance, the action in Part B represents the moment in which subconscious memories start to emerge and mix with the conscious ones the performer is trying to gain control over. But this is only the start of a process that culminates with Part C, in which the performer loses control, his conscious memories wiped out by forces he cannot control. These forces, however, allow for a personal re-interpretation of the same memories the performer has manipulated up until now. Figure 5. Agorá: screenshot from live performance at Ormston Gallery - Limerick. Conclusions This article has presented and discussed the journey that led to the development of a sound theatre performance, Agorá, written for a solo data glove and quadrophonic audio setup. Its development, rather than being linear, was guided by an intertwined series of both technical and aesthetic issues and constraints. A key element was the design of nuanced gestural performative actions that were both visually and sonically legible to audience members, in order to facilitate the audience’s reading of the cause/effect mechanisms that link the performer's gestures to the sounds. This helped to form a clearer picture of the modalities of interactions enabled by the DMI itself and ultimately to facilitate the making of an informed opinion on the overall performance. This source-bonding, in conjunction with the space delimited by the quadrophonic setup, also enabled a play of specific areas of interest described in terms of local and field functions. In light of these considerations, the reasoning around the concepts of affordances, constraints, metaphors, functions and frames has helped to define a framework for the development and analysis of a DMI sound-theatre performance.. In particular, it was found that the concept of metaphor was central to the establishment of coherent choices for the development of all the elements involved in the performance. The establishment of coherent metaphors, within the theme of the performance, has helped devise sophisticated (while also readable) gestures to sound mappings. Also, it influenced the selection of the sound sources and the decision as to whether these needed to be controlled live or via an automated chain of events. As pointed out by several authors and practitioners, contemporary practices in digital media and performance arts require a wide range of expertise across several fields of research which can include engineering, computer science as well as aesthetics. It is true, at the same time, that in order to engage in productive multidisciplinary collaborations, a familiarity with the language used in both engineering and arts needs to be developed by all parties involved. I believe this familiarity should be a concern for practitioners, audience and critics. Such an approach can ultimately facilitate the analysis of live performances in which novel technologies, multimedia and multidisciplinary elements are deployed. References Bar-Itzhack, I. Y., and R. R. Harman. 1996, Optimized Triad Algorithm for Attitude Estimation, USA: NASA Goddard Space Flight Center. Emmerson, S. 2007, ‘Playing Spaces’: Towards an Aesthetics of Live Electronics, in Living Electronic Music. Aldershot: Ashgate Publishing Limited, 90-116. Fels, S., A. Gadd, and A. Mulder. 2002, “Mapping transparency through metaphor:towards more expressive musical instruments.” Organised Sound, 7 (2): 109–126. doi:10.1017/S1355771802002042 Fischman, R. 2013. “A Manual Action Expressive System (MAES)”, Organised Sound 10 (3): 328-345. doi: ​ http://dx.doi.org/10.1017/S1355771813000307 Fyans, C. A., and M. Gurevich. 2011, “Perceptions of Skill in Performances with Acoustic and Electronic Instruments.” In Proceedings of the Conference on New Interfaces for Musical Expression. 495–498. Oslo: University of Oslo and Norwegian Academy of Music. Godøy, M., and R. Leman. 2010. Musical Gestures: Sound, Movement and Meaning. New York: Routledge. Goudeseune, C. 2002. “Interpolated mappings for musical instruments.” Organised Sound, 7 (2): 85 96. doi:10.1017/S1355771802002029 Magnusson, T. 2010. “Designing Constraints: Composing and Performing with Digital Music Systems.” Computer Music Journal, 34 (4): 62–73. Malloch, J., S. Sinclair, A. Hollinger, and M. Wanderley. 2011. Musical Robots and Interactive Multimodal Systems. Berlin: Springer-Verlag. Marier, M. 2012. “Designing Mappings for Musical Interfaces Using Preset Interpolation.” In Proceedings of the Conference on New Interfaces for Musical Expression. Ann Arbor, Michigan: University of Michigan. Moore, F. R. 1988. “The dysfunctions of MIDI.” Computer Music Journal, 12 (1): 19–28. Pearce, M., and G. A. Wiggins. 2002. “Aspects of a Cognitive Theory of Creativity in Musical Composition.” in 2nd International Workshop on Creative Systems, European Conference on Artificial Intelligence. Schwarz, D. 2004. “Data-Driven Concatenative Sound Synthesis.” Ph.D. diss., Ircam - Centre Pompidou - University of Paris 6 Pierre et Marie Curie. Torre, G. 2013a, Pointing-at Video Documetary, Accessed Nov 3 2014. https://vimeo.com/52557261 Torre, G. 2013b. “The Design of a New Musical Glove: A Live Performance Approach.” PhD diss., University of Limerick. Torre, G. 2013c, Agorá, Accessed Nov 3 2014. https://vimeo.com/44650248 Torre, G., Fernstrom M., O'Flynn B. and Angove P. 2007. “Celeritas: Wearable Wireless System', In Proceedings of the Conference on New Interfaces for Musical Expression, 205-208. New York: ACM Verfaille, V., M. Wanderley, and P. Depalle. 2006. “Mapping Strategies for Gestural and Adaptive Control of Digital Audio Effects.” Journal of New Music Research 35 (1): 71–93. Wanderley, M. 2002. “Mapping Strategies in Real-time Computer Music.” Organised Sound 7 (2): 97–108. Wilkins, C. 2013. “A dramaturgy of digital sound: Towards an aesthetic approach.” International Journal of Performance Art & Digital Media 9 (2): 329-345. doi:10.1386/padm.9.2.329_1 Wilson-Bokowiec, J., and Bokowiec, M,. (2006), Kineasonics: The Intertwining Relationship of Body and Sound. Contemporary Music Review. Special Issue: Bodily Instruments and Instrumental Bodies. 25 (1+2): 47-58. Wilson-Bokowiec, J. (2010), “Physicality: The ​techne​ of the physical in interactive digital performance”, International Journal of Performance Art & Digital Media 6 (1): 61-75. doi:10.1386/padm.6.1.61_1 Woodman, O. J. 2007. “An Introduction to Inertial Navigation.” Technical report, UCAM-CL-TR-696 - University of Cambridge. Wordsworth, W. 1850. ‘Volume Three - Book Twelfth’, The Prelude. 